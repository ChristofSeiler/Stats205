---
title: "ANOVA"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output:
  beamer_presentation:
    incremental: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(1234)
library(tidyr)
library(dplyr)
```

## Overview

Before:

* One-Sample, Two-Sample Problems

Today:

* ANOVA (two or more samples)

<!--
## What is ANOVA?

* Classical ANOVA for balanced data does three things at once:

1. Additive data decomposition of the variance of each component
2. Comparisons of mean squares
3. Linear model fit
-->

## One-Way ANOVA

* Determine the effect of a single factor $A$ on a response over a specific population
* Assume $A$ consists of $k$ levels or treatmens
* In a completely randomize design, $n$ subjects are randomly selected from the reference population
* $n_j$ randomly assigned to treatment $j = 1,\dots,k$
* Let $i$th be the response in the $j$th treatment denoted by $Y_{ij}$, $i = 1,\dots,n_j$
* **Assumptions:**
    * Responses are independent of another
    * Distribution among levels differ by at most shifts in location

## One-Way ANOVA

* **Data**:
$$
\begin{tabular}{cccc}
\hline
\multicolumn{4}{c}{Treatment} \\
1 & 2 & \dots & k \\
\hline
$Y_{11}$ & $Y_{12}$ & $\dots$ & $Y_{1k}$ \\
$Y_{21}$ & $Y_{22}$ & \dots & $Y_{2k}$ \\
\vdots & \vdots &       & \vdots \\
$Y_{n_j1}$ & $Y_{n_j2}$ & \dots & $Y_{n_jk}$ 
\end{tabular}
$$
* **Model**
$$Y_{ij} = \theta + \mu_i + e_{ij}, \hspace{0.5cm} i=1,\dots,n_j, \hspace{0.5cm} j = 1,\dots,k$$
with 
    * overall median $\theta$
    * $\mu_i$ is the treatment effect
    * $e_{ij}$ samples from continuous distribution with median $0$

## One-Way ANOVA

* The null hypothesis
$$H_0: \mu_1 = \dots = \mu_k $$
underlying distributions $F_1,\dots,F_k$ are connected through the relationship
$$F_j(t) = F(t-\mu_j), -\infty < t < \infty$$
* The alternative is that at least two of the treatment are not equal 
$$H_A: \mu_1,\dots,\mu_k \text{ not all equal}$$

## Kruskal-Wallis Test

* Total sample size $n = \sum_{j=1}^k n_j$
* Rank $R_{ij}$ of response $Y_{ij}$ among all $n$ observations; ranking done without knowledge of treatment
* Let $R_{.j}$ denotes the average of the ranks for sample $j$
* The **Kruskal-Wallis test statistic**
$$H = \frac{12}{n(n+1)} \sum_{j=1}^k n_j \left( R_{.j} - \frac{n+1}{2}\right)^2$$
* Asymptotically $\chi^2$ distributed with $k-1$ degrees of freedom

## Kruskal-Wallis Test

* Motivation for the test
* The **Kruskal-Wallis test statistic**
$$H = \frac{12}{n(n+1)} \sum_{j=1}^k n_j \left( R_{.j} - \frac{n+1}{2}\right)^2$$
* The average rank sample $j = 1,\dots,k$ is 
$$\operatorname{E_{H_0}}(R_{.j}) = \operatorname{E_{H_0}} \left( \frac{1}{n_j} \sum_{i=1}^{n_j} R_{ij} \right) = \frac{1}{n_j} \sum_{i=1}^{n_j} \operatorname{E_{H_0}}(R_{ij}) = \frac{n + 1}{2}$$

## Kruskal-Wallis Test (Example)

* Efficiency self-clearing mechanism of respiratory tract that conducts air into the lungs from the rate of dust in the three groups: 
    * normal subjects, 
    * subjects with obstructive airway disease, and 
    * subjects with asbestosis
* Responses are the clearance half-times of the subjects
* Sample sizes: $n_1 = n_3 = 5$ and $n_2 = 4$

## Kruskal-Wallis Test (Example)

```{r echo=TRUE}
normal = c(2.9,3.0,2.5,2.6,3.2)
obstruct = c(3.8,2.7,4.0,2.4)
asbestosis = c(2.8,3.4,3.7,2.2,2.0)
x = c(normal,obstruct,asbestosis)
g = c(rep(1,5),rep(2,4),rep(3,5))
test = kruskal.test(x,g)
test$statistic
test$p.value
```

## Two-Way ANOVA

* Same as before but now we have blocks:
$$
\begin{tabular}{l|cccc}
\hline
& \multicolumn{4}{c}{Treatment} \\
Blocks & 1 & 2 & \dots & k \\
\hline
1 & $Y_{111}$ & $Y_{121}$ & $\dots$ & $Y_{1k1}$ \\
& \vdots & \vdots &       & \vdots \\
& $Y_{11c_{11}}$ & $Y_{12c_{12}}$ & \dots & $Y_{1kc_{1k}}$ \\
2 & $Y_{211}$ & $Y_{221}$ & $\dots$ & $Y_{2k1}$ \\
& \vdots & \vdots &       & \vdots \\
& $Y_{21c_{21}}$ & $Y_{22_{c_{22}}}$ & \dots & $Y_{2kc_{2k}}$ \\
\vdots & \vdots & \vdots &       & \vdots \\
\end{tabular}
$$
* The **Friedman test** is analog to the Kruskal-Wallis test

<!--
## Two-Way ANOVA

* The full **model** is
$$Y_{kj} = \alpha + \beta_j + b_k + e_{kj}, \hspace{0.5cm} k=1,\dots,n, \hspace{0.5cm} j = 1,\dots,k$$
with 
    * $\alpha$ is an intercept parameter
    * $\beta_j$ is the $j$th treatment effect
    * $b_k$ is the random effect due to cluster $k$, and
    * $\epsilon_{kj}$ is the $jk$th random error
    * assume that the random errors are iid and are independent of the random effects

## Friedman's Test

* The **Friedman test statistics**:
$$\overline{R}_{.j} = \frac{\sum_{k=1}^m R_{kj}}{m}$$
$$H = \frac{12m}{n(n+1)} \sum_{j=1}^n n_i \left( \overline{R}_{.j} - \frac{n+1}{2}\right)^2$$
-->

## Median Polish

* For special case of no repetitions (one observation per block/treatment cell)
* This may be the actual data we observe or someone may have summarized all the entries in each cell with a single number
* **Data**:
$$
\begin{tabular}{l|ccc}
\hline
& \multicolumn{3}{c}{j} \\
$i$ & $1$ & \dots & $J$ \\
\hline
$1$ & $Y_{11}$ & \dots & $Y_{1J}$ \\
\vdots & \vdots &  & \vdots \\
$I$ & $Y_{I1}$ & \dots & $Y_{IJ}$
\end{tabular}
$$

## Median Polish

* Infant mortality rates in the United States 1964-1966 by region and father's eduction
* Cell entires are number of deaths (under one year old) per 1000 live births
```{r}
df = data.frame(row.names = c("NE","NC","S","W"),
                 ed8       = c(25.3,32.1,38.8,25.4), 
                 ed9to11   = c(25.3,29,31,21.1),
                 ed12      = c(18.2,18.8,19.3,20.3),
                 ed13to15  = c(18.3,24.3,15.7,24),
                 ed16      = c(16.3,19,16.8,17.5)
                 )
df
```

## Median Polish -- Cleveland Dot Plot

```{r}
dotchart( as.matrix(df), cex=1.2)
```

## Median Polish -- Cleveland Dot Plot

```{r}
dotchart( t(as.matrix(df)), cex=1.2)
```

## Median Polish

```{r}
df.l <- df %>%
  mutate(Region = as.factor(row.names(.)) )  %>%
  gather(key=Edu, value = Deaths, -Region) %>% 
  mutate(Edu = factor(Edu, levels=names(df)))

# side-by-side plot
OP <- par(mfrow=c(1,2))
plot(Deaths ~ Region + Edu, df.l)
par(OP)
```

## Median Polish

* **Additive model**:
$$Y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}$$
* Overall typical value $\mu$
* Row effect $\alpha_i$
* Column effect $\beta_j$
* Random fluctuation $\epsilon_{ij}$

## Median Polish

* Compute overall typical value $\mu$
```{r echo=TRUE}
mu = median(as.matrix(df)); mu; df
```

## Median Polish

* Compute the row medians
```{r echo=TRUE}
df = as.matrix(df) - mu
rowMedian = apply(df,1,median)
df = df - rowMedian; df
```

## Median Polish

* Add row median to residual table
```{r echo=TRUE}
df = cbind(roweff=c(rowMedian),df); df
```

## Median Polish

* Add and overall value to residual table
```{r echo=TRUE}
df = rbind(coleff=rep(0,6),df); df[1,1] = mu; df
```

## Median Polish

* Compute column median
```{r echo=TRUE}
colMedian = apply(df[2:5,],2,median); colMedian
```

## Median Polish

* Create new residual table from column medians
```{r echo=TRUE}
df[1,] = df[1,]+colMedian
df[2:5,] = sweep(df[2:5,],2,colMedian); df
```

## Median Polish

* Second iteration: Add row effects to left margin and subtract from residuals
```{r echo=TRUE}
rowMedian = apply(df[,2:6],1,median); rowMedian
df[,1] = df[,1]+rowMedian
df[,2:6] = sweep(df[,2:6],1,rowMedian); df
```

## Median Polish

* Second iteration: Add column effects to top margin and subtract from residuals
```{r echo=TRUE}
colMedian = apply(df[2:5,],2,median); colMedian
df[1,] = df[1,]+colMedian
df[2:5,] = sweep(df[2:5,],2,colMedian); df
```

## Median Polish

```{r}
df
```

* Infant mortality rates are highest in North Central region and lowest in Northeast
* The education of the father is a stronger factor in distinguishing among these rates than geography
<!--
* With a single exception: the high school graduates and the college dropouts, where the mortality rates increase as the father's eduction increases
-->
* In particular, completion of high school appears to exert the greatest single influence in reducing the mortality rates among infant offspring

## Median Polish

```{r}
df
```
* The residual of 11.15 for the least educated fathers in the South calls for a closer look

## Tukey Additivity Plot

* Comparison value $\alpha_i \beta_j / \mu$
```{r warning=FALSE,out.width=".8\\linewidth"}
df = data.frame(row.names = c("NE","NC","S","W"),
                 ed8       = c(25.3,32.1,38.8,25.4), 
                 ed9to11   = c(25.3,29,31,21.1),
                 ed12      = c(18.2,18.8,19.3,20.3),
                 ed13to15  = c(18.3,24.3,15.7,24),
                 ed16      = c(16.3,19,16.8,17.5)
                 )
df.med = medpolish(df , maxiter=3)
plot(df.med,cex = 1.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
```

## Tukey Additivity Plot

* Why $\alpha_i \beta_j / \mu$ against $\epsilon_{ij}$? 
* To show why this makes sense, we start by asking:
* Can we find a power transformation of the data so that model 
$$y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}$$
will adequately summarize the transformed data?
* If so, then equation
$$y_{ij}^p = m + a_i + b_j + r_{ij}$$
will hold for some value $p$
* If fit exact, then
$$y_{ij}^p = m + a_i + b_j$$
or 
$$y_{ij} = (m + a_i + b_j)^{1/p}$$

## Tukey Additivity Plot

* Compare 
$$y_{ij} = (m + a_i + b_j)^{1/p}$$
to simple additive model, we use a second-order approximation
* Rewrite
$$y_{ij} = m^{1/p} \left( 1 + \frac{a_i}{m} + \frac{b_j}{m} \right)^{1/p}$$
* Taylor expand second factor $(1 + t)^{1/p}$
$$y_{ij} \approx m^{1/p} \left( 1 + \frac{1}{p}\left( \frac{a_i}{m}+\frac{b_j}{m} \right) + \frac{1-p}{2p^2} \left( \frac{a_i}{m}+\frac{b_j}{m} \right)^2 \right)$$

## Tukey Additivity Plot

* Arrange terms in this expansion into four groups, terms that depend 
    * on neither $i$ nor $j$
    * only on $i$
    * only on $j$
    * both $i$ and $j$
* In simplified notation:
$$y_{ij} \approx D \left( 1+\frac{A_i}{D}+\frac{B_j}{D}+\frac{C_{ij}}{D} \right) \hspace{1cm} y_{ij} \approx D + A_i + B_j + C_{ij}$$
$$D = m^{1/p} \hspace{1cm} \frac{A_i}{D} = \frac{1}{p}\frac{a_i}{m}+\frac{1-p}{2p^2}\frac{a_i^2}{m^2} \hspace{1cm} \frac{B_j}{D} = \frac{1}{p}\frac{b_j}{m}+\frac{1-p}{2p^2}\frac{b_j^2}{m^2}$$
$$\frac{C_{ij}}{D} = \frac{1-p}{2p^2}\frac{2a_i b_j}{m^2} = \frac{1-p}{p^2}\frac{a_i}{m}\frac{b_j}{m}$$

## Tukey Additivity Plot

* Through Taylor expansion, we obtained
$$y_{ij} \approx D + A_i + B_j + C_{ij}$$
which is now a function of $p$
* Examine term when $a_i/m$ and $b_j/m$ are close to $0$ (which means that row and column effects are much smaller than common value)
* With this assumption expressions $a_i^2/m^2$, $b_j^2/m^2$, and $a_i b_j / m^2$ can be ignored
$$\frac{A_i}{D}\frac{B_j}{D} \approx \frac{1}{p^2}\frac{a_i}{m}\frac{b_j}{m}$$
* Using this
$$\frac{C_{ij}}{D} \approx (1-p)\frac{A_i}{D}\frac{B_j}{D}$$
* Using this
$$y_{ij} \approx D \left( 1 + \frac{A_i}{D} + \frac{B_j}{D} + (1-p)\frac{A_i}{D}\frac{B_j}{D} \right)$$

## Tukey Additivity Plot

* Rewrite
$$y_{ij} \approx D + A_i + B_j + (1-p)\frac{A_i B_j}{D}$$
* And we conclude that if $y_{ij}^p$ is approximated by an additive model, then, to a second-order approximation, $y_{ij}$ is given by the above approximation
* For the diagnostic plot
$$y_{ij} - D - A_i - B_j \approx (1-p) \frac{A_i B_j}{D}$$
* If $R_{ij} = y_{ij} - D - A_i - B_j$ are residuals, then
$$R_{ij} \approx (1-p) \frac{A_i B_j}{D}$$
<!--
* So for $p = 1$, we shouldn't see a slope
$$y_{ij} \approx D + A_i + B_j$$
-->

## References

* Hoaglin, Mosteller, and Tukey (1983). Understanding Robust and Exploratory Data Analysis
* Manuel Gimond Course Notes: http://mgimond.github.io/ES218/Week11a.html
