---
title: "Bootstrap (Part 4)"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, Stats 205"
output:
  beamer_presentation:
    incremental: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(bootstrap)
library(mvtnorm)
```

## Overview

* So far:
    * Nonparametric bootstrap on the rows (e.g. regression, PCA with random rows and columns)
    * Nonparametric bootstrap on the residuals (e.g. regression)
    * Parametric bootstrap (e.g. PCA with fixed rows and columns)
    * Studentized bootstrap
* Today:
    * Bias-Corrected-accelerated (BCa) bootstrap
    * From BCa to ABC

## Motivation

* Correlation coefficient of bivariate normal with $\rho = 0.577$
```{r echo=TRUE}
sigma = matrix(nrow = 2,ncol = 2)
diag(sigma) = 1
rho = 0.577
sigma[1,2] = sigma[2,1] = rho
sigma
```
* Distribution of sample correlation coefficient ($n = 10$)
* Compare: Percentile, Studentized, and Bias-Corrected-Accelerated (BCa) bootstrap

## Motivation

```{r fig.width=10,fig.height=6}
n <- 10
sim <- 100000
corhat <- numeric(sim)
for (j in 1:sim) {
  xdata <- rmvnorm(n,mean=rep(0,2),sigma=sigma)
  corhat[j] <- cor(xdata[,1],xdata[,2])
}
hist(corhat,breaks=200)
abline(v = rho,col = "blue",lwd = 4)
abline(v = mean(corhat),col = "red",lwd = 4)
```

```{r echo=TRUE}
bias = rho - mean(corhat); bias
```

## Motivation

```{r}
set.seed(43245)
r <- 100
B <- 1000
simple.contains <- numeric(r)
simple.ci <- matrix(NA,nrow=r,ncol=2)
for (i in 1:r) {
  sigma = matrix(nrow = 2,ncol = 2)
  diag(sigma) = 1
  sigma[1,2] = sigma[2,1] = rho
  xdata <- rmvnorm(n,mean=rep(0,2),sigma=sigma)
  #xdata <- matrix(rnorm(2*n),ncol=2)
  theta <- function(x,xdata){ cor(xdata[x,1],xdata[x,2]) }
  results <- bootstrap(1:n,B,theta,xdata)
  simple.ci[i,] <- quantile(results$thetastar,c(.025,.975))
}
simple.contains <- (rho > simple.ci[,1] & rho < simple.ci[,2])
#sum(simple.contains)/r
#mean(simple.ci[,2] - simple.ci[,1])
```

```{r fig.width=6,fig.height=6}
plot(0,0,type="n",xlim=c(-1,1.5),ylim=c(0,r),xlab="",ylab="",main="Percentile Bootstrap")
segments(simple.ci[,1],1:r,simple.ci[,2],1:r,col=ifelse(simple.contains,"black","red"))
abline(v=rho,col="blue")
```

## Motivation

* Studentized bootstrap with variance stabilization fails due to numerical problems 

## Motivation

```{r warning=FALSE}
set.seed(43245)
pivot.contains <- numeric(r)
pivot.ci <- matrix(NA,nrow=r,ncol=2)
for (i in 1:r) {
  sigma = matrix(nrow = 2,ncol = 2)
  diag(sigma) = 1
  sigma[1,2] = sigma[2,1] = rho
  xdata <- rmvnorm(n,mean=rep(0,2),sigma=sigma)
  #xdata <- matrix(rnorm(2*n),ncol=2)
  theta <- function(x,xdata){ cor(xdata[x,1],xdata[x,2]) }
  results <- boott(1:n,theta,xdata,perc=c(.025,.975),VS=FALSE,nboott=B)
  pivot.ci[i,] <- c(results$confpoints[1],results$confpoints[2])
}
pivot.contains <- (rho > pivot.ci[,1] & rho < pivot.ci[,2])
#sum(pivot.contains)/r
#mean(pivot.ci[,2] - pivot.ci[,1])
```

```{r fig.width=6,fig.height=6}
plot(0,0,type="n",xlim=c(-1,1.5),ylim=c(0,r),xlab="",ylab="",main="Studentized Bootstrap Without Variance Stabilization")
segments(pivot.ci[,1],1:r,pivot.ci[,2],1:r,col=ifelse(pivot.contains,"black","red"))
abline(v=rho,col="blue")
```

## Motivation

```{r}
set.seed(43245)
bca.contains <- numeric(r)
bca.ci <- matrix(NA,nrow=r,ncol=2)
for (i in 1:r) {
  sigma = matrix(nrow = 2,ncol = 2)
  diag(sigma) = 1
  sigma[1,2] = sigma[2,1] = rho
  xdata <- rmvnorm(n,mean=rep(0,2),sigma=sigma)
  #xdata <- matrix(rnorm(2*n),ncol=2)
  theta <- function(x,xdata){ cor(xdata[x,1],xdata[x,2]) }
  results <- bcanon(1:n,B,theta,xdata,alpha=c(.025,.975))
  bca.ci[i,] <- c(results$confpoints[1,2],results$confpoints[2,2])
}
bca.contains <- (rho > bca.ci[,1] & rho < bca.ci[,2])
#sum(bca.contains)/r
#mean(bca.ci[,2] - bca.ci[,1])
```

```{r fig.width=6,fig.height=6}
plot(0,0,type="n",xlim=c(-1,1.5),ylim=c(0,r),xlab="",ylab="",main="BCa Bootstrap")
segments(bca.ci[,1],1:r,bca.ci[,2],1:r,col=ifelse(bca.contains,"black","red"))
abline(v=rho,col="blue")
```

## Motivation

```{r fig.width=6,fig.height=6}
par(mfrow=c(3,1))

plot(0,0,type="n",xlim=c(-1,1.5),ylim=c(0,r),xlab="",ylab="",main="Percentile Bootstrap")
segments(simple.ci[,1],1:r,simple.ci[,2],1:r,col=ifelse(simple.contains,"black","red"))
abline(v=rho,col="blue")

plot(0,0,type="n",xlim=c(-1,1.5),ylim=c(0,r),xlab="",ylab="",main="Studentized Bootstrap Without Variance Stabilization")
segments(pivot.ci[,1],1:r,pivot.ci[,2],1:r,col=ifelse(pivot.contains,"black","red"))
abline(v=rho,col="blue")

plot(0,0,type="n",xlim=c(-1,1.5),ylim=c(0,r),xlab="",ylab="",main="BCa Bootstrap")
segments(bca.ci[,1],1:r,bca.ci[,2],1:r,col=ifelse(bca.contains,"black","red"))
abline(v=rho,col="blue")
```

## BCa Bootstrap

* The bias-corrected bootstrap is similar to the percentile bootstrap
* Recall the percentile bootstrap:
* Take bootstrap samples $$\hat{\theta}^{*1},\dots,\hat{\theta}^{*B}$$
* Order them $$\hat{\theta}^{(*1)},\dots,\hat{\theta}^{(*B)}$$
* Define interval as $$(\hat{\theta}^{(*B\alpha)},\hat{\theta}^{(*B(1-\alpha))})$$ (assuming that $B\alpha$ and $B(1-\alpha)$ are integers)

## BCa Bootstrap

* Assume that there is an monotone increasing transformation $g$ such that 
$$\phi = g(\theta) \hspace{0.5cm}\text{and}\hspace{0.5cm} \hat{\phi} = g(\hat{\theta})$$
* The BCa bootstrap is based on this model
$$\frac{\hat{\phi}-\phi}{\sigma_{\phi}} \sim N(-z_0,1) \hspace{0.5cm} \text{with} \hspace{0.5cm} \sigma_{\phi} = 1+a\phi$$
* Which is a generalization of the usual normal approximation
$$\frac{\hat{\theta}-\theta}{\sigma} \sim N(0,1)$$

## BCa Bootstrap

* $\hat{z}_0$ is the bias estimate
* $\hat{z}_0$ measures discrepancy between the median of $\hat{\theta}^*$ and $\hat{\theta}$
* It is estimated with
$$\hat{z}_0 = \Phi^{-1} \left( \frac{\#\{ \hat{\theta}^{*b} < \hat{\theta} \}}{B} \right)$$
* We obtain $\hat{z}_0 = 0$ if half of the $\hat{\theta}^{*b}$ values are less than or equal to $\hat{\theta}$

## BCa Bootstrap

* $\hat{a}$ is the skewness estimate
* $\hat{a}$ measures the rate of change of the standard error of $\hat{\theta}$ with respect to the true parameter $\theta$
* It is estimated using the Jackknife
    * Delete $i$th observation in original sample denote new sample by $\hat{\theta}_{(i)}$ and estimate 
$$\hat{\theta}_{(\cdot)} = \sum_{i=1}^n \frac{\hat{\theta}_{(i)}}{n}$$
* Then 
$$\hat{a} = \frac{\sum_{i=1}^n (\hat{\theta}_{(\cdot)}-\hat{\theta}_{(i)})^3}{6\{\sum_{i=1}^n (\hat{\theta}_{(\cdot)}-\hat{\theta}_{(i)})^2\}^{3/2}}$$

## BCa Bootstrap

* The bias-corrected version makes two additional corrections to the percentile version
* By redefining lower $\alpha_1$ and upper $\alpha_2$ levels as
$$\hspace{-1cm} \alpha_1 = \Phi\left(\hat{z}_0 + \frac{\hat{z}_0+z^{(\alpha)}}{1-\hat{\alpha}(\hat{z}_0+z^{(\alpha)})}\right) \hspace{0.5cm} \alpha_2 = \Phi\left(\hat{z}_0 + \frac{\hat{z}_0+z^{(1-\alpha)}}{1-\hat{\alpha}(\hat{z}_0+z^{(1-\alpha)})}\right)$$
with $z^{(\alpha)}$ being the 100$\alpha$ percentile of standard normal  
and $\Phi$ normal CDF
* When $\hat{a}$ and $\hat{z}_0$ are equal to zero then $\alpha_1=\alpha$ and $\alpha_2=1-\alpha$
* The interval is then given by
$$(\hat{\theta}^{(*B\alpha_1)},\hat{\theta}^{(*B\alpha_2)})$$ (assuming that $B\alpha_1$ and $B\alpha_2$ are integers)

## BCa Bootstrap

* Same asymptotic accuracy as the studentized bootstrap
* Can handle out of range problem as well
* Efron (1987) for detailed justification of this model

## BCa Bootstrap in R

```{r echo=TRUE}
library(bootstrap)
xdata = matrix(rnorm(30),ncol=2); n = 15
theta = function(x,xdata) { 
  cor(xdata[x,1],xdata[x,2]) 
  }
results = bcanon(1:n,100,theta,xdata,
                 alpha=c(0.025, 0.975))
results$confpoints
```

<!--
don't need it, computers are fast enough today

## Accelerated Bias-Corrected Bootstrap

TODO: Chapter 14
-->

## Properties of Different Boostrap Methods

$$
\small
\begin{tabular}{l|cccc}
                                        & Standard      & Percentile    & Studentized$^*$ & BCa       \\
\hline                                                                                                \\
Asymptotic Acurracy                     & $O(\sqrt{n})$ & $O(\sqrt{n})$ & $O(1/n)$        & $O(1/n)$  \\
Range-Preserving                        & No            & Yes           & No              & Yes       \\
Transformation-Invariant                & No            & Yes           & No              & Yes       \\
Bias-Correcting                         & No            & No            & No              & Yes       \\
Skeweness-Correcting                    & No            & Yes           & Yes             & Yes       \\
$\hat{\sigma},\hat{\sigma}^*$ required  & No            & No            & Yes             & No        \\
Analytic constant or\\
variance stabilizing\\ 
tranformation required                  & No            & No            & Yes             & Yes       \\
\end{tabular}
$$

$^*$ with variance stabilization

## Properties of Different Boostrap Methods

For nonparametric boostrap: 

```{r out.width="1.0\\linewidth"}
knitr::include_graphics("BootstrapGuide.png")
```

Source: Carpenter and Bithell (2000)

## Many More Topics

* Using the boostrap for better confidence in model selection (Efron 2014)
* Using the jackknife and the infinitesimal jackknife for confidence intervals in random forests prediction or classification (Wager, Hastie, and Efron 2014)

## Approximate Bayesian Computation (ABC)

* Goal: We wish to sample from the posterior distribution $p(\theta|D)$ given data $D$
$$
p(\theta|D) = \frac{p(D|\theta)p(\theta)}{p(D)}
$$
* Setting:
    * The likelihood $p(D|\theta)$ is hard to evaluate or expensive to compute  (e.g. missing normalizing constant)
    * Easy to sample form likelihood $p(D|\theta)$
    * Easy to samples from prior $p(\theta)$
* Examples:
    * Population genetics (latent variables)
    * Ecology, epidemiology, systems biology (models based on differential equations)

## Approximate Bayesian Computation (ABC)

* Sampling algorithm (with data $D = \{y_1,\dots,y_n\}$):
    1. Sample $\theta_i \sim p(\theta)$
    2. Sample $x_i \sim p(x|\theta_i)$
    3. Reject $\theta_i$ if $x_i \ne y$
* ABC sampling (define statistics $\mu$, distance $\rho$, and tolerance $\epsilon$):
    1. Sample $\theta_i \sim p(\theta)$
    2. Sample $x_i \sim p(x|\theta_i)$
    3. Reject $\theta_i$ if $\rho(\mu(x_i),\mu(y)) > \epsilon$

----

```{r out.width="1\\linewidth"}
knitr::include_graphics("Approximate_Bayesian_computation_conceptual_overview.pdf")
```

## References

* Efron (1987). Better Bootstrap Confidence Intervals
* Hall (1992). The Bootstrap and Edgeworth Expansion
* Efron and Tibshirani (1994). An Introduction to the Bootstrap
* Carpenter and Bithell (2000). Bootstrap Conidence Intervals: When, Which, What? A Practical Guide for Medical Statisticians
* Marin, Pudlo, Robert, and Ryder (2012). Approximate Bayesian Computational Methods
* Efron (2014). Estimation and Accuracy after Model Selection
* Wager, Hastie, and Efron (2014). Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife

<!--
* Fisher and Hall (1989). Bootstrap Confidence Regions for Directional Data
* Fisher, Hall, Jing, and Wood (1996). Pivotal Methods for Constructing Confidence Regions With Directional Data
-->
