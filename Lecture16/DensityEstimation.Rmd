---
title: "Density Estimation"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output:
  beamer_presentation:
    incremental: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(1234)
library(ggplot2)
library(reshape2)
library(ElemStatLearn)
```

## Overview

Last time:

* Cross-Validation
* Variance Estimation
* Confidence Bands

Today: 

* Density Estimation

## Density Estimation

* Let $F$ be a distribution with density $f$ and let sample
$$X_1,X_2,\dots,X_n \sim F$$
be iid from $F$
* The goal is to estimate $f$ with as few assumptions about $f$ as possible
* Denote the estimator by $\widehat{f}_n$
* The nonparametric estiamtor will depend on smoothing parameter $h$

## Density Estimation

```{r out.width=".7\\linewidth"}
knitr::include_graphics("BartSimpsonDensity.png")
```

Source: Wassermann (2006)

## Cross-Validation

* We will do leave-one-out cross-validation
* We use the same risk as in the regression case
$$L = \int (\widehat{f}_n(x) - f(x))^2 dx$$
* But there are some differences to the regression case, because we don't have response variable
* The loss function is ($\widehat{f}_n(x)$ will depend on $h$)
$$L(h) = \int \widehat{f}_n^2(x) dx - \int \widehat{f}_n(x)f(x)dx + \int f^2(x) dx$$
* The last term doesn't depend on $h$, so minimizing $L(h)$ is equivalent to minimizing
$$J(h) = \int \widehat{f}_n^2(x) - 2 \int \widehat{f}_n(x)f(x) dx$$

## Cross-Validation

* We define the cross-validation estimator of risk as
$$\widehat{J}(h) = \int \widehat{f}_n^2(x) - \frac{2}{n} \sum_{i=1}^n \widehat{f}_{(-i)}(X_i)$$
where as in the regression case $\widehat{f}_{(-i)}$ is the density estimator obtained after removing the $i$th observation

## Histograms

* One of the simplest estimators is the histogram
* Suppose that $f$ has support in interval $[0,1]$
* Let $m$ be an integer and define bins
$$B_1 = \left[0,\frac{1}{m}\right), B_1 = \left[\frac{1}{m},\frac{2}{m}\right),\dots,B_m = \left[\frac{m-1}{m},1\right]$$
* Define bindwith $h = 1/m$, let $Y_j$ be the number of observations in $B_j$
* Let $\widehat{p}_j = Y_j/n$ and $p_j = \int_{B_j} f(u) du$
* Then the **histogram estimator** is defined by 
$$\widehat{f}_n(x) = \sum_{j=1}^m \frac{\widehat{p}_j}{h} I(x \in B_j)$$
* Motivation: for $x \in B_j$ and $h$ small
$$\operatorname{E}(\widehat{f}_n(x)) = \frac{\operatorname{E}(\widehat{p}_j)}{h} = \frac{p_j}{h} = \frac{\int_{B_j}f(u)du}{h} \approx \frac{f(x)h}{h} = f(x)$$

## Smoothed Bootstrap

* Parametric bootstrap and the nonparmaetric bootstrap
* Inermediate choice smoothing empirical cdf

TODO: Efron and Tibshirani (1994), page 164-166

http://statweb.stanford.edu/~susan/courses/s208/node20.html

https://www.jstor.org/stable/2336686

## References

* Wassermann (2006). All of Nonparametric Statistics
* Efron and Tibshirani (1994). An Introduction to the Bootstrap
