---
title: "Nonlinear Regression (Part 3)"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output:
  beamer_presentation:
    incremental: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(1234)
library(ggplot2)
library(reshape2)
library(ElemStatLearn)
```

## Overview

Last time:

* Linear Smoothers
    * Local Averages
    * Local Regression
    * Penalized Regression

Today: 

* Cross-Validation
* Variance Estimation
* Confidence Bands
* Smooth Bootstrap

## Nonlinear Regression

* We are given $n$ pairs of obserations $(x_1,Y_1),\dots,(x_n,Y_n)$
* The **response variable** is related to the **covariate** 
$$Y_i = r(x_i) + \epsilon_i \hspace{2cm} \operatorname{E}(\epsilon_i) = 0, i = 1,\dots,n$$
with $r$ being the **regression function**
* For now, assume that variance $\operatorname{Var}(\epsilon_i) = \sigma^2$ is independent of $x$
* The covariates $x_i$ are fixed

## Choosing the Smoothing Parameter

* The choice of kernel is not too important
* Estimates obtained by using different kernels are usually numerically very similar
* Can be confirmed by theoretical calculations showing that risk is insensitive to choice of kernel
* Choice of bandwidth matters which controls the amount of smoothing
* Small bandwidths give very rough estimates while larger bandwidths give smoother estimates

## Choosing the Smoothing Parameter

* If the bandwidth is small, $r(x_0)$ is an average of a small number of $Y_i$ close to $x_0$
* Its variance will be relatively largeâ€”close to that of an individual $Y_i$
* The bias will tend to be small, because each of the $E(Y_i) = r(x_i)$ should be close to $r(x_0)$

* If the bandwidth is large
* The variance of $r(x_0)$ will be small relative to the variance of any $y_i$, because of the effects of averaging 
* The bias will be higher, because we are now using observations $x_i$ further from $x_0$, and there is no guarantee that $r(x_i)$ will be close to $r(x_0)$

## Choosing the Smoothing Parameter

* The smoothers depend on some smoothing parameter $h$
* We define a risk 
$$R(h) = \operatorname{E}\left( \frac{1}{n} \sum_{i=1}^n (\widehat{r_n}(x_i) - r(x_i))^2 \right)$$
* Ideally, we would like to choose $h$ to minimize $R(h)$ 
* But $R(h)$ depends on unkown function $r(x)$
* Instead we minimize an estimate $\widehat{R}(h)$
* As first guess, we might try minimizing the **training error**
$$\frac{1}{n} \sum_{i=1}^n (Y_i - \widehat{r_n}(x_i))^2$$
* This is a poor estimator, because it overfits (undersmoothing)
* We use the data twice: to estimate the function and to estimate the risk

## Choosing the Smoothing Parameter

* A better idea is to use leave-one-out cross-validation
$$\operatorname{cv} = \widehat{R}(h) = \frac{1}{n} \sum_{i=1}^n (Y_i - \widehat{r}_{(-i)}(x_i))^2$$
with $\widehat{r}_{(-i)}$ estimator obtained by omitting the ith pair $(x,Y)$
* Define 
$$\widehat{r}_{(-i)} = \sum_{j=1}^n Y_j l_{j,(-i)}(x)$$
* and we set the weight on $x_i$ to $0$ and renormalize the other weights to sum to one
$$l_{j,(-i)}(x) = 
\begin{cases}
0 & \text{if } j = i \\
\frac{l_j(x)}{\sum_{k \ne i} l_k(x)} & \text{if } j \ne i
\end{cases}$$
* Cross-validation is approximately the predictive risk (predicting the left-one-out observation)

## Choosing the Smoothing Parameter

* We can compute leave-one-out cross-validation without leaving one observation out
$$
\widehat{R}(h) = \frac{1}{n} \sum_{i=1}^n \left( \frac{Y_i - \widehat{r_n}(x_i)}{1-L_{ii}} \right)
$$
* This is exactly true not an approximation!
* After some algebra, we can see that
$$\widehat{r}(x_i) = (1-L_{ii})\widehat{r}_{(-i)}(x_i) + L_{ii}Y_i$$

## Variance Estimation

* There are several variance estimators for linear smoothers
* Let $$\widehat{r}_n(x)$$ be a linear smoother and
* with $$\nu = \operatorname{tr}(L), \tilde{\nu} = \operatorname{tr}(L^TL) = \sum_{i=1}^n \| l(x_i)^2 \|^2$$
* and if $r$ is sufficiently smooth 
* then $\widehat{\sigma}^2$ is a consistent estimator (it converges in probability to the true value of the parameter)

## Variance Estimation

* The expected value of our estimator is
$$\operatorname{E}(\widehat{\sigma}^2) = \frac{\operatorname{E}(Y^T\Lambda Y)}{\operatorname{tr}(\Lambda)} = \sigma^2 + \frac{ \boldsymbol{r}^T\Lambda\boldsymbol{r} }{n-2\nu+\tilde{\nu}}$$
with $$\Lambda = (I-L)^T(I-L)$$ and $$\operatorname{E}(Y^TQY) = \operatorname{tr}(QV)+\mu^TQ\mu$$
where $V = \operatorname{Var}(Y)$ is covariance matrix of $Y$ and $\mu = \operatorname{E}(Y)$ is the mean vector
* Assuming that $\nu$ and $\widehat{\nu}$ do not grow too quickly, and that $r$ is smooth, the second term is small for large $n$
* So $\operatorname{E}(\widehat{\sigma^2}) \sim \sigma^2$
* and one can show that $\operatorname{Var}(\widehat{\sigma^2}) \to 0$

## Variance Estimation

* Another variance estimator 
$$\widehat{\sigma}^2 = \frac{1}{2(n-1)} \sum_{i=1}^{n-1} (Y_{i+1}-Y_i)^2$$
* Assuming $r$ is smooth
$$Y_{i+1}-Y_i = [ r(x_{i+1})+\epsilon_{i+1} ] - [ r(x_{i})+\epsilon_i ] \approx \epsilon_{i+1} - \epsilon_i$$
* Therefore 
$$\operatorname{E}(Y_{i+1}-Y_i) \approx \operatorname{E}(\epsilon_{i+1}) + \operatorname{E}(\epsilon_{i}) = 2\sigma^2$$

## Confidence Bands

* Variability bands
$$\widehat{r}_n(x) \pm 2\widehat{\sigma}(x)$$
* There is a problem with that 
$$\frac{\widehat{r}_n(x)-r(x)}{\widehat{\sigma}(x)} = \frac{\widehat{r}_n(x)-\bar{r}_n(x)}{\widehat{\sigma}(x)} + \frac{\bar{r}_n(x)-r(x)}{\widehat{\sigma}(x)}$$
with $\hat{r}$ being the mean
* First term converges to a normal
* If we do a good job trading off bias and variance, the second term doesn't vanish with large $n$
$$
\frac{\bar{r}_n(x)-r(x)}{\widehat{\sigma}(x)} = \frac{\operatorname{Bias}(\widehat{r}_n(x))}{\sqrt{\operatorname{Variance}(\widehat{r}_n(x)}}
$$

## Confidence Bands

* The result is that the confidence interval will not be centered around the true function $r$ due to the smoothing bias
* Possible solutions:
1. Accept the fact that confidence band is for $\bar{r}_n$ not $r$
2. Estimate bias (this is difficult because it involves estimating $r''(x)$
3. Undersmooth: less smoothing will bias results less, and asymptotically the bias will decrease faster than the variance
* We will go with the first approach

## Constructing Confidence Bands

* For linear smoother $\widehat{r}_n(x)$ with 
$$\bar{r}(x) = \operatorname{E}(\widehat{r}_n(x)) = \sum_{i=1}^n l_i(x) r(x_i)$$
and assuming constant variance 
$$\operatorname{Var}(\widehat{r}_n(x)) = \sigma^2\|l(x)\|^2$$
* Consider confidence bands
$$I(x) = (\widehat{r}_n(x) - c \widehat{\sigma}\|l(x)\|, \widehat{r}_n(x) + c \widehat{\sigma}\|l(x)\|)$$
for some $c$ and $a \le x \le b$

## Constructing Confidence Bands

TODO: Wasserman, page 91 (and appendix p. 119), tube formula

## Smooth Bootstrap

TODO: Efron and Tibshirani (1994), page 70

## References

* Wassermann (2006). All of Nonparametric Statistics
* Efron and Tibshirani (1994). An Introduction to the Bootstrap
