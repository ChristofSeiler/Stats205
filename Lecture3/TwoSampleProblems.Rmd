---
title: "Two-Sample Problems"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output:
  ioslides_presentation:
    incremental: yes
    smaller: yes
    transition: faster
    widescreen: yes
  beamer_presentation:
    incremental: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Last Lecture

* We revisited the bootstrap
* We learned how to use Monte Carlo simulations 
* We learned how to enumerate all possible outcomes
* We saw an examples evaluating correlations between LSAT and GPS in law school

## Today

We will learn how to treat two-sample problems using 

* permutation tests, 
* rank-based methods, and 
* the boostrap

## Hypothesis Testing

* Computer-intensive method that was invented before computers!
* R.A. Fisher introduced it to support Student's $t$-test in the 1930's
* Today, with the computational power, we can use it on a routine basis

* The main application of the permutaiton test is the two sample problem:
$$ \boldsymbol{z} = ( z_1,z_2,\dots,z_n ) \sim F $$
$$ \boldsymbol{y} = ( y_1,y_2,\dots,y_m ) \sim G $$

* $F$ and $G$ are two unkown probability distributions

* We want to test the hypothesis
$$ H_0: F = G $$

* Here the equal sign means that $F$ and $G$ assign same probability to any subset $A$ of common sample space
$$ P_F\left(A\right) = P_G\left(A\right) $$

## Hypothesis Testing

Mouse data of survival time of mice with treatment 
```{r}
library(bootstrap)
mouse.t
```
and control (no treatment)
```{r}
mouse.c
```

observed difference in means
```{r}
theta.hat = mean(mouse.t) - mean(mouse.c)
theta.hat
```

## Hypothesis Testing

* Is *test statistics* $\hat{\theta}$ large? Can we reject the null hypothesis?
* Here we focus on the alternative hypothesis that the treatment increases survival times.
* So we want to compute this probability
$$ P_{H_0}\left(\hat{\theta}^* \ge \hat{\theta}\right) $$
* $\hat{\theta}^*$ is a random variable distributed according to the null distribution
* $\hat{\theta}$ is the observed test statistics
* Hypothesis testing means computing $P_{H_0}$ and checking if small enough
* Usually we choose small probability $\alpha = 0.05$ or $\alpha = 0.01$ and reject if smaller
* If bigger then we reject in favour of alternative (in our case treatment increases survival time)

## Parametric Testing

* Assume that $F$ and $G$ are normal distributions with different means
$$ F = N\left(\mu_T,\sigma^2\right) \hspace{1cm} G = N\left(\mu_C,\sigma^2\right) $$
* The null hypothesis is then 
$$ H_0: \mu_T = \mu_C $$
* If this assumption is correct then $\hat{\theta} = \bar{z} - \bar{y}$ is distributed as 
$$ H_0: \hat{\theta} \sim N\left(0,\sigma^2 \left( \frac{1}{n} + \frac{1}{m} \right) \right) $$
* Using the cumulative distribution of the normal gives our desired probability 
$$ P_{H_0}\left(\hat{\theta}^* \ge \hat{\theta}\right) = 1 - \Phi\left( \frac{\hat{\theta}}{\sigma \sqrt{1/n+1/m}} \right) $$
* We actually don't know $\sigma$, so we have to estimate it which turns the normal into the Student's $t$ distribution (that's why we call it the $t$ test)

## Parametric Testing

For the mouse data, we get

```{r}
 t.test(mouse.t,mouse.c,alternative = "greater",var.equal = TRUE)
```

# Permutation Tests

## Parametric Versus Nonparametric

* The problem parametric test is that we had to estimate the variance $\sigma^2$
* Student's method solves this problem for normal distributions
* But what can we do in general?
* Permuations tests solve it for general distributions
* The only assumption is that observations are *exchangeable*

## Assumptions

* ### Exchangeable Observations
* Consider a collection of random variables 
$$ X_1,\dots,X_n $$
* If their joint distribution are equal under permuations $\pi$
$$P_{X_1,\dots,X_n}(A) = P_{X_{\pi(1)},\dots,X_{\pi(n)}}(A)$$ 
* then $X_1,\dots,X_n$ are called exchangable 

### In practise

Are my observations exchangeable? 

### Answer is yes if

Under the null hypothesis of no differences, we are allowed to exchange group assignments

### Difference between Exchangeability and Independence

TODO

## Test Statistic

Our test statistic:
```{r}
T = function(a,b) { mean(a) - mean(b) }
T(mouse.t,mouse.c)
```

Combine in one vector with group labels:
```{r}
n = length(mouse.t)
m = length(mouse.c)
N = n + m
nPerm = choose(N,n)
nPerm
v = c(mouse.t,mouse.c)
g = c(rep(1,n),rep(0,m))
```

## Enumerations

Compute all possible ways of partitioning N elements into two subsets n and m
```{r}
allPerm = combn(N,n)
theta.hat.star = apply(allPerm,2, function(i) { 
                                    groupA = v[i]; groupB = v[-i]; T(groupA,groupB) })
pvalue = sum(theta.hat.star >= theta.hat) / dim(allPerm)[2]; pvalue
```

```{r echo=FALSE, fig.height=3}
library(ggplot2)
theta.hat.star2 = data.frame(theta.hat.star=theta.hat.star)
ggplot(theta.hat.star2, aes(theta.hat.star)) + 
  geom_histogram(binwidth = 2) +
  geom_vline(xintercept = theta.hat,colour = "red",size = 1.5)
```

## Mont Carlo Simulation

The number of possible combination increases exponentially with the sample size

So for moderately large samples, we'll use Monte Carlo simulations

```{r}
nSamples = 10000; randomPerm = replicate(nSamples,sample(g,replace = FALSE))
theta.hat.star = apply(randomPerm,2, function(x) { T(v[x==1],v[x==0]) } )
pvalue = sum(theta.hat.star >= theta.hat) / nSamples; pvalue
```

```{r echo=FALSE, fig.height=3}
theta.hat.star2 = data.frame(theta.hat.star=theta.hat.star)
ggplot(theta.hat.star2, aes(theta.hat.star)) + 
  geom_histogram(binwidth = 2) +
  geom_vline(xintercept = theta.hat,colour = "red",size = 1.5)
```

# Rank-Based Methods

## Wilcoxon Test

TODO

# Example

## Autism Brain Imaging Data

[Start](NeuroimagingPresentation.html)

# Overall Conclusion

## Summary

TODO

* ... 

## Next Lecture

We will learn about nonparametrics for regression.

## Homeworks

Homework 3 will be posted on the course website today.

Deadline: Tuesday, April 20th before class at 1:30 pm.
