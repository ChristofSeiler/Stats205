---
title: "Autism Brain Imaging Data"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output:
  ioslides_presentation: 
    smaller: yes
    transition: faster
    widescreen: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Neuroimaging

We encouter two-sample problems in many modern applications.

* Compare two populations, such as people who are autistic and healthy controls
* Find morphological, connectivity, and functional difference 
* Inform new therapies and cures

We work on preprocessed neuroimaging data from the Autism Brain Imaging Data Exchange (ABIDE). 
The data is openly avaialbe on the ABIDE website [(here)](http://preprocessed-connectomes-project.org/abide/).

* ABIDE is a collaboration of 16 international imaging sites that have aggregated 
* Open sharing neuroimaging data from 539 individuals suffering from ASD and 573 typical controls
* We will use a subset of 39 paricipants to speedup our analysis

## Data Preparation

* Download data
* Load all images and store them in a big matrix

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(SimpleITK)
library(oro.nifti)
makeImage = function(vecImg, refImg) {
  arrImg = array(vecImg, dim = refImg$GetSize())
  newImg = as.image(arr = arrImg,spacing = refImg$GetSpacing(),origin = refImg$GetOrigin())
  newImg$CopyInformation(refImg)
  return(newImg)
}
```

```{r}
fileNames = list.files(pattern = "_anat_thickness.nii.gz")
nBrains = length(fileNames)
nBrains
```

## One Participant

```{r echo=FALSE}
img = ReadImage(fileNames[1])
orthographic(as.array(img))
```

## Mask

```{r echo=FALSE}
brainList = lapply(1:nBrains, function(i) c(as.array(ReadImage(fileNames[i]))))
brainMatrix = do.call(rbind, brainList)

minThickness = apply(brainMatrix, 2, min)
mask = minThickness > 0
maskImg = makeImage(mask,img)
orthographic(as.array(maskImg))
```

## Part 1: Hypothesis Tests with Rank-Based Method

Split the data in two groups: autistic (1) and healthy participants (2).

```{r}
PaInfo <- read.csv("Phenotypic_V1_0b_preprocessed1.csv", header=TRUE)
copiedFilesInd = PaInfo$FILE_ID %in% substr(fileNames,1,16)
copiedFilesGroup = PaInfo$DX_GROUP[copiedFilesInd]
ThicknessAutistic = brainMatrix[copiedFilesGroup==1,mask==1]
ThicknessHealthy = brainMatrix[copiedFilesGroup==2,mask==1]
```

Peform voxelwise nonparametric test Wilcoxon two-sample rank test.

```{r eval=FALSE}
uncorrectedPValues = sapply(1:dim(ThicknessAutistic)[2], 
                            function(i) 
                              wilcox.test(ThicknessAutistic[,i],
                                          ThicknessHealthy[,i],
                                          alternative = "two.sided")$p.value)
```

## Part 1: Uncorrected $p$-Value Image

```{r  echo=FALSE, warning=FALSE}
library(ggplot2)

uncorrectedPValues = sapply(1:dim(ThicknessAutistic)[2], 
                            function(i) wilcox.test(ThicknessAutistic[,i],ThicknessHealthy[,i],alternative = "two.sided")$p.value)
uncorrectedPValuesVec = rep(1,length(mask))
uncorrectedPValuesVec[mask==1] = uncorrectedPValues
uncorrectedPValuesImg = makeImage(uncorrectedPValuesVec,img)
orthographic(as.array(uncorrectedPValuesImg))
```

## Part 1: Thresholded $p$-Value Image

```{r  echo=FALSE, warning=FALSE}
# significance levels
primary = 0.001

# maximum cluster size
thresholdedImg = BinaryThreshold(uncorrectedPValuesImg,
                                 lowerThreshold=0,
                                 upperThreshold=primary,
                                 insideValue=1,outsideValue=0)
thresholdedSegmentedImg = RelabelComponent(ConnectedComponent(thresholdedImg))
thresholdedSegmentedArr = as.array(thresholdedSegmentedImg)
obsvComps = data.frame(Count=table(thresholdedSegmentedArr[thresholdedSegmentedArr > 0]))
orthographic(as.array(thresholdedImg))
```

## Part 1: Observed Cluster Sizes

Count how many voxels are in one clusters after thresholding.

```{r}
head(obsvComps)
```

## Part 1: Observed Cluster Sizes

```{r echo=FALSE}
labels = floor(seq(1,length(obsvComps$Count.Var1),length.out=20))
ggplot(obsvComps, aes(x = Count.Var1, y = Count.Freq)) + 
  geom_bar(stat="identity") +
  scale_x_discrete(breaks=labels, labels=as.character(labels))
```

Need to account for multiple statistical hypothesis testing

## Part 2: Null Distribution of Cluster Sizes

Using permutations. The red line is signifiance level $\alpha = 0.05$.

```{r echo=FALSE}
nperm = 1000
seed = 1234
maxClusterFileName = paste0("maxCluster.seed",seed,".Rdata")
load(maxClusterFileName)

alpha = 0.05

alphaLargestInd = ceiling(alpha*nperm)
alphaLargest = sort(maxCluster,decreasing = TRUE)[alphaLargestInd]

maxCluster2 = data.frame(maxCluster=maxCluster)
ggplot(maxCluster2, aes(maxCluster)) + 
  geom_histogram(binwidth = 5) +
  geom_vline(xintercept = alphaLargest,colour = "red",size = 1)
```

## Part 2: Compare to Observed Clusters

Are the observed clusters significant?

```{r echo=FALSE}
labels = floor(seq(1,length(obsvComps$Count.Var1),length.out=20))
ggplot(obsvComps, aes(x = Count.Var1, y = Count.Freq)) + 
  geom_bar(stat="identity") +
  scale_x_discrete(breaks=labels, labels=as.character(labels)) +
  geom_hline(yintercept = alphaLargest,colour = "red",size = 1)
```

## Conclusions

* Above horizontal line are significant clusters
* Go back to the image and color all significant voxel clusters

```{r}
nSignificantClusters = sum(obsvComps$Count.Freq >= alphaLargest)
nSignificantClusters
```

After correcting for mulitple hypothesis testing, none of the clusters are significant!
