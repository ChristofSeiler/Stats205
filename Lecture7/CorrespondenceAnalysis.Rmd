---
title: "Correspondence Analysis"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output:
  ioslides_presentation:
    incremental: yes
    smaller: no
    transition: faster
    widescreen: yes
  beamer_presentation:
    incremental: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(1234)
```

## Independence of Two Discrete RVs

* Observations are two measurements on the same subject (e.g. eye color and hair color)
* Random samples $(X_1,Y_1),\dots,(X_n,Y_n)$
* $X$ and $Y$ with different ranges $\{1,2,\dots,r\}$ and $\{1,2,\dots,c\}$
* Consider hyothesis test:
$$
\begin{align}
H_0: P(X=i,Y=j) & = P(X=i)P(Y=j) \text{ for all } i \text{ and } j \\
H_A: P(X=i,Y=j) & \ne P(X=i)P(Y=j) \text{ for some } i \text{ and } j 
\end{align}
$$
* Construct contingency table of $n$ observations 
$$O_{ij} = \#_{1 \le l \le n} \{ (X_l,Y_l) = (i,j) \}$$

## Independence of Two Discrete RVs {.build}

Expected frequencies under independence assumption are product of marginals
$$\widehat{E}_{ij} = n \frac{O_{i\cdot}}{n} \frac{O_{\cdot j}}{n}$$
```{r}
library(ca)
HairEye = margin.table(HairEyeColor, c(1, 2)); HairEye
```

Test statistics $\chi^2 = \sum_{i=1}^r \sum_{j=1}^c \frac{(O_{ij}-\widehat{E}_{ij})^2}{\widehat{E}_{ij}}$

Asymptotically $\chi^2$ with $(r−1)(c−1)$ degrees of freedom

This is called *test of independence*

## Contingency Tables

* Several discrete random variables with same categories $1,\dots,c$
    * Test of homogeneity
    * Are the observations generated from one multinomial distribution with one probability vector?
    * Possibly different sample size among random variable
    * E.g. crime example, types of crimes (theft, fraud, etc.) commited by alcoholic and non-alcoholic criminals
* Two discrete random variables with different categories $1,\dots,r$ and $1,\dots,c$
    * Test of independence
    * Are the pairwise observation independent?
    * Fixed sample size for all random variables
    * E.g. hair color, eye color example

## Goodness of Fit

* The test of homogeneity and independence are called goodness-of-fit tests
* They quantify how well a model fits observations
* They measure discrepancy between observed values and expected values under the model
* Rather than reject this global null hypthesis, can we find what is driving the statistic?
* Can we visualize the residuals?
* Two ways:
    * Visualization of contingency table with association and mosaic plots
    * Visualization of deviation from model with correspondence analysis

## Hair and Eye Color of Statistics Students

Survey of students at the University of Delaware reported by Snee (1974)
 
```{r}
HairEye = margin.table(HairEyeColor, c(1, 2))
```

```{r echo=TRUE}
HairEye
```

*Rows* hair color of 592 statistics students   
*Columns* eye color of the same 592 statistics students

## Hair and Eye Color of Statistics Students  {.build}

```{r echo=TRUE}
chisq.test(HairEye)
```

## Association Plots

* This is a followup on $\chi^2$ tests when they are rejected
* Pearson residuals
$$r_{ij} = \frac{O_{ij}-E_{ij}}{\sqrt{E_{ij}}}$$
* $\chi^2$ statistics is squaring and summing over all cells
$$\chi^2 = \sum_j \sum_i r_{ij}^2$$

## Association Plots

* Each cell of contingency table is represented by a rectangle encoding information in width, height, location, and color of the rectangle
* *Height:* Proportional to Pearson residual $\frac{O_{ij}-E_{ij}}{\sqrt{E_{ij}}}$
* *Width:*  Proportional to $\sqrt{E_{ij}}$
* *Area:* Proportional to $O_{ij}-E_{ij}$
* *Baseline:* 
    * If $O_{ij} > E_{ij}$, then rectanble above
    * otherwise rectangle below
* *Color:* Standardized Pearson residuals that are asymptotically standard normal

## Association Plots

```{r message=FALSE,fig.height=6}
library(vcd); assoc(HairEye,shade = TRUE)
```

## Mosaic Plots

* Cell frequency $O_{ij}$, cell probability $p_{ij} = O_{ij} / O_{\cdot\cdot}$
* Take unit square
* Divide unit square into rectangles 
    * *Height* proportional to observed marginal frequencies $O_{i\cdot}$
    * which is proportional to marginal probabilities $p_i = O_{i\cdot} / O_{\cdot\cdot}$
* Subdivide resulting rectangles horizontally 
    * *Width* proportional to $p_{j|i} = O_{ij}/O_{i\cdot}$
    * which is the conditional probabilities of the second variable given the first
* *Area* is proportional to the observed cell frequency and probability
$p_{ij} = p_{i} \times p_{j|i} = ( O_{i\cdot}/O_{\cdot\cdot} ) \times ( O_{ij} / O_{i\cdot} )$

## Mosaic Plots

* The order of conditioning matters
* Placing explanatory variable(s) first shows how the response(s) depend on them
* *Color:* Standardized Pearson residuals that are asymptotically standard normal

## Mosaic Plots

Marginal probabilities of first variable

```{r echo=TRUE}
frequency = rowSums(HairEye)
proportions = frequency/sum(frequency)
data.frame(frequency=round(frequency,0),proportions=round(proportions,2))
```

## Mosaic Plots

Marginal probabilities of second given first variable

```{r echo=TRUE}
HairEye/rowSums(HairEye)
```

## Mosaic Plots

```{r fig.height=6.5}
mosaic(HairEye,shade = TRUE)
```

## Mosaic Plots

If hair color and eye color were independent $p_{ij} =p_i \times p_j$, then then the tiles in each row would all align

<div class="columns-2">

```{r fig.height=5.5}
mosaic(HairEye,shade = TRUE)
```

```{r fig.height=5.5}
mosaic(chisq.test(HairEye)$expected,shade = TRUE)
```

</div>

## Correspondence Analysis

* Actively developped in 1965 in France
* French school of data analysis: Geometry instead of probability
* Can be used to plot contingency tables for exploratory data analysis
* Decomposes the $\chi^2$ statistic into orthogonal factors
* To first approximation: extension Principal Components Analysis (PCA) by 
* replacing the variance in PCA with an inertia proportional to the $\chi^2$ distance of the table from independence
* decomposes this measure of departure from independence along axes that are orthogonal according to a $\chi^2$ inner product

## Correspondence Analysis

* Denote $m \times p$ contingency table as $N$
* Total number of observations $n = \sum_{i=1}^m \sum_{j=1}^p n_{ij}$
* Denote frequency table as $F = \frac{N}{n}$
* Vector of row sums of $F$: $\operatorname{diag}(D_r) = r = \frac{1}{n} N 1_p$
* Vector of column sums of $F$: $\operatorname{diag}(D_c) = c^T = \frac{1}{n} N^T 1_m$
* Peform Principle Component Analysis (PCA) on new data matrix
$$X = D_r^{-1} F D_c^{-1} − 1_m^T 1_p$$
* Generalized singular value decomposition on $X = USV^T$  
with constrains $U^T D_r U = I_m$ and $V^T D_c V = I_p$

## Correspondence Analysis

* Relationship to $\chi^2$ statistic
$$D_r X^T D_c X = \frac{\chi^2}{n}$$
* In standard PCA, principle components explain variance
* In CA, principle components explain devation from independence

## Correspondence Analysis

```{r}
library(ca)
ca.res = ca(HairEye); ca.res
```

## Correspondence Analysis

```{r fig.width=10, fig.height=5}
plot(ca.res)
```

Color code: <span style="color:blue">Hair</span>, <span style="color:red">Eye</span>
