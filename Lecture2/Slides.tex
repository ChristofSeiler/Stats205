\documentclass[10pt]{beamer}

\input ../pack.tex
\input ../defs.tex
\input ../form.tex


\title{\large \bfseries Stats 205: \\ Introduction to Nonparametric Statistics \linebreak \linebreak \linebreak
Lecture 2: \\ Enumerations}

\author{Instructor: Christof Seiler}

\date{Spring 2016}

\begin{document}

\frame{
\thispagestyle{empty}
\titlepage
}

\begin{frame}
\frametitle{Last Lecture}

In our last lecture, we saw two examples of nonparametric statistics in action. \newline

With a ranked-based method, we tested whether social awareness is improved when sending kids to school versus home schooling.
We assumed that the error was independent and identically distributed (iid) and symmetric around $0$ but made no futher shape assumptions. \newline

In a second example, we saw how to calculate a confidence interval around an estimate using the bootstrap procedure. We used this to evaluate whether aspirin helps reduce the risk of heart attacks in middle-aged men. 
Again, we did not make any assumption about the distribution the noise that corrupted the underlying parameter. 

\end{frame}

\begin{frame}
\frametitle{Today}

We will take a closer look at the computational tools that are needed to for both examples. In both example, we needed to somehow 
\begin{itemize}
\item describe all possible values that a parameter can take, 
\item compare this distribution to the observed value, and 
\item judge how probable it is to observe it. 
\end{itemize}

\vspace{0.3cm}
Most of the time it is not possible to enumerate all possible values, so we need ways to approximate it. Some ways are:
\begin{itemize}
\item Monte Carlo
\item Markov chain Monte Carlo
\end{itemize}

\vspace{0.3cm}
Today, to get a feeling for this, we focus on a special case where we can actually enumerate all options using Gray Codes.

\end{frame}

\begin{frame}
\frametitle{Gray Codes}

Gray codes are ordered lists of binary $n$-tuples. \newline

They are ordered so that success values only differ in a single space.

For instance, for $n = 3$, the list of is:
\[ 000, 001, 011, 010, 110, 111, 101, 100 \]

Notice, a computer scientist might intuitively want to write this:
\[ 000, 001, \cancel{010}, 011, \cancel{100}, 101, \cancel{110}, 111 \]
This is wrong. \newline

A better than trying to reorder the wrong elements, we can define recursive algorithm to generate a valid list.

\end{frame}

\begin{frame}
\frametitle{Gray Codes}

\begin{itemize}
\item Start with list for $n = 1$, which is just $0$ and $1$ 
\item Get two list by putting a zero before each entry and a one before each entry in $L_n$ 
\item To get $L_{n+1}$ concatenate the two list by first followed by second in reversed order 
\end{itemize}

\vspace{0.3cm}
So from $0,1$, we get two lists
\[ 00, 01 \]
\[ 10, 11 \]
and concatenate
\[  00, 01, 11, 10 \]
repeat...

\end{frame}

\begin{frame}
\frametitle{Gray Codes}

We can also just add one successor at the time. 
As we saw before, a computer scientist would encode integer an $m = \sum \epsilon_i 2^i$
using the binary sequence 
\[ \dots \, \epsilon_3 \, \epsilon_2 \, \epsilon_1 \, \epsilon_0 \]

This can be mapped to gray codes 
\[ \dots \, e_3 \, e_2 \, e_1 \, e_0 \]
using $e_i = \epsilon_i + \epsilon_{i+1} \, (\bmod 2) \text{ for } (i = 0,1,2,\dots)$ \newline

Example: 
When $n = 4$ the integer $6$ in binary code is $0110$ and using the mapping 
\[ e_0 = 0 + 1 = 1, e_1 = 1 + 1 = 0, e_2 = 1 + 0 = 1, e_3 = 0 + 0 = 0 \]
we get the corresponding gray code $0101$

\end{frame}

\begin{frame}
\frametitle{Gray Codes for the Bootstrap}

We observe data $\mathcal{X}_n = \{ x_1, x_2, \dots, x_n \}$ in some space $\mathcal{X}$ \newline

The data are drawn identically and independently from a unknown distribution $F$ \newline

The bootstrap supposes that the empirical distribution $F_n$ is a good description of the unknown distribution $F$ \newline

This way one can draw as many samples from $F_n$ to compute sample variability of all kinds of estimators, e.g. the sample mean \newline

This is done via choosing $n$ samples with replacement (you can pick the same sample multiple times)

\end{frame}

\begin{frame}
\frametitle{Gray Codes for the Bootstrap}

There are $n^n$ different bootstrap samples but some of them have the same subset: 
\begin{align}
\mathcal{X}_n^1 & = \{ x_1^1, x_2^1, \dots, x_n^1 \} \\
\mathcal{X}_n^{n^n} & = \{ x_1^{n^n}, x_2^{n^n}, \dots, x_n^{n^n} \}
\end{align}

We group the same bootstrap sample and assign a weight $k_i$ describing the number of times it occurs, so $k_1 + \dots + k_n = n$ \newline

Denote the space of compositions of $n$ into at most $n$ parts as
\[ 
\mathcal{C}_n = \{ \boldsymbol{k} = (k_1,\dots,k_n), k_1+\dots+k_n=n, k_i \ge 0, k_i \text{ integer} \}
\]
and the size of this space is $|\mathcal{C}_n| = \binom{2n-1 }{n-1}$ \newline

A uniform distribution on $\mathcal{X}_n^n$ induces a multinomial distribution on $\mathcal{C}_n$ with probability mass function $m_n(k)$
 
\end{frame}

\begin{frame}
\frametitle{Gray Codes for the Bootstrap}

To form an exhaustive bootstrap distribution of statistic $T(\mathcal{X}_n)$, \newline
we need to compute  
\begin{itemize}
\item $|\mathcal{C}_n| = \binom{2n-1 }{n-1}$ statistics and 
\item associated weights $m_n(k)$
\end{itemize}

\vspace{0.3cm}
The shift from $\mathcal{X}_n^n$ to $\mathcal{C}_n$ gives substantial savings \newline

For an example with $n = 15$, the number of enumerations reduce from $15^{15} \approx 4.38 \times 10^{17}$ to $\binom{29}{14} \approx 7.7 \times 10^7$ \newline

Next, we will see how efficient updating using Gray Codes avoids multiplying such large numbers by factors of $n$ \newline

\alert{R markdown session:} But before, we look at an example about the correlation between GPA and LSAT for a sample of American law schools

\end{frame}

\begin{frame}
\frametitle{Gray Codes for the Bootstrap}

\end{frame}

\begin{frame}
\frametitle{Next Lecture}

In the next lecture, we will focus on
\begin{itemize}
\item rank-based methods
\end{itemize}

\end{frame}

\end{document}
