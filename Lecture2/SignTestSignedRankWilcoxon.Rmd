---
title: "Sign Test and Signed-Rank Wilcoxon"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output:
  ioslides_presentation:
    incremental: yes
    smaller: no
    transition: faster
    widescreen: yes
  beamer_presentation:
    incremental: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Location Model

* $X_1,X_2,\dots,X_n$ denotes random sample following the model
$$ X_i = \theta + e_i, $$
random errors $e_i$ are independently and identically distributed with a continuous probability density function $f(t)$ symmetric around $0$.

* Hypothesis testing:
$$ H_0: \theta = 0 \text{ versus } H_A: \theta > 0 $$

* A *null hypothesis* is associated with a **contradiction** to a theory one would like to prove
* An *alternative hypothesis* is associated with the actual theory one would like to prove

## Sign Test

* To every statistical test such as the sign test, we associate a *test statistic*
* A test *statistic* is a function of the data

* The sign test has test statistic: 
$$ S = \sum_{i=1}^n \operatorname{sign}(X_i) $$
with $\operatorname{sign}(t) = 
\begin{cases}
-1 & \text{if } t < 0 \\
0 & \text{if } t = 0 \\
1 & \text{if } t > 0
\end{cases}$

## Sign Test

* Looking just at the positive observations: 
$$ S^+ = \#_i \{ X_i > 0 \} $$
* $0$ observations are ignored and sample size is reduced
* If observing a $-1$ or $1$ for each $X_i$ is a coin flip then
* $S^+$ follows a binomial distribution with $n$ trials and success probability $1/2$
* This is the *null distribution* of the test statistic
* It doesn't depend on the error distribution
* This property is called *distribution-free*
* Compare null distribution with observed test statistic

## Sign Test Example

* Comparison of ice cream brands A and B
* Theory: Brand A is preferred over B
* Null hypothsis: No difference between A and B
* Alternative: A is preferred over B

* Blindfolded taster gives preference of one ice cream over the other
* Experiment with 12 taster
    + 10 tasters prefer brand A over brand B
    + 1 taster prefers brand B over A
    + 1 no preference
* Pretty convincing evidence in favor of brand A
* But how likely is such a result due to chance if the null hypothesis is true (no preference in brands)?


## Sign Test Example

* For our sign test, the test statistic $S^+$ is the number positive signs
* Evaluated on our data the observed test statistic is $s^+ = 10$
* We compare the observed test statistic with the binomial distribution with $n = 11$ trials and success probabilty $1/2$

## Sign Test Example

The $p$-value is $P_{H_0}(S^+ \ge s^+) = 1 − F_B(s^+ − 1,n,\frac{1}{2})$

```{r echo=FALSE}
S = 10; n = 11; p = 1/2; pvalue = sum(dbinom(x = S:n,size = n,prob = p)); pvalue
```

```{r}
x = seq(0,n,1)
colors = rep("white",length(x))
colors[x >= S] = "red"
barplot(dbinom(x,n,1/2),
        names.arg = x,
        main = paste0("dbinom(x,n = ", n, ",p = ",p,")"),
        col = colors)
```

## Sign Test Example

Significance *level*: the probabilty of rejecting the null when it is **true**

We can reject the null hypothesis at significance level $\alpha = 0.05$ in favor of the alternative that Brand A is tastier than Brand B.

## Traditional $t$-Test

* The $t$-test depends on the population pdf of $f(t)$ 
* Thus it is not distribution free
* The usual test statistic is 
$$ t = \frac{\bar{X}}{s/\sqrt{n}} $$
here $\bar{X}$ is the mean and $s$ is the standard deviation of the sample
* Usually $s$ is not known and has to be estimated, then if the population is normal $t$ has a Student $t$-distribution with $n-1$ degress of freedom
* And the $p$-value $P_{H_0}(t \ge t_0) = 1 - F_T(t_0,n-1)$

## Signed-Rank Wilcoxon Test

* In contrast to the $t$-test, the sign-rank Wilcoxon test uses only the ranks of the distance from $0$
$$ W = \sum_{i>0}^n \operatorname{sign}(X_i) \operatorname{R} |X_i| $$
* No closed-form, iterative algorithms, e.g. ``psignrank`` in R
* Our test statistics considers positive items only
$$ W^+ = \sum_{X_i > 0} \operatorname{R} |X_i| = \frac{1}{2} W + \frac{n(n+1)}{4} $$
* The distribution is symmetric around $\frac{n(n+1)}{4}$

## Signed-Rank Wilcoxon Test

The distribution of test statistic cannot be obtained in closed-form.

Enumerate all possible outcomes for sample size three $n = 3$:

## Signed-Rank Wilcoxon Test

The distribution of test statistic cannot be obtained in closed-form.

Enumerate all $2^n$ possible outcomes for sample size three $n = 3$:

```{r message=FALSE}
library(R.utils)
sign = intToBin(0:7)
sign = sapply(sign,function(i) c(substr(i,1,1),substr(i,2,2),substr(i,3,3)) )
sign = t(matrix(as.integer(sign),nrow = 3))
rank = t(matrix(rep(1:3,8),nrow = 3))
outcome = data.frame(sign=sign,SumRankPlus=NaN)
outcome
```

## Signed-Rank Wilcoxon Test

```{r echo = TRUE}
SumRankPlus = apply(sign*rank,1,sum)
outcome$SumRankPlus[1] = SumRankPlus[1]; outcome
```

## Signed-Rank Wilcoxon Test

```{r echo = TRUE}
outcome$SumRankPlus[2] = SumRankPlus[2]
outcome
```

## Signed-Rank Wilcoxon Test

```{r echo = TRUE}
outcome$SumRankPlus = SumRankPlus; outcome
table(outcome$SumRankPlus)
```

## Signed-Rank Wilcoxon Test

Or through Monte Carlo simulations:

```{r echo = TRUE}
n = 3; nsim = 10000
X = matrix(rnorm(n*nsim),ncol=n)
WplusSim = apply(X,1, function(x) { sum( rank(abs(x)) * (x>0) ) })
table(WplusSim)/nsim
```

and compare with theoretical result:

```{r echo=FALSE}
prop = c(1/8,1/8,1/8,2/8,1/8,1/8,1/8)
names(prop) = 0:6
prop
```

## Signed-Rank Wilcoxon Test Example

* Comparison of social awareness in kids going to school versus home schooling
* Theory: Sending kids to school improves social awareness
* Null hypothsis: No difference between kids going to school and staying at home
* Alternative: Kids going to school have improved social awareness

* Eight pairs of identical twins who are of nursery school age. 
* For each pair, one is randomly selected to attend nursery school while the other remains at home.  
* At the end of the study period, all 16 children are given the same social awareness test.

## Signed-Rank Wilcoxon Test Example

The statistic is the sum of the ranks of positive items:

```{r echo=TRUE}
school = c(82,69,73,43,58,56,76,65); home = c(63,42,74,37,51,43,80,62)
response = school - home
response
rank(abs(response))
```

## Signed-Rank Wilcoxon Test Example

Calculate ranks of absolute values:

```{r echo=TRUE}
response
rank(abs(response))
```

The statistic is the sum of the ranks of positive items:

```{r}
wplus = sum((response>0)*rank(abs(response)))
wplus
```

## Signed-Rank Wilcoxon Test Example

```{r echo=FALSE}
n = length(response)
x = seq(0,n*(n+1)/2,1)
colors = rep("white",length(x))
colors[x >= wplus] = "red"
barplot(dsignrank(x,n),names.arg = x,main = paste0("dsignrank(x, n = ", n, ")"),col = colors)
```

## Signed-Rank Wilcoxon Test Example

and using the cumulative distribution function $F_{W^+}(w^+-1,n)$ of our test statistic:

```{r}
plot(x,psignrank(x, n = n),type = "s",main = paste0("psignrank(x, n = ", n, ")"))
abline(v=wplus,col = "red"); abline(h=psignrank(wplus,n),col = "blue")
```

## Signed-Rank Wilcoxon Test Example

We can find the $p$-value using $P_{H_0}(W^+ \ge w^+) = 1 - F_{W^+}(w^+ - 1,n)$:

```{r echo=TRUE}
pvalue = 1-psignrank(wplus-1,n,lower.tail = TRUE)
pvalue
```

## Signed-Rank Wilcoxon Test Example

Significance *level*: the probabilty of rejecting the null when it is **true**

We can reject the null hypothesis at significance level $\alpha = 0.05$ in favor of the alternative that sending kids to school improves their social awareness.

## Estimation and Confidence Intervals

* All three tests (sign test, signed-rank Wilcoxon, and $t$-test) have an associated estimate and confidence interval for the location parameter $\theta$
* Recall the model for our sample $X_1,X_2,\dots,X_n$ 
$$ X_i = \theta_i + e_i $$

* Some definitions:
* *order statistics*: $X_{(1)}$ minimum observation, $X_{(n)}$ largest observation
* in increasing order $X_{(1)} < X_{(2)} < \dots < X_{(n)}$
* *quantiles*: equally spaced splitting points of continuous intervals with equal probabalities or sample

## Estimation and Confidence Intervals of Sign Test

Estimator is the median:
$$ \hat{\theta} = \operatorname{median}\{ X_1,X_2,\dots,X_n \} $$

Confidence interval $(1−\alpha)100\%$:
$$ \left(X_{(c_1+1)},X_{(n−c_1)} \right), $$ 
where $c_1$ is the $\frac{\alpha}{2}$ quantile of the binomial distribution

## Estimation and Confidence Intervals of Sign-Rank Wilcoxon

Hodges–Lehmann estimator:
$$ \hat{\theta} = \operatorname{median}_{i\le j} \left\{ \frac{X_i+X_j}{2} \right\} $$

$A_{ij} = \frac{(X_i +X_j)}{2}, i \le j$ are called the Walsh averages 

and used to form confidence interval $(1−\alpha)100\%$:
$$ \left(A_{(c_2 +1)},A_{([n(n+1)/2]−c_2 )} \right), $$
$c_2$ is the $\frac{\alpha}{2}$ quantile of the signed-rank Wilcoxon distribution

## How Are Walsh Averages and Wilcoxon Related?

* Sketch of proof of equality $W^+ = A^+$ by induction 
    * Wilcoxon test statistic: $W^+ = \sum_{i>0} \operatorname{R} |X_i|$ and
    * number of Walsh averages greater than $\theta$: $A^+ = \#\{ A_{ij} > \theta \}$  

* *Base of the Induction:*
    * **(A)** Assume $\theta$ is greater than all $X_1,X_2,\dots,X_n$  
        * hence greater than all Walsh averages  
        * hence $X_i - \theta$ are all negative
    1. Under **(A)** all ranks are negative, hence $W^+ = 0$
    2. Under **(A)** all of Walsh average are less than $\theta$, hence $A^+ = 0$

## How Are Walsh Averages and Wilcoxon Related?

* *Induction Steps:*
    * Move $\theta$ to the left passing through $X_1,X_2,\dots,X_n$ one and two at the time and show that 
    * $A^+$ changes value when moving past an Walsh average by the same amount
    * $W^+$ changes value when 
        * ranks of some $|X_i - \theta|$ change or
        * sign of some rank change by the same amount

* The full proof is [here](http://www.stat.umn.edu/geyer/s06/5601/theo/wilcox.pdf)

## Some Comparisons

* *Power* of a statistical test:  
the probability of rejection the null hypothesis when it is **false**
* The power of the sign test can be low relative to $t$-test
* The power of signed-rank Wilcoxon test is nearly that of the $t$-test for normal distributions and generally greater than that of the $t$-test for distributions with heavier tails than the normal distribution

## Power Simulation $\theta = 0$

```{r echo=TRUE}
n = 30; df = 2; nsims = 10000; mu = 0; collwil = rep(0,nsims); collt = rep(0,nsims)
for(i in 1:nsims) {
  x = rt(n,df) + mu
  wil = wilcox.test(x) 
  collwil[i] = wil$p.value 
  ttest = t.test(x) 
  collt[i] = ttest$p.value
}
powwil = rep(0,nsims); powwil[collwil <= .05] = 1; powerwil = sum(powwil)/nsims
powt = rep(0,nsims); powt[collt <= .05] = 1; powert = sum(powt)/nsims
powerwil
powert
```

## Power Simulation $\theta = 0.5$

```{r echo=TRUE}
n = 30; df = 2; nsims = 10000; mu = 0.5; collwil = rep(0,nsims); collt = rep(0,nsims)
for(i in 1:nsims) {
  x = rt(n,df) + mu
  wil = wilcox.test(x) 
  collwil[i] = wil$p.value 
  ttest = t.test(x) 
  collt[i] = ttest$p.value
}
powwil = rep(0,nsims); powwil[collwil <= .05] = 1; powerwil = sum(powwil)/nsims
powt = rep(0,nsims); powt[collt <= .05] = 1; powert = sum(powt)/nsims
powerwil
powert
```

## Power Simulation $\theta = 1$

```{r echo=TRUE}
n = 30; df = 2; nsims = 10000; mu = 1; collwil = rep(0,nsims); collt = rep(0,nsims)
for(i in 1:nsims) {
  x = rt(n,df) + mu
  wil = wilcox.test(x) 
  collwil[i] = wil$p.value 
  ttest = t.test(x) 
  collt[i] = ttest$p.value
}
powwil = rep(0,nsims); powwil[collwil <= .05] = 1; powerwil = sum(powwil)/nsims
powt = rep(0,nsims); powt[collt <= .05] = 1; powert = sum(powt)/nsims
powerwil
powert
```

## Summary

Assumptions on $f(t)$:

* Sign Test: any continuous distribution
* Signed-Rank Test: any symmetric continuous distribution
* $t$-test: any normal distribution

* The continuity assumption assures that ties are impossible: With probability one we have $X_i \ne X_j$ when $i \ne j$
* The continuity assumption is only necessary for exact hypothesis tests not for estimates and confidence intervals

Assumptions on $e_i,e_2,\dots,e_n$ and thus $X_1,X_2,\dots,X_n$:

* independent and identically distributed
