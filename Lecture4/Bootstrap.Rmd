---
title: "The Bootstrap"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
#output: html_document
output:
  ioslides_presentation:
    incremental: yes
    smaller: no
    transition: faster
    widescreen: yes
  beamer_presentation:
    incremental: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
fastMode = TRUE
```

## What Happened So Far

* We have samples $x_1,x_2,\dots,x_n$ from distribution $F$
* We want to compute functionals $\theta = T(F)$, e.g. 
    * the mean: $T(F) = \int x \, dF(x) = \int x \, f(x) \, dx$
    * the median: $T(F) = F^{-1}(1/2)$ 
* Parameter of interest $\theta$ is functional of unkown $F$
* We tested $\theta = \theta_0$ from a sample $\boldsymbol{x} = \{ x_1,x_2,\dots,x_n \}$ by comparing the observed statistic $T(\boldsymbol{x})$ to the null distribution of test statistic
* We estimated $\theta$ and confidence intervals from a sample and quantified robustness to outliers

## Background

* Computer more powerful, resampling procedure more widespread
* The bootstrap is a general tool to measure error in an estimate or significance in of a test hypothesis
* Context for sampling distribution of a statistic
* A bootstrap sample is a sample from the original sample taken (with replacement)
* This works when the histogram of the sample is representative of the population
* In other words, the histogram of the sample resembles the pdf of the random variable
* Drawing sample from the histogram is then the same as drawing samples from the population
* Thus yields an estimate of the sampling distribution of the statistic

## Plug-in Principle

* We observe data $\mathcal{X}_n = \{ x_1, x_2, \dots, x_n \}$ in some space $\mathcal{X}$ 
* The data are drawn independently and identically from a unknown distribution $F$
* The bootstrap supposes that the empirical distribution $F_n$ is a good description of the unknown distribution $F$
* This way one can draw as many samples from $F_n$ to compute sample variability of all kinds of \textit{statistics} $T(x_1, x_2, \dots, x_n)$, e.g. the sample mean, sample median
* This is done via choosing $n$ samples with replacement (you can pick the same sample multiple times) 

## Plug-in Principle

* We observe the empirical distribution $F_n$
$$ F_n(t) = \frac{1}{n} \sum_{i=1}^n I(x_i \le t) $$
* In other words, the distribution that puts mass $1/n$ at each $x_i$
* Denote bootstrap sample from $F_n$ (sample with replacement) as 
$$\boldsymbol{x}^* = \left[ x_1^*,x_2^*,\dots,x_n^* \right]^T$$
* Estimate based on original observations $\widehat{\theta}^* = T(\boldsymbol{x}^*)$
* Denote a collection of $B$ bootstrap samples as $\widehat{\theta}^*_1,\widehat{\theta}^*_2,\dots,\widehat{\theta}^*_B$

## Percentile Confidence Intervals

* Order bootstrap samples: $\widehat{\theta}^*_{(1)} \le \widehat{\theta}^*_{(2)} \le \dots \le \widehat{\theta}^*_{(B)}$
* Let $m = \alpha/2 \times B$ then
* we get the approximate $(1-\alpha) \times 100\%$ confidence interval 
$$\left( \widehat{\theta}^*_{(m)},\widehat{\theta}^*_{(B-m)} \right)$$

## Paired Problem

* Consider the paired problem with $d_1,\dots,d_n$ are the differences
* In the bootstrap sample $d_i$ and $-d_i$ each have probability $1/2$ of being selected
* This forms an estimate of the null distribution of the test statistic $T$
* Then
$$\text{p-value } = \frac{\#\{T_i^*\ge T\}}{B}$$

## One Sample Location Problem

* Test the hypothesis:
$$ H_0: \theta = \theta_0 \text{ versus } \theta > \theta_0 $$
* We have to make sure that the null hypothesis is true, so we take our bootstrap samples from
$$x_1-\widehat{\theta}+\theta_0,\dots,x_n-\widehat{\theta}+\theta_0$$
* Then
$$\text{p-value } = \frac{\#\{\widehat{\theta}_i^*\ge \widehat{\theta}\}}{B}$$

## Two Sources of Errors

* Sampling variability:   
We only have a sample of size $n$ and not the entire population
* Bootstrap resampling variability:   
We only use $B$ bootstrap samples rather than an infinite number

----

```{r, out.height = 600, fig.retina = NULL, echo=FALSE}
knitr::include_graphics("BootstrapError.JPG")
```

## When It Fails

* Maximum order statistic of a random sample from a $U(0,\theta)$ distribution
* $\theta$ is unkown and we want to estimate using the boostrap

[link to stackexchange](!http://stats.stackexchange.com/questions/9664/what-are-examples-where-a-naive-bootstrap-fails)

## Complete Enumerations

* There are $n^n$ different bootstrap samples but some of them have the same subset: 
$$
\begin{align}
\mathcal{X}_n^1 & = \{ x_1^1, x_2^1, \dots, x_n^1 \} \\
 & \vdots \\
\mathcal{X}_n^{n^n} & = \{ x_1^{n^n}, x_2^{n^n}, \dots, x_n^{n^n} \}
\end{align}
$$
* We group the same bootstrap sample and assign a weight $k_i$ describing the number of times it occurs, so $k_1 + \dots + k_n = n$
* Denote the space of compositions of $n$ into at most $n$ parts as
$$\mathcal{C}_n = \{ \boldsymbol{k} = (k_1,\dots,k_n), k_1+\dots+k_n=n, k_i \ge 0, k_i \text{ integer} \}$$
* and the size of this space is $|\mathcal{C}_n| = \binom{2n-1 }{n-1}$ 

## Complete Enumerations

* Why $|\mathcal{C}_n| = \binom{2n-1 }{n-1}$? 
* Place $n-1$ bars inbetween $n$ balls, example $n=3$, for example
* ``` 0|0|0 ``` corresponse to $x_1,x_2,x_3$
* ``` 000|| ``` corresponse to $x_1,x_1,x_1$
* The space of compositions can be represented as a simplex (bonus material later)

## Complete Enumerations

* A uniform distribution on $\mathcal{X}_n^n$ induces a multinomial distribution on $\mathcal{C}_n$ with probability mass function $m_n(k)$ 
* To form an exhaustive bootstrap distribution of statistic $T(\mathcal{X}_n)$, we need to compute  
    * $|\mathcal{C}_n| = \binom{2n-1}{n-1}$ statistics and 
    * associated weights $m_n(k)$
* The shift from $\mathcal{X}_n^n$ to $\mathcal{C}_n$ gives substantial savings 
* For an example with $n = 15$, the number of enumerations reduce from  
$15^{15} \approx 4.38 \times 10^{17}$ to $\binom{29}{14} \approx 7.7 \times 10^7$

## Law Schools Example

```{r echo = TRUE}
library(bootstrap); data(law); t(law)
```

## Law Schools Example

```{r echo=FALSE}
library(ggplot2)
law2 = data.frame(Observation = 1:dim(law)[1],law)
ggplot(law2, aes(x = LSAT, y = GPA)) + 
  geom_text(aes(label = Observation),hjust = 0,vjust = 0)
```

## Law Schools Example

Sample correlation coefficient:

```{r}
theta.hat = cor(law$LSAT,law$GPA)
theta.hat
```

How accurate is this estimate. Let's look at the bootstrap samples:

```{r echo=TRUE}
draw.bootstrap.sample = function() {
  n = dim(law)[1]
  ind = sample(n,replace = TRUE)
  return(cor(law[ind,]$LSAT,law[ind,]$GPA))
}
nrep = 40000
thetastar = replicate(nrep,draw.bootstrap.sample())
thetastar2 = data.frame(cor=thetastar)
```

## Law Schools Example

```{r}
ggplot(thetastar2, aes(cor)) + 
  geom_histogram(binwidth = 0.01) +
  geom_vline(xintercept = theta.hat,colour = "red",size = 1.5)
```

## Law Schools Example

Evaluate the correlation coefficient for all $\binom{2n-1}{n-1}$ enumeration. 
```{r echo=FALSE, warning=FALSE, eval=TRUE}
library(partitions)
library(assertthat)
library(parallel)

if(!file.exists("enumData.Rdata")) {
  n = 15
  allCompositions = compositions(n,n)
  nCompositions = dim(allCompositions)[2]
  nCompositions
  # Check if we get the same value as in theory
  nCompositionsTheory = choose(2*n-1,n-1); 
  if(!assert_that(nCompositions == nCompositionsTheory))
    print("there is a problem with the number of compositions")
  ptm = proc.time()
  enumData = mclapply(1:nCompositions, function(i) {
    ind = allCompositions[,i]
    law.list = lapply(1:n,function(j) matrix(rep(law[j,],ind[j]),ncol = 2 ,byrow = TRUE))
    newLaw = do.call(rbind, law.list)
    c(cor(unlist(newLaw[,1]),unlist(newLaw[,2])),dmultinom(ind,prob = rep(1,n)))
    },
    mc.cores = 4
    )
  proc.time() - ptm
  enumData = t(simplify2array(enumData))
  colnames(enumData) = c("cor","weight")
  save(enumData,file = "enumData.Rdata")  
} else {
  load("enumData.Rdata")
}
enumDataFrame = data.frame(enumData)
if(!fastMode) {
  ggplot(enumDataFrame, aes(cor, weight = weight)) + 
    geom_histogram(binwidth = 0.003) +
    geom_vline(xintercept = theta.hat,colour = "red",size = 1.5)
}
```

## Law Schools Example

Use the library parallel to speedup computations. 

```{r eval=FALSE,echo=TRUE}
  library(parallel)
  enumData = mclapply(1:nCompositions, function(i) { ... }, mc.cores = 4 )
```

On a laptop with 4 cores this takes about 5 hours to compute.  

We can speedup enumeration by changing only two coordinates at the time using Gray codes.

## Geometry of the Bootstrap

Efron & Tibshirani Chapter 20

## Theoretical Underpinnings of the Bootstrap

For background on statistical functionals and asymptotics of the boostrap:

* Lehmann 1999   
*Elements of Large-Sample Theory*   
Chapter 6   
[Stanford library link](!https://searchworks.stanford.edu/view/4010023)
* Van der Vaart 1998   
*Asymptotic statistics*   
Chapter 23   
[Stanford library link](!https://searchworks.stanford.edu/view/4109687)

