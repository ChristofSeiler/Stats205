\documentclass{article}
\setlength{\topmargin}{-3cm}
\setlength{\textwidth}{18cm}
\setlength{\textheight}{26cm}
\setlength{\oddsidemargin}{-1cm}
\setlength{\evensidemargin}{-1cm}
\setlength{\parindent}{0cm}
 \usepackage{fontspec}
\usepackage{xltxtra}
\setmainfont[Mapping=tex-text]{Chalkboard}
\setmonofont{Courier}
\setsansfont{Courier}

%\title{The Bootstrap }
%\author{Susan Holmes\\
%\maketitle
%\usepackage{concrete}
\usepackage{graphicx}
\begin{document}
%\tableofcontents
\section*{\sf Nonparametric Tests \hfill Stats208, Spring, 2009}
\subsection*{\sf Comparing two distances}
Suppose we have two distance matrices  $A$ and $B$ 
beteen the same entities computed
in two different ways.
This statistic was used by
F.N. David for comparing disease events
to see if they were `signs' of epidemics,
by Henry Daniels, 1944, Biometrika,
and by Knox.

Recently the 
permutation test
based on thsi statistic
has been renamed
`Mantel's test' by the
ecological community
(Manly 1992).


The statistic we look at is
$\sum_{i<j} a_{ij} b_{ij}$ Which gives
an indication of their correlation.

In fact we  remark that if each distance matrix
were vectorized, this is the equivalent of an
uncentered
covariance between the vectors.

If the two distances are small in the same places,
and big in the same places the statistic will be large
indicating a strong link between the distances.

In order to actually test the significance of such a link,
a permutation test is performed: 
the null distribution
is obtained by permuting the elements
of $A$ to obtain what is denoted by $\Pi A$
, keeping the matrix symmetric, and
recomputing
the statistic:
$$\sum_{i<j} (\pi a)_{ij} b_{ij}$$

The R progam enabling the 
implementation of this algorithm can be
found below.

What will be useful to us here
is the idea of a vectorial covariance 
between distance matrices, which we will
apply to the special case of correlation
matrices which are in fact similarities.

{\small
\begin{verbatim}
daniel=function(mata, matb, nperm){
        if(ncol(mata) != ncol(matb))
                stop("matrices should have same dimension")
        if(nrow(mata) != nrow(matb))
                stop("matrices should have same dimension")
        r0 <- stat(mata, matb)
        r <- matrix(0, nperm, 1)
        for(i in (1:nperm)) {
                matp <- permute(mata)
                r[i] <- stat(matp, matb)
        }
        p <- as.numeric(r < r0)/(nperm + 1)
        return(p, r0, r)
}
> permute=function(mata){
        ap <- matrix(0, nrow(mata), nrow(mata))
        ni <- sample(nrow(mata))
        for(i in 1:nrow(mata)) {
                for(j in 1:(i - 1)) {
                        ap[i, j] <- mata[ni[i], ni[j]]
                }
        }
        return(ap)
}
> stat<-
function(mata, matb)
{
        r0 <- 0
        for(i in 2:nrow(mata))
                for(j in (1:(i - 1))) {
                        r0 <- r0 + mata[i, j] * matb[i, j]
                }
        return(r0)
}
\end{verbatim}
}
\subsection*{Wald-Wolfowitz Test on the Line}
Pool the data, rank the data , count the number of
`runs', sequences of  observations
that are from the same sample and follow each other.

More precisely, suppose we have two sample of observations,
we will call them the $X$'s and the $Y$'s.

Null hypothesis $F_X=F_Y$.

Test statistic is the number of runs $R$, that is the
number of consecutive sequences of identical labels.
The null distribution of the test statistic
can be found by a combinatorial argument:
$$W=\frac{R-\frac{2mn}{N}-1}{\sqrt{\frac{2mn(2mn-N)}{N^2(N-1)}}} \sim {\mathcal N}(0,1)$$
as long as the ratio of the sample sizes stays a number.
(bounded away from zero as they increase.)
\subsection*{Smirnov Test on the line}
Sort the univariate observations, $r_i$ is the number of
$X$'s that that have a rank smaller or equal
to $i$.$s_i$ is the same but for the $Y_i$'s.
The statistic here is $$D=max|d_i|\mbox{ where }d_i=
\frac{r_i}{m}-\frac{s_i}{n}
$$
Reject for large values of $D$
\subsection*{Minimal Spanning Tree Based Tests}
This method is inspired by the Wald-Wolfowitz, and
solves the problem that there is no
natural multidimensional `ordering'.

The main reference is 
Friedman and Rafsky 1979, Ann stat.,pp.  697-717.

One may either use multidimensional data
or
first make the data bidimensionnal by principal components.
(or another dimension reducing technique),
in order to be able to represent the tree
in a graphic.
\subsubsection*{ Minimal Spanning Tree Algorithm}
Not the travelling salesman problem.
Algorithms include Prim's, Kruskals', 
I have implemented
the greedy algorithm which is as follows:
(it is a recursive algorithm)
$T_0$ is a point.
\begin{itemize}
\item Suppose we have $T_{i}$, add an edge to it
by finding the minimal edge between a point
in the tree and a point that is not, join these
to make $T_{i+1}$.

Algorithm:\\
\begin{enumerate}
\item Set $tree(i)\; <-\; (-n)$, for $i =1:n-1$ 
\item Do $n-1$ times:
\begin{itemize}
\item $dmin= min\{ dist(i, |tree(i)|)\; for\; i <n\; with\; tree(i)<0\}$
\item $i_{min}=$ index at which the minimum is attained.
\item Add edge : $tree(i_{min}) <- (-tree(i_{min}))$
\item Update list of nearest vertices: for $i=1:n-1$ do:
\begin{itemize}
\item if $tree(i)<0$ and $dist(i,i_{min})< dist(i,-tree(i))$\\
$tree(i)\; <-\; -i_{min}$
\end{itemize}
\end{itemize}
\end{enumerate}
\end{itemize}

The output to create will be a vector of numbers
that correspond to where the $i$th point should be connected
to.

Here are some examples with R:
{\small
\begin{verbatim}
mstree(vincr[1:10,  ])
> mstree(vincr[1:10,  ])
$x:
 [1]  4.20959759 -3.70445704  4.54166603  7.22903347  4.89603710  2.28360796
 [7]  0.00000000 -2.00589728  1.03156424  0.04926162
$y:
 [1] -7.0463328 -6.4673190  0.4050449 -0.3888339 -2.2247181  0.0000000
 [7]  0.0000000 -3.2911484 -0.8079203 -1.5704904
$mst:
[1] 10  8  6  3  3  7 10 10 10
mst:   vector of length nrow(x)-1 describing  the  edges  in  the
       minimal spanning tree.  The ith value in this vector is an
       observation number, indicating that this  observation  and
       the  ith observation should be linked in the minimal span-
       ning tree.
$order:
      [,1] [,2] 
 [1,]    4    7
 [2,]    3   10
 [3,]    5    6
 [4,]    6    9
 [5,]    7    8
 [6,]   10    3
 [7,]    1    1
 [8,]    9    4
 [9,]    8    5
[10,]    2    2
order:    matrix, of size nrow(x) by 2, giving two types of  ord-
       ering:  The  first  column  presents the standard ordering
       from one extreme of the minimal spanning tree to the  oth-
       er.  This  starts on one end of a diameter and numbers the
       points in a certain order so that points close together in
       Euclidean  space  tend  to  be close in the sequence.  The
       second column presents the radial ordering, based on  dis-
       tance from the center of the minimal spanning tree.  These
       can be used to detect clustering.   See  below  for  graph
       theory definitions.
\end{verbatim}
}
Example with R (mstree is not contained in R)
{\small
\begin{verbatim}
plot(x, y)   # plot original data
library(ape)
xy.mst <- mst(cbind(x, y), plane=F)   # minimal spanning tree
          # show tree on plot
segments(x[seq(mst)], y[seq(mst)], x[mst], y[mst])
 
 i <- rbind(iris[,,1], iris[,,2], iris[,,3])
       tree <- mst(dist(i))    # multivariate planing
       plot(tree, type="n")  # plot data in plane
 text(tree, label=c(rep(1, 50), rep(2, 50), rep(3, 50))) # identify points

                # get the absolute value stress e
       disti <- dist(i)
       dist2 <- dist(cbind(tree$x, tree$y))
       e <- sum(abs(disti - dist2))/sum(distp)
 0.1464094
\end{verbatim}
}
\subsubsection*{Two-sample test}
\begin{itemize}
\item Pool the two samples together.
\item Make a minimal spanning tree.
\item Count the number of 'pure' edges,
ie edges of the minimal spanning tree whose
vertices come from the same sample. 
\item An equivalent statistic is 
provided
by taking
out all the edges that have mixed colors
and counting how many `separate' trees remain.
\end{itemize}



This extends to the case of more than two levels
of treatment by doing the following:
\subsubsection*{Many -sample test}
\begin{itemize}
\item Pool the $K$ samples together.
\item Make a minimal spanning tree.
\item Count the number of 'pure' edges,
ie edges of the minimal spanning tree whose
vertices come from the same treatment level. 
\end{itemize}



{\bf Example: Wine data}
As you may remember, this data was 14 physico-chemical
composition varaibles (all continuous), that we are
trying to relate to several categorical variables.
{\small
\begin{verbatim}
acp.vin <- princomp(vin[, 1:14])
#Two components was the choice here
plot(acp.vin$scores[,1:2])
 plot(acp.vin$scores, type = "n")
text(acp.vin$scores[as.numeric(vin[, 15]) == 1,  ], "1")
text(acp.vin$scores[as.numeric(vin[, 15]) == 2,  ], "2")
text(acp.vin$scores[as.numeric(vin[, 15]) == 3,  ], "3")
mst.wine <- mst(dist(acp.vin$scores))
plot(mst.wine,x1=acp.vin$scores[,1],x2=acp.vin$scores[,2])
title("Wines Minimal Spanning Tree")
neighbors=which(mst.wine==1,arr.ind=TRUE)
types=cbind(vin[neighbors[,1],15],vin[neighbors[,2],15])
sum(as.numeric(types[,2]-types[,1]==0))
[1] 72
sum(wine[neighbors[,1],15]==wine[neighbors[,2],15])
 sum(vin[neighbors[sample(154,154,F),1],15]==vin[neighbors[,2],15])
[1] 47
 sum(vin[neighbors[sample(154,154,F),1],15]==vin[neighbors[,2],15])
[1] 53
 sum(vin[neighbors[sample(154,154,F),1],15]==vin[neighbors[,2],15])
[1] 44
 sum(vin[neighbors[sample(154,154,F),1],15]==vin[neighbors[,2],15])
[1] 49
 for (k in (1:1000)){
 null0[k]=sum(vin[neighbors[sample(154,154,F),1],15]==vin[neighbors[,2],15])
 }
\end{verbatim}
}
\begin{minipage}{3in}
\includegraphics[width=3in]{/Users/susan/courses/s227/mst.pdf}
\end{minipage}
\begin{minipage}{3in}
\includegraphics[width=3in]{/Users/susan/courses/s227/null0.pdf}
\end{minipage}
\subsubsection*{Exact tests}
{\tt exactRankTests} is a package which contains many `exact' procedure
which enumerate all possibilities to get the p-values instead of using Monte Carlo.
\begin{verbatim}
x<- c(0.5, 0.5, 0.6, 0.6, 0.7, 0.8, 0.9)
y <- c(0.5, 1.0, 1.2, 1.2, 1.4, 1.5, 1.9, 2.0)
r <- cscores(c(x,y), type="Wilcoxon")
pperm(sum(r[seq(along=x)]), r, 7)
 pperm(sum(r[seq(along=x)]), r, 7)
[1] 0.004351204
\end{verbatim}
\end{document}


