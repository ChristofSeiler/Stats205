---
title: "Multivariate Rank Tests"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, Stats 205"
output:
  beamer_presentation:
    incremental: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(1234)
```

## Introduction

* We saw many rank-based hypothesis tests for univariate observations
* There is a generalizing to the mulitvariate case
* It is based on **depth functions**
* One example is the Tukey depth

## Some Material to Cover

wald-wolfowitz runs test

friedman rafsky test

minimal spanning tree

## Comparing two distances

* Suppose we have two distance matrices  $A$ and $B$ 
beteen the same entities computed
in two different ways.
This statistic was used by
F.N. David for comparing disease events
to see if they were `signs' of epidemics,
by Henry Daniels, 1944, Biometrika,
and by Knox.
* Recently the 
permutation test
based on thsi statistic
has been renamed
`Mantel's test' by the
ecological community
(Manly 1992).
* The statistic we look at is
$\sum_{i<j} a_{ij} b_{ij}$ Which gives
an indication of their correlation.

## Comparing two distances

* In fact we  remark that if each distance matrix
were vectorized, this is the equivalent of an
uncentered
covariance between the vectors.
* If the two distances are small in the same places,
and big in the same places the statistic will be large
indicating a strong link between the distances.
* In order to actually test the significance of such a link,
a permutation test is performed: 
the null distribution
is obtained by permuting the elements
of $A$ to obtain what is denoted by $\Pi A$
, keeping the matrix symmetric, and
recomputing
the statistic:
$$\sum_{i<j} (\pi a)_{ij} b_{ij}$$

## Comparing two distances

* The R progam enabling the 
implementation of this algorithm can be
found below.
* What will be useful to us here
is the idea of a vectorial covariance 
between distance matrices, which we will
apply to the special case of correlation
matrices which are in fact similarities.

## Comparing two distances

```{r echo=TRUE}
daniel=function(mata, matb, nperm){
        if(ncol(mata) != ncol(matb))
                stop("matrices should have same dimension")
        if(nrow(mata) != nrow(matb))
                stop("matrices should have same dimension")
        r0 <- stat(mata, matb)
        r <- matrix(0, nperm, 1)
        for(i in (1:nperm)) {
                matp <- permute(mata)
                r[i] <- stat(matp, matb)
        }
        p <- as.numeric(r < r0)/(nperm + 1)
        return(p, r0, r)
}
```

## Comparing two distances

```{r echo=TRUE}
permute=function(mata){
        ap <- matrix(0, nrow(mata), nrow(mata))
        ni <- sample(nrow(mata))
        for(i in 1:nrow(mata)) {
                for(j in 1:(i - 1)) {
                        ap[i, j] <- mata[ni[i], ni[j]]
                }
        }
        return(ap)
}
```

## Comparing two distances

```{r echo=TRUE}
stat<-
function(mata, matb)
{
        r0 <- 0
        for(i in 2:nrow(mata))
                for(j in (1:(i - 1))) {
                        r0 <- r0 + mata[i, j] * matb[i, j]
                }
        return(r0)
}
```

## Wald-Wolfowitz Test on the Line

* Pool the data, rank the data , count the number of
`runs', sequences of  observations
that are from the same sample and follow each other.
* More precisely, suppose we have two sample of observations,
we will call them the $X$'s and the $Y$'s.
* Null hypothesis $F_X=F_Y$.

## Wald-Wolfowitz Test on the Line

* Test statistic is the number of runs $R$, that is the
number of consecutive sequences of identical labels.
* The null distribution of the test statistic can be found by a combinatorial argument:
$$W=\frac{R-\frac{2mn}{N}-1}{\sqrt{\frac{2mn(2mn-N)}{N^2(N-1)}}} \sim {\mathcal N}(0,1)$$
as long as the ratio of the sample sizes stays a number. (bounded away from zero as they increase.)

## Smirnov Test on the line

Sort the univariate observations, $r_i$ is the number of
$X$'s that that have a rank smaller or equal
to $i$.$s_i$ is the same but for the $Y_i$'s.
The statistic here is $$D=max|d_i|\mbox{ where }d_i=
\frac{r_i}{m}-\frac{s_i}{n}
$$
Reject for large values of $D$

## Minimal Spanning Tree Based Tests

This method is inspired by the Wald-Wolfowitz, and
solves the problem that there is no
natural multidimensional `ordering'.

## Wald-Wolfowitz Test on the Line

* The main reference is 
Friedman and Rafsky 1979, Ann stat.,pp.  697-717.
* One may either use multidimensional data
or
first make the data bidimensionnal by principal components.
(or another dimension reducing technique),
in order to be able to represent the tree
in a graphic.


## Minimal Spanning Tree Algorithm

Not the travelling salesman problem.
Algorithms include Prim's, Kruskals', 
I have implemented
the greedy algorithm which is as follows:
(it is a recursive algorithm)
$T_0$ is a point.

## Wald-Wolfowitz Test on the Line

* Suppose we have $T_{i}$, add an edge to it
by finding the minimal edge between a point
in the tree and a point that is not, join these
to make $T_{i+1}$.

* Algorithm:
```
* Set $tree(i)\; <-\; (-n)$, for $i =1:n-1$ 
* Do $n-1$ times:
    * $dmin= min\{ dist(i, |tree(i)|)\; for\; i <n\; with\; tree(i)<0\}$
    * $i_{min}=$ index at which the minimum is attained.
    * Add edge : $tree(i_{min}) <- (-tree(i_{min}))$
    * Update list of nearest vertices: for $i=1:n-1$ do:
        * if $tree(i)<0$ and $dist(i,i_{min})< dist(i,-tree(i)) tree(i)\; <-\; -i_{min}$
```

The output to create will be a vector of numbers
that correspond to where the $i$th point should be connected
to.

## Wald-Wolfowitz Test on the Line

Here are some examples with R:
```{r echo=TRUE}
library(ade4)
#mstree(vincr[1:10,  ])
```

## Wald-Wolfowitz Test on the Line

Example with R (mstree is not contained in R)

```
plot(x, y)   # plot original data
library(ape)
xy.mst <- mst(cbind(x, y), plane=F)   # minimal spanning tree
          # show tree on plot
segments(x[seq(mst)], y[seq(mst)], x[mst], y[mst])
 
 i <- rbind(iris[,,1], iris[,,2], iris[,,3])
       tree <- mst(dist(i))    # multivariate planing
       plot(tree, type="n")  # plot data in plane
 text(tree, label=c(rep(1, 50), rep(2, 50), rep(3, 50))) # identify points

                # get the absolute value stress e
       disti <- dist(i)
       dist2 <- dist(cbind(tree$x, tree$y))
       e <- sum(abs(disti - dist2))/sum(distp)
 0.1464094
```

## Two-sample test

* Pool the two samples together.
* Make a minimal spanning tree.
* Count the number of 'pure' edges,
ie edges of the minimal spanning tree whose
vertices come from the same sample. 
* An equivalent statistic is 
provided
by taking
out all the edges that have mixed colors
and counting how many `separate' trees remain.
* This extends to the case of more than two levels
of treatment by doing the following:

## Many-sample test

* Pool the $K$ samples together.
* Make a minimal spanning tree.
* Count the number of 'pure' edges,
ie edges of the minimal spanning tree whose
vertices come from the same treatment level. 

## Example: Wine data

* As you may remember, this data was 14 physico-chemical
composition varaibles (all continuous), that we are
trying to relate to several categorical variables.
```
acp.vin <- princomp(vin[, 1:14])
#Two components was the choice here
plot(acp.vin$scores[,1:2])
 plot(acp.vin$scores, type = "n")
text(acp.vin$scores[as.numeric(vin[, 15]) == 1,  ], "1")
text(acp.vin$scores[as.numeric(vin[, 15]) == 2,  ], "2")
text(acp.vin$scores[as.numeric(vin[, 15]) == 3,  ], "3")
mst.wine <- mst(dist(acp.vin$scores))
plot(mst.wine,x1=acp.vin$scores[,1],x2=acp.vin$scores[,2])
title("Wines Minimal Spanning Tree")
neighbors=which(mst.wine==1,arr.ind=TRUE)
types=cbind(vin[neighbors[,1],15],vin[neighbors[,2],15])
sum(as.numeric(types[,2]-types[,1]==0))
[1] 72
sum(wine[neighbors[,1],15]==wine[neighbors[,2],15])
 sum(vin[neighbors[sample(154,154,F),1],15]==vin[neighbors[,2],15])
[1] 47
 sum(vin[neighbors[sample(154,154,F),1],15]==vin[neighbors[,2],15])
[1] 53
 sum(vin[neighbors[sample(154,154,F),1],15]==vin[neighbors[,2],15])
[1] 44
 sum(vin[neighbors[sample(154,154,F),1],15]==vin[neighbors[,2],15])
[1] 49
 for (k in (1:1000)){
 null0[k]=sum(vin[neighbors[sample(154,154,F),1],15]==vin[neighbors[,2],15])
 }
```

## Exact Tests

* ``exactRankTests`` is a package which contains many `exact' procedure
which enumerate all possibilities to get the p-values instead of using Monte Carlo.
```
x <- c(0.5, 0.5, 0.6, 0.6, 0.7, 0.8, 0.9)
y <- c(0.5, 1.0, 1.2, 1.2, 1.4, 1.5, 1.9, 2.0)
r <- cscores(c(x,y), type="Wilcoxon")
pperm(sum(r[seq(along=x)]), r, 7)
```

## References

* Bhattacharya (2015). Power Of Graph-Based Two-Sample Tests
* Cuesta-Albertos and Nieto-Reyes (2007). The Random Tukey Depth
* Tukey (1974). Mathematics and the Picturing of Data
* http://www.inside-r.org/packages/cran/spdep/docs/mstree
* http://statweb.stanford.edu/~susan/courses/b494/node2.html
* http://statweb.stanford.edu/~susan/courses/b494/node3.html#SECTION00396000000000000000
