---
title: "Bootstrap (Part 3)"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, Stats 205"
output:
  beamer_presentation:
    incremental: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(1234)
```

## Overview

* So far we used three different bootstraps:
    * Nonparametric boostrap on the rows (e.g. regression, PCA with random rows and columns)
    * Nonparameteric bootstrap on the residuals (e.g. regression)
    * Parameteric bootstrap (e.g. PCA with fixed rows and columns)
* Today, we will improve the boostrap using pivoting:
    * Studentized bootstrap
    * Bias-corrected bootstrap

<!--
already done:
    * Percentile Bootstrap
-->

## Introduction

* A statistics is (asymptotically) pivotal if its limiting distribution does not depend on unkown quantities
* For example, with observations $X_1,\dots,X_n$ from normal distribution with unkown mean and variance, the pivotal is 
$$g(\theta,X_1,\dots,X_n) = \sqrt{n} \left( \frac{\theta-\hat{\theta}}{\hat{\sigma}} \right)$$
with unbiased estimate for sample mean and variance 
$$\hat{\theta} = \frac{1}{n} \sum_{i=1}^n X_i \hspace{0.5cm} \hat{\sigma}^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\hat{\theta})^2$$
* Then $g(\theta,X_1,\dots,X_n)$ is a pivot following the Student's t-distribution with $\nu = n-1$ degrees of freedom
* Because the distribution of $g(\mu,X_1,\dots,X_n)$ does not depend on $\mu$ or $\sigma^2$

## Introduction

* The boostrap is better at estimating the distribution of a pivtotal statistics than at a nonpivotal statistics
* We will see an asymptotic argument using Edgeworth expansions

## Studentized Bootstrap

* Consider $X_1,\dots,X_n$ from $F$
* Let $\hat{\theta}$ be an estimate of some $\theta$
* Let $\hat{\sigma}^2$ be a standard error for $\hat{\theta}$ estiamted using the boostrap
* Most of the time as $n$ grows
$$\frac{\hat{\theta}-\theta}{\hat{\sigma}} \overset{.}{\sim} N(0,1)$$
* Let $z^{(\alpha)}$ be the $100\cdot\alpha$th percentile of $N(0,1)$
* Then a standard confidence interval is with coverage probability $1-2\alpha$ is
$$\hat{\theta} \pm z^{(1-\alpha)} \cdot \hat{\sigma}$$
* As $n \to \infty$, the bootstrap and standard intervals converge

## Studentized Bootstrap

* How can we improve the standard confidence interval?
* The intervals is valid under assumption that
$$Z = \frac{\hat{\theta}-\theta}{\hat{\sigma}} \overset{.}{\sim} N(0,1)$$
* But this is only valid as $n \to \infty$
* And approximate for finite $n$
* When $\hat{\theta}$ is the sample mean, a better approximation is 
$$Z = \frac{\hat{\theta}-\theta}{\hat{\sigma}} \overset{.}{\sim} t_{n-1}$$
and $t_{n-1}$ is the Student's $t$ distribution with $n-1$ degrees of freedom

## Studentized Bootstrap

* With this new approximation, we have
$$\hat{\theta} \pm t_{n-1}^{(1-\alpha)} \cdot \hat{\sigma}$$
* As $n$ grows the $t$ distriubtion converges to the normal distribution
* Intuitively, the it widens the interval to account for unknown standard error
* It does not account for all errors when underlying population when $\hat{\theta}$ is not the sample mean, e.g. for skewness
* The Studentized bootstrap can adjust for such errors

## Studentized Bootstrap

* We estimate the distribution of 
$$Z = \frac{\hat{\theta}-\theta}{\hat{\sigma}} \overset{.}{\sim} \hspace{0.5cm} ?$$
* by generating $B$ bootstrap samples $X^{*1},X^{*2},\dots,X^{*B}$
* and computing
$$Z^{*b} = \frac{\hat{\theta}^{*b}-\hat{\theta}}{\hat{\sigma}^{*b}}$$
* Then the $\alpha$th percentile of $Z^{*b}$ is estimated by the value $\hat{t}^{(\alpha)}$ such that
$$\frac{\#\{Z^{*b} \le \hat{t}^{(\alpha)}\}}{B} = \alpha$$
* Which yields the studentized bootstrap interval
$$(\theta-\hat{t}^{(1-\alpha)}\cdot\hat{\sigma},\theta-\hat{t}^{(\alpha)}\cdot\hat{\sigma})$$

## Asymptotic Argument in Favor of Pivoting

* Take the pivotal statistics 
$$S = \sqrt{n} \left( \frac{\hat{\theta}-\theta}{\hat{\sigma}} \right)$$
with estimate $\hat{\theta}$ asymptotic variance estimate $\hat{\sigma}^2$
* Then, we can use Edgeworth expansions
$$P(S \le x) = \Phi(X) + \sqrt{n} q(x) \phi(x) + O(\sqrt{n})$$
with  
$\Phi$ standard normal distribution,  
$\phi$ standard normal density, and  
$q$ even polynomials of degree 2

## Asymptotic Argument in Favor of Pivoting

* Bootstrap estimates are 
$$S = \sqrt{n} \left( \frac{\hat{\theta}^*-\hat{\theta}}{\hat{\sigma}^*} \right)$$
* Then, we can use Edgeworth expansions
$$P(S^* \le x | X_1,\dots,X_n) = \Phi(X) + \sqrt{n} \hat{q}(x) \phi(x) + O(\sqrt{n})$$
* $\hat{q}$ is obtain by replacing unkowns in $q$ with boostrap estimates
* Asymptotically, we further have
$$\hat{q}-q = O(\sqrt{n})$$

## Asymptotic Argument in Favor of Pivoting

* Then, the boostrap approximation to the distribution of $S$ is 
$$P(S \le x) - P(S^* \le x | X_1,\dots,X_n) = $$
$$\bigg(\Phi(X) + \sqrt{n} q(x) \phi(x) + O(\sqrt{n})\bigg) - \bigg(\Phi(X) + \sqrt{n} \hat{q}(x) \phi(x) + O(\sqrt{n})\bigg)$$
$$= O\left( \frac{1}{n} \right)$$
* Compared to the normal approximation $\sqrt{n}$
* Which the same as the error when using standard boostrap (can be shown with the same argument)

## Studentized Bootstrap

* These pivtal intervals are more accurate in large samples than that of standard intervals and $t$ intervals
* Accuracy comes at the cost of generality
    * standard normal tables apply to all samples and all samples sizes
    * t tables apply to all samples of fixed $n$
    * studentized boostrap tables apply only to given sample
* The studentized boostrap can be asymmteric
* It can be used for simple statisics, like mean, median, timmed mean, and sample percentile
* But not for more general problems, like correlation coefficients:
    * Nested samples if both paramter and standard error have to be bootstrapped
    * Interval can fall outside of allowable range

## Studentized Bootstrap

* The Studentized bootstrap works better for variance stabilized parameters
* Consider a random variable $X$ with mean $\theta$ and standard deviation $s(\theta)$ that varies as a function of $\theta$
* Using the delta method and solving an ordinary differential equation, we can show that
$$g(x) = \int^x \frac{1}{s(u)} du$$
will make the variance of $g(X)$ constant
* Usually $s(u)$ is unkown
* So we need to estimate this as well using the bootstrap

## Studentized Bootstrap

1. First bootstrap $\hat{\theta}^*$, second bootstrap $\hat{\sigma}(\hat{\theta}^*)$
2. Fit curve through points $(\hat{\theta}^*,\hat{\sigma}(\hat{\theta}^*))$
3. Variance stabilization $g(\hat{\theta})$ by numerical integration
4. Studentized bootstrap on $\phi = g(\theta)$ with $g(\hat{\theta}^*)-g(\hat{\theta})$  
(no denominator, since variance is now approximately one)
5. Map back through transformation $g^{-1}$

```{r out.width="0.8\\linewidth"}
knitr::include_graphics("VarianceStabilization.png")
```

Source: Efron and Tibshirani (1994)

<!--
already done

## Percentile Bootstrap

* One way to improve upon the Studentized bootstrap
-->

## Bias-Corrected Bootstrap

* The bias-corrected bootstrap is similar to the percentile boostrap
* Recall the percentile boostrap:
* Take bootstrap samples $$\hat{\theta}^{*1},\dots,\hat{\theta}^{*B}$$
* Order them $$\hat{\theta}^{(*1)},\dots,\hat{\theta}^{(*B)}$$
* Define interval as $$(\hat{\theta}^{(*B\alpha)},\hat{\theta}^{(*B(1-\alpha))})$$ (assuming that $B\alpha$ and $B(1-\alpha)$ are integers)

## Bias-Corrected Bootstrap

* For the bias-corrected version makes two corrects to the percentile version
* By redefining lower $\alpha_1$ and upper $\alpha_2$ levels as
$$\alpha_1 = \Phi\left(\hat{z}_0 + \frac{\hat{z}_0+z^{(\alpha)}}{1-\hat{\alpha}(\hat{z}_0+z^{(\alpha)})}\right)$$
$$\alpha_2 = \Phi\left(\hat{z}_0 + \frac{\hat{z}_0+z^{(1-\alpha)}}{1-\hat{\alpha}(\hat{z}_0+z^{(1-\alpha)})}\right)$$
* The nterval is then given by
$$(\hat{\theta}^{(*B\alpha_1)},\hat{\theta}^{(*B(1-\alpha_2))})$$ (assuming that $B\alpha$ and $B(1-\alpha)$ are integers)
* When $\hat{a}$ and $\hat{z}_0$ are equal to zero then $\alpha_1=\alpha$ and $\alpha_2=1-\alpha$

## Bias-Corrected Bootstrap

* $\hat{z}_0$ measures discrepancy between the median of $\hat{\theta}^*$ and $\hat{\theta}$
* They are estimated with
$$\hat{z}_0 = \Phi^{-1} \left( \frac{\#\{ \hat{\theta}^{*b} < \hat{\theta} \}}{B} \right)$$
* We obtain $\hat{z}_0 = 0$ if half of the $\hat{\theta}^{*b}$ values are less than or equal to $\hat{\theta}$

## Bias-Corrected Bootstrap

* $\hat{a}$ measures the rate of change of teh standard error of $\hat{\theta}$ with repect to the true parameter $\theta$
* It is estimated using the Jackknife
    * Delete $i$ observation in orignial sample denote by $\hat{\theta}_{(i)}$ and estimate 
$$\hat{\theta}_{(\cdot)} = \sum_{i=1}^n \frac{\hat{\theta}_{(i)}}{n}$$
* Then 
$$\hat{a} = \frac{\sum_{i=1}^n (\hat{\theta}_{(\cdot)}-\hat{\theta}_{(i)})^3}{6\{\sum_{i=1}^n (\hat{\theta}_{(\cdot)}-\hat{\theta}_{(i)})^2\}^{3/2}}$$
* Same accuracy as the studentized boostrap
* Can handle out of range problem as well

## Bias-Corrected Bootstrap

* The bias-corrected bootstrap is based on this model
$$\frac{\hat{\phi}-\phi}{\sigma_{\phi}} \sim N(-z_0,1) \hspace{0.5cm} \text{with} \hspace{0.5cm} \sigma_{\phi} = \sigma_{\phi_0} \cdot \left( 1+a(\phi-\phi_0) \right)$$
* Which is a generalization of the usual normal approximation
$$\frac{\hat{\theta}-\theta}{\sigma} \sim N(0,1)$$
* Efron (1987) for justification of this model

<!--
don't need it, computers are fast enough today

## Accelerated Bias-Corrected Bootstrap

TODO: Chapter 14
-->

## References

* Efron (1987). Better Bootstrap Confidence Intervals
* Hall (1992). The Bootstrap and Edgeworth Expansion
* Efron and Tibshirani (1994). An Introduction to the Bootstrap

<!--
* Fisher and Hall (1989). Bootstrap Confidence Regions for Directional Data
* Fisher, Hall, Jing, and Wood (1996). Pivotal Methods for Constructing Confidence Regions With Directional Data
-->
