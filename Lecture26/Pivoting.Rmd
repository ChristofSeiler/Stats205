---
title: "Bootstrap (Part 3)"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, Stats 205"
output:
  beamer_presentation:
    incremental: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(1234)
```

## Overview

* So far we used three different bootstraps:
    * Nonparametric boostrap on the rows (e.g. regression, PCA with random rows and columns)
    * Nonparameteric bootstrap on the residuals (e.g. regression)
    * Parameteric bootstrap (e.g. PCA with fixed rows and columns)
* Today, we will improve the boostrap using pivoting:
    * Studentized bootstrap
    * Bias-corrected bootstrap

<!--
already done:
    * Percentile Bootstrap
-->

## Introduction

* A statistics is (asymptotically) pivotal if its limiting distribution does not depend on unkown quantities
* For example, with observations $X_1,\dots,X_n$ from normal distribution with unkown mean and variance, the pivotal is 
$$g(\theta,X_1,\dots,X_n) = \sqrt{n} \left( \frac{\theta-\hat{\theta}}{\hat{\sigma}} \right)$$
with unbiased estimate for sample mean and variance 
$$\hat{\theta} = \frac{1}{n} \sum_{i=1}^n X_i \hspace{0.5cm} \hat{\sigma}^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\hat{\theta})^2$$
* Then $g(\theta,X_1,\dots,X_n)$ is a pivot following the Student's t-distribution with $\nu = n-1$ degrees of freedom
* Because the distribution of $g(\mu,X_1,\dots,X_n)$ does not depend on $\mu$ or $\sigma^2$

## Introduction

* The boostrap is better at estimating the distribution of a pivtotal statistics than at a nonpivotal statistics
* We will see an asymptotic argument using Edgeworth expansions

## Motivation

* Take 20 random exponential variables with mean 3
```{r eval=FALSE,echo=TRUE}
x = rexp(20,rate=1/3)
```
* Then generate $B=1000$ bootstrap samples of x, and calculate the mean for each bootstrap sample
```{r eval=FALSE,echo=TRUE}
s = numeric(B)
for (j in 1:B) {
  boot = sample(n,replace=TRUE)
  s[j] = mean(x[boot]) }
```
* Form confidence interval from bootstrap samples using quantiles ($\alpha = .05$)
```{r eval=FALSE,echo=TRUE}
simple.ci = quantile(s,c(.025,.975))
```
* Repeat this process 100 from the start
* Check how often the intervals actually contain the true mean

## Motivation

```{r}
n <- 20
r <- 100
B <- 1000
true.mean <- 3
simple.contains <- numeric(r)
simple.ci <- matrix(NA,nrow=r,ncol=2)
for (i in 1:r) {
  x <- rexp(n,rate=1/true.mean)
  s <- numeric(B)
  for (j in 1:B) {
    boot <- sample(n,replace=TRUE)
    s[j] <- mean(x[boot])
  }
  # this is equivalent to using the 'boot' package:
  #simple.boot <- boot(x,R=B,function(x,i) mean(x[i]))
  #s <- simple.boot$t
  simple.ci[i,] <- quantile(s,c(.025,.975))
}
simple.contains <- (true.mean > simple.ci[,1] & true.mean < simple.ci[,2])
#sum(simple.contains)/r
#mean(simple.ci[,2] - simple.ci[,1])
```

```{r fig.width=6,fig.height=6}
plot(0,0,type="n",xlim=c(0,8),ylim=c(0,r),xlab="",ylab="",main="bootstrap conf intervals")
segments(simple.ci[,1],1:r,simple.ci[,2],1:r,col=ifelse(simple.contains,"black","red"))
abline(v=true.mean,col="blue")
```

## Motivation

* Another way is to calculate a pivotal quantity as the bootstrapped statistic
* Calculate the mean and standard deviation 
```{r eval=FALSE,echo=TRUE}
x = rexp(n,rate=1/true.mean)
mean.x = mean(x)
sd.x = sd(x)
```
* For each bootstrap sample, calculate 
```{r eval=FALSE,echo=TRUE}
z = numeric(B)
for (j in 1:B) {
  boot = sample(n,replace=TRUE)
  z[j] = (mean.x - mean(x[boot]))/sd(x[boot]) }
```
* Form a confidence interval like this
```{r eval=FALSE,echo=TRUE}
pivot.ci = mean.x + sd.x*quantile(z,c(.025,.975))
```

## Motivation

```{r}
pivot.contains <- numeric(r)
pivot.ci <- matrix(NA,nrow=r,ncol=2)
for (i in 1:r) {
  x <- rexp(n,rate=1/true.mean)
  mean.x <- mean(x)
  sd.x <- sd(x)
  z <- numeric(B)
  for (j in 1:B) {
    boot <- sample(n,replace=TRUE)
    z[j] <- (mean.x - mean(x[boot]))/sd(x[boot])
  }
  pivot.ci[i,] <- mean.x + sd.x*quantile(z,c(.025,.975))
}
pivot.contains <- (true.mean > pivot.ci[,1] & true.mean < pivot.ci[,2])
#sum(pivot.contains)/r
#mean(pivot.ci[,2] - pivot.ci[,1])
```

```{r fig.width=6,fig.height=6}
plot(0,0,type="n",xlim=c(0,8),ylim=c(0,r),xlab="",ylab="",main="bootstrap conf intervals")
segments(pivot.ci[,1],1:r,pivot.ci[,2],1:r,col=ifelse(pivot.contains,"black","red"))
abline(v=true.mean,col="blue")
```

## Studentized Bootstrap

* Consider $X_1,\dots,X_n$ from $F$
* Let $\hat{\theta}$ be an estimate of some $\theta$
* Let $\hat{\sigma}^2$ be a standard error for $\hat{\theta}$ estiamted using the boostrap
* Most of the time as $n$ grows
$$\frac{\hat{\theta}-\theta}{\hat{\sigma}} \overset{.}{\sim} N(0,1)$$
* Let $z^{(\alpha)}$ be the $100\cdot\alpha$th percentile of $N(0,1)$
* Then a standard confidence interval is with coverage probability $1-2\alpha$ is
$$\hat{\theta} \pm z^{(1-\alpha)} \cdot \hat{\sigma}$$
* As $n \to \infty$, the bootstrap and standard intervals converge

## Studentized Bootstrap

* How can we improve the standard confidence interval?
* The intervals is valid under assumption that
$$Z = \frac{\hat{\theta}-\theta}{\hat{\sigma}} \overset{.}{\sim} N(0,1)$$
* But this is only valid as $n \to \infty$
* And approximate for finite $n$
* When $\hat{\theta}$ is the sample mean, a better approximation is 
$$Z = \frac{\hat{\theta}-\theta}{\hat{\sigma}} \overset{.}{\sim} t_{n-1}$$
and $t_{n-1}$ is the Student's $t$ distribution with $n-1$ degrees of freedom

## Studentized Bootstrap

* With this new approximation, we have
$$\hat{\theta} \pm t_{n-1}^{(1-\alpha)} \cdot \hat{\sigma}$$
* As $n$ grows the $t$ distriubtion converges to the normal distribution
* Intuitively, it widens the interval to account for unknown standard error
* It does not account for all errors when underlying population when $\hat{\theta}$ is not the sample mean, e.g. for skewness
* The Studentized bootstrap can adjust for such errors

## Studentized Bootstrap

* We estimate the distribution of 
$$Z = \frac{\hat{\theta}-\theta}{\hat{\sigma}} \overset{.}{\sim} \hspace{0.5cm} ?$$
* by generating $B$ bootstrap samples $X^{*1},X^{*2},\dots,X^{*B}$
* and computing
$$Z^{*b} = \frac{\hat{\theta}^{*b}-\hat{\theta}}{\hat{\sigma}^{*b}}$$
* Then the $\alpha$th percentile of $Z^{*b}$ is estimated by the value $\hat{t}^{(\alpha)}$ such that
$$\frac{\#\{Z^{*b} \le \hat{t}^{(\alpha)}\}}{B} = \alpha$$
* Which yields the studentized bootstrap interval
$$(\theta-\hat{t}^{(1-\alpha)}\cdot\hat{\sigma},\theta-\hat{t}^{(\alpha)}\cdot\hat{\sigma})$$

## Asymptotic Argument in Favor of Pivoting

* Take the pivotal statistics 
$$S = \sqrt{n} \left( \frac{\hat{\theta}-\theta}{\hat{\sigma}} \right)$$
with estimate $\hat{\theta}$ asymptotic variance estimate $\hat{\sigma}^2$
* Then, we can use Edgeworth expansions
$$P(S \le x) = \Phi(X) + \sqrt{n} q(x) \phi(x) + O(\sqrt{n})$$
with  
$\Phi$ standard normal distribution,  
$\phi$ standard normal density, and  
$q$ even polynomials of degree 2

## Asymptotic Argument in Favor of Pivoting

* Bootstrap estimates are 
$$S = \sqrt{n} \left( \frac{\hat{\theta}^*-\hat{\theta}}{\hat{\sigma}^*} \right)$$
* Then, we can use Edgeworth expansions
$$P(S^* \le x | X_1,\dots,X_n) = \Phi(X) + \sqrt{n} \hat{q}(x) \phi(x) + O(\sqrt{n})$$
* $\hat{q}$ is obtain by replacing unkowns in $q$ with boostrap estimates
* Asymptotically, we further have
$$\hat{q}-q = O(\sqrt{n})$$

## Asymptotic Argument in Favor of Pivoting

* Then, the boostrap approximation to the distribution of $S$ is 
$$P(S \le x) - P(S^* \le x | X_1,\dots,X_n) = $$
$$\bigg(\Phi(X) + \sqrt{n} q(x) \phi(x) + O(\sqrt{n})\bigg) - \bigg(\Phi(X) + \sqrt{n} \hat{q}(x) \phi(x) + O(\sqrt{n})\bigg)$$
$$= O\left( \frac{1}{n} \right)$$
* Compared to the normal approximation $\sqrt{n}$
* Which the same as the error when using standard boostrap (can be shown with the same argument)

## Studentized Bootstrap

* These pivtal intervals are more accurate in large samples than that of standard intervals and $t$ intervals
* Accuracy comes at the cost of generality
    * standard normal tables apply to all samples and all samples sizes
    * t tables apply to all samples of fixed $n$
    * studentized boostrap tables apply only to given sample
* The studentized boostrap can be asymmteric
* It can be used for simple statisics, like mean, median, timmed mean, and sample percentile
* But not for more general problems, like correlation coefficients:
    * Nested samples if both paramter and standard error have to be bootstrapped
    * Interval can fall outside of allowable range

## Studentized Bootstrap

* The Studentized bootstrap works better for variance stabilized parameters
* Consider a random variable $X$ with mean $\theta$ and standard deviation $s(\theta)$ that varies as a function of $\theta$
* Using the delta method and solving an ordinary differential equation, we can show that
$$g(x) = \int^x \frac{1}{s(u)} du$$
will make the variance of $g(X)$ constant
* Usually $s(u)$ is unkown
* So we need to estimate this as well using the bootstrap

## Studentized Bootstrap

1. First bootstrap $\hat{\theta}^*$, second bootstrap $\hat{\sigma}(\hat{\theta}^*)$
2. Fit curve through points $(\hat{\theta}^*,\hat{\sigma}(\hat{\theta}^*))$
3. Variance stabilization $g(\hat{\theta})$ by numerical integration
4. Studentized bootstrap on $\phi = g(\theta)$ with $g(\hat{\theta}^*)-g(\hat{\theta})$  
(no denominator, since variance is now approximately one)
5. Map back through transformation $g^{-1}$

```{r out.width="0.8\\linewidth"}
knitr::include_graphics("VarianceStabilization.png")
```

Source: Efron and Tibshirani (1994)

## Studentized Bootstrap in R

```{r echo=TRUE}
library(boot)
mean.fun = function(d, i) { 
  m = mean(d$hours[i])
  n = length(i)
  v = (n-1)*var(d$hours[i])/n^2
  c(m, v) }
air.boot <- boot(aircondit, mean.fun, R = 999)
results = boot.ci(air.boot, type = c("basic", "stud"))
```

## Studentized Bootstrap in R

```{r echo=TRUE}
results
```

<!--
already done

## Percentile Bootstrap

* One way to improve upon the Studentized bootstrap
-->

## Bias-Corrected Bootstrap

* The bias-corrected bootstrap is similar to the percentile boostrap
* Recall the percentile boostrap:
* Take bootstrap samples $$\hat{\theta}^{*1},\dots,\hat{\theta}^{*B}$$
* Order them $$\hat{\theta}^{(*1)},\dots,\hat{\theta}^{(*B)}$$
* Define interval as $$(\hat{\theta}^{(*B\alpha)},\hat{\theta}^{(*B(1-\alpha))})$$ (assuming that $B\alpha$ and $B(1-\alpha)$ are integers)

## Bias-Corrected Bootstrap

* For the bias-corrected version makes two corrects to the percentile version
* By redefining lower $\alpha_1$ and upper $\alpha_2$ levels as
$$\alpha_1 = \Phi\left(\hat{z}_0 + \frac{\hat{z}_0+z^{(\alpha)}}{1-\hat{\alpha}(\hat{z}_0+z^{(\alpha)})}\right)$$
$$\alpha_2 = \Phi\left(\hat{z}_0 + \frac{\hat{z}_0+z^{(1-\alpha)}}{1-\hat{\alpha}(\hat{z}_0+z^{(1-\alpha)})}\right)$$
* The nterval is then given by
$$(\hat{\theta}^{(*B\alpha_1)},\hat{\theta}^{(*B(1-\alpha_2))})$$ (assuming that $B\alpha$ and $B(1-\alpha)$ are integers)
* When $\hat{a}$ and $\hat{z}_0$ are equal to zero then $\alpha_1=\alpha$ and $\alpha_2=1-\alpha$

## Bias-Corrected Bootstrap

* $\hat{z}_0$ measures discrepancy between the median of $\hat{\theta}^*$ and $\hat{\theta}$
* They are estimated with
$$\hat{z}_0 = \Phi^{-1} \left( \frac{\#\{ \hat{\theta}^{*b} < \hat{\theta} \}}{B} \right)$$
* We obtain $\hat{z}_0 = 0$ if half of the $\hat{\theta}^{*b}$ values are less than or equal to $\hat{\theta}$

## Bias-Corrected Bootstrap

* $\hat{a}$ measures the rate of change of teh standard error of $\hat{\theta}$ with repect to the true parameter $\theta$
* It is estimated using the Jackknife
    * Delete $i$ observation in orignial sample denote by $\hat{\theta}_{(i)}$ and estimate 
$$\hat{\theta}_{(\cdot)} = \sum_{i=1}^n \frac{\hat{\theta}_{(i)}}{n}$$
* Then 
$$\hat{a} = \frac{\sum_{i=1}^n (\hat{\theta}_{(\cdot)}-\hat{\theta}_{(i)})^3}{6\{\sum_{i=1}^n (\hat{\theta}_{(\cdot)}-\hat{\theta}_{(i)})^2\}^{3/2}}$$
* Same accuracy as the studentized boostrap
* Can handle out of range problem as well

## Bias-Corrected Bootstrap

* The bias-corrected bootstrap is based on this model
$$\frac{\hat{\phi}-\phi}{\sigma_{\phi}} \sim N(-z_0,1) \hspace{0.5cm} \text{with} \hspace{0.5cm} \sigma_{\phi} = \sigma_{\phi_0} \cdot \left( 1+a(\phi-\phi_0) \right)$$
with $\phi$ being uknown monotone increasing transformation of $\theta$
* Which is a generalization of the usual normal approximation
$$\frac{\hat{\theta}-\theta}{\sigma} \sim N(0,1)$$
* Efron (1987) for justification of this model

## Bias-Corrected Bootstrap in R

```{r echo=TRUE}
library(bootstrap)
xdata = matrix(rnorm(30),ncol=2); n = 15
theta = function(x,xdata) { 
  cor(xdata[x,1],xdata[x,2]) 
  }
results = bcanon(1:n,100,theta,xdata,
                 alpha=c(0.025, 0.975))
results$confpoints
```

<!--
don't need it, computers are fast enough today

## Accelerated Bias-Corrected Bootstrap

TODO: Chapter 14
-->

## References

* Efron (1987). Better Bootstrap Confidence Intervals
* Hall (1992). The Bootstrap and Edgeworth Expansion
* Efron and Tibshirani (1994). An Introduction to the Bootstrap
* Love (2010). Bootstrap-t confidence intervals [(Link to blog entry)](http://mike-love.net/r-scripts/bootstrap-t.R)

<!--
* Fisher and Hall (1989). Bootstrap Confidence Regions for Directional Data
* Fisher, Hall, Jing, and Wood (1996). Pivotal Methods for Constructing Confidence Regions With Directional Data
-->
