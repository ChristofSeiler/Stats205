---
title: "Pivoting"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, Stats 205"
output:
  beamer_presentation:
    incremental: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(1234)
```

## Overview

* So far we used three different bootstraps:
    * Nonparametric boostrap on the rows (e.g. regression, PCA with random rows and columns)
    * Nonparameteric bootstrap on the residuals (e.g. regression)
    * Parameteric bootstrap (e.g. PCA with fixed rows and columns)
* Today, we will improve the boostrap using pivoting:
    * Studentized bootstrap
    * Bias-corrected bootstrap
    * Accelerated bias-corrected Bootstrap

<!--
already done:
    * Percentile Bootstrap
-->

## Introduction

* A statistics is (asymptotically) pivotal if its limiting distribution does not depend on unkown quantities
* For example, with observations $X_1,\dots,X_n$ from normal distribution with unkown mean and variance, the pivotal is 
$$g(\theta,X_1,\dots,X_n) = \sqrt{n} \left( \frac{\theta-\hat{\theta}}{\hat{\sigma}} \right)$$
with unbiased estimate for sample mean and variance 
$$\hat{\theta} = \frac{1}{n} \sum_{i=1}^n X_i \hspace{0.5cm} \hat{\sigma}^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\hat{\theta})^2$$
* Then $g(\theta,X_1,\dots,X_n)$ is a pivot following the Student's t-distribution with $\nu = n-1$ degrees of freedom
* Because the distribution of $g(\mu,X_1,\dots,X_n)$ does not depend on $\mu$ or $\sigma^2$

## Introduction

* The boostrap is better at estimating the distribution of a pivtotal statistics than at a nonpivotal statistics
* We will see an asymptotic argument using Edgeworth expansions

## Studentized Bootstrap

* Consider $X_1,\dots,X_n$ from $F$
* Let $\hat{\theta}$ be an estimate of some $\theta$
* Let $\hat{\sigma}^2$ be a standard error for $\hat{\theta}$ estiamted using the boostrap
* Most of the time as $n$ grows
$$\frac{\hat{\theta}-\theta}{\hat{\sigma}} \overset{.}{\sim} N(0,1)$$
* Let $z^{(\alpha)}$ be the $100\cdot\alpha$th percentile of $N(0,1)$
* Then a standard confidence interval is with coverage probability $1-2\alpha$ is
$$\hat{\theta} \pm z^{(1-\alpha)} \cdot \hat{\sigma}$$
* As $n \to \infty$, the bootstrap and standard intervals converge

## Studentized Bootstrap

* How can we improve the standard confidence interval?
* The intervals is valid under assumption that
$$Z = \frac{\hat{\theta}-\theta}{\hat{\sigma}} \overset{.}{\sim} N(0,1)$$
* But this is only valid as $n \to \infty$
* And approximate for finite $n$
* When $\hat{\theta}$ is the sample mean, a better approximation is 
$$Z = \frac{\hat{\theta}-\theta}{\hat{\sigma}} \overset{.}{\sim} t_{n-1}$$
and $t_{n-1}$ is the Student's $t$ distribution with $n-1$ degrees of freedom

## Studentized Bootstrap

* With this new approximation, we have
$$\hat{\theta} \pm t_{n-1}^{(1-\alpha)} \cdot \hat{\sigma}$$
* As $n$ grows the $t$ distriubtion converges to the normal distribution
* Intuitively, the it widens the interval to account for unknown standard error
* It does not account for all errors when underlying population when $\hat{\theta}$ is not the sample mean, e.g. for skewness
* The Studentized bootstrap can adjust for such errors

## Studentized Bootstrap

* We estimate the distribution of 
$$Z = \frac{\hat{\theta}-\theta}{\hat{\sigma}} \overset{.}{\sim} \hspace{0.5cm} ?$$
* by generating $B$ bootstrap samples $X^{*1},X^{*2},\dots,X^{*B}$
* and computing
$$Z^*(b) = \frac{\hat{\theta}^*(b)-\hat{\theta}}{\hat{\sigma}^*(b)}$$
* Then the $\alpha$th percentile of $Z^*(b)$ is estimated by the value $\hat{t}^{(\alpha)}$ such that
$$\frac{\#\{Z^*(b) \le \hat{t}^{(\alpha)}\}}{B} = \alpha$$
* Which yields the studentized bootstrap interval
$$(\theta-\hat{t}^{(1-\alpha)}\cdot\hat{\sigma},\theta-\hat{t}^{(\alpha)}\cdot\hat{\sigma})$$

## Asymptotic Argument in Favor of Pivoting

* Take the pivotal statistics 
$$S = \sqrt{n} \left( \frac{\hat{\theta}-\theta}{\hat{\sigma}} \right)$$
with estimate $\hat{\theta}$ asymptotic variance estimate $\hat{\sigma}^2$
* Then, we can use Edgeworth expansions
$$P(S \le x) = \Phi(X) + \sqrt{n} q(x) \phi(x) + O(\sqrt{n})$$
with  
$\Phi$ standard normal distribution,  
$\phi$ standard normal density, and  
$q$ even polynomials of degree 2

## Asymptotic Argument in Favor of Pivoting

* Bootstrap estimates are 
$$S = \sqrt{n} \left( \frac{\hat{\theta}^*-\hat{\theta}}{\hat{\sigma}^*} \right)$$
* Then, we can use Edgeworth expansions
$$P(S^* \le x | X_1,\dots,X_n) = \Phi(X) + \sqrt{n} \hat{q}(x) \phi(x) + O(\sqrt{n})$$
* $\hat{q}$ is obtain by replacing unkowns in $q$ with boostrap estimates
* Asymptotically, we further have
$$\hat{q}-q = O(\sqrt{n})$$

## Asymptotic Argument in Favor of Pivoting

* Then, the boostrap approximation to the distribution of $S$ is 
$$P(S \le x) - P(S^* \le x | X_1,\dots,X_n) = $$
$$\bigg(\Phi(X) + \sqrt{n} q(x) \phi(x) + O(\sqrt{n})\bigg) - \bigg(\Phi(X) + \sqrt{n} \hat{q}(x) \phi(x) + O(\sqrt{n})\bigg)$$
$$= O\left( \frac{1}{n} \right)$$
* Compared to the normal approximation $\sqrt{n}$
* Which the same as the error when using standard boostrap (can be shown with the same argument)

## Studentized Bootstrap

* These pivtal intervals are more accurate in large samples than that of standard intervals and $t$ intervals
* Accuracy comes at the cost of generality
    * standard normal tables apply to all samples and all samples sizes
    * t tables apply to all samples of fixed $n$
    * studentized boostrap tables apply only to given sample
* The studentized boostrap can be asymmteric
* It can be used for simple statisics, like mean, median, timmed mean, and sample percentile
* But not for more general problems, like correlation coefficients:
    * $B^2$ samples if both paramter and standard error have to be bootstrapped
    * interval can fall outside of allowable range

<!--
already done

## Percentile Bootstrap

* One way to improve upon the Studentized bootstrap
-->

## Bias-Corrected Bootstrap

TODO: Chapter 14

## Accelerated Bias-Corrected Bootstrap

TODO: Chapter 14

## Bioequivalence Example

TODO: Chatper 25

## References

* Efron (1987). Better Bootstrap Confidence Intervals
* Hall (1992). The Bootstrap and Edgeworth Expansion
* Efron and Tibshirani (1994). An Introduction to the Bootstrap

<!--
* Fisher and Hall (1989). Bootstrap Confidence Regions for Directional Data
* Fisher, Hall, Jing, and Wood (1996). Pivotal Methods for Constructing Confidence Regions With Directional Data
-->
