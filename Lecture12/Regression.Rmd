---
title: "Regression"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output:
  ioslides_presentation:
    incremental: yes
    smaller: no
    transition: faster
    widescreen: yes
  beamer_presentation:
    incremental: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(1234)
library(ggplot2)
library(Rfit)
library(npsm)
```

## Overview

* Rank-based linear regression
* Smoothing or estimating curves
    * Density Estimation
    * Non-linear regression

## Curve Estimation

* A curve of interest can be a probability density function $f$ or a regression function $r$
* In density estimation, we observe $X_1,\dots,X_n$ from some unknown cdf $F$ with density $f$
$$ X_1,\dots,X_n \sim f $$
and the goal is to estimate density $f$
* In regression, we observe pairs $(x_1,Y_1),\dots,(x_n,Y_n)$ that are related as
$$ Y_i = r(x_i) + e_i $$
with $E(e_i) = 0$, and the goal is to estimate the regression function $r$

## Density Estimation

```{r fig.width=10,fig.height=5}
dat <- data.frame(cond = factor(rep(c("A","B"), each=200)), 
                   rating = c(rnorm(200),rnorm(200, mean=.8)))
ggplot(dat, aes(x=rating)) + 
  geom_histogram(aes(y=..density..),binwidth=.1,colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666")
```

## Non-Linear Regression

```{r fig.width=10,fig.height=5}
data(poly)
x = poly[,1]
y = poly[,2]
plot(y~x,xlab=expression(x),ylab=expression(y))
title("Bandwidth 0.5")
lines(ksmooth(x,y,"normal",0.5),lwd=2,col="red")
```

## Rank-Based Linear Regression

* But today, we focus on linear regression
* We generalize rank-based methods from two-sample location problems to general linear models

## Rank-Based Linear Regression

```{r message=FALSE,fig.width=10,fig.height=5}
data(engel)
plot(engel)
abline(rfit(foodexp~income,data=engel))
abline(lm(foodexp~income,data=engel),lty=2)
legend("topleft",c('R','LS'),lty=c(1,2))
```

## Some Notation

* Setup the following linear model (for $i = 1,\dots,n$)
$$ Y_i = x_i^T \boldsymbol{\beta} + e_i^* $$
where $\boldsymbol{\beta}$ is a $1 \times p$ vector of unkown parameters
* $\boldsymbol{\beta}$ are the parameter of interst
* Center (usually using the median $T(e_i^*) = \alpha$) the errors $e_i = e_i^* - \alpha$
$$ Y_i = \alpha + \boldsymbol{x}_i \boldsymbol{\beta} + e_i$$
* Let $f(t)$ be the pdf of the erros $e_i$
* Assumption: $f(t)$ can be either asymmetric or symmetric depending on whether signs or ranks are used
* The intercept $\alpha$ is independent of the slope $\boldsymbol{\beta}$

## Some Notation

* Let $\boldsymbol{Y} = (Y_1,\dots,Y_n)^T$ denote the $n \times 1$ vector of observations
* Let $\boldsymbol{X}$ denote the $n × p$ matrix with rows $x^T_i$
* Then we can write the linear model in matrix form:
$$ \boldsymbol{Y} = \boldsymbol{1}\alpha + \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{e} $$
* $\boldsymbol{X}$ is centered (that's fine since we have $\alpha$ in the model), and assume $\boldsymbol{X}$ is full column rank
* Let $\Omega_F$ be the column space spanned by coluns of $\boldsymbol{X}$
* So we can rewrite the linear model as (coordinate-free because not restricited to any specific basis vectors)
$$ \boldsymbol{Y} = \boldsymbol{1}\boldsymbol{\beta} + \boldsymbol{\eta} + \boldsymbol{e} $$
with $\boldsymbol{\eta} = \Omega_F$

## Some Notation

* Now we can estimate $\boldsymbol{\beta}$
* And test hypothesis
$$ H_0: \boldsymbol{M}\boldsymbol{\beta} = 0 \hspace{2cm} H_A: \boldsymbol{M}\boldsymbol{\beta} \ne 0 $$
* $\boldsymbol{M}$ is a $q \times p$ matrix of full rank

## The Geometry of Estimation

$$ \boldsymbol{Y} = \boldsymbol{1}\boldsymbol{\beta} + \boldsymbol{\eta} + \boldsymbol{e} \hspace{1cm}\text{with}\hspace{1cm} \boldsymbol{\eta} = \Omega_F$$

* Task is to minimize some distance between $\boldsymbol{Y}$ and subspace $\Omega_F$
* Think of $\boldsymbol{\eta}$ as a hyperplane and the task as projecting $\boldsymbol{Y}$ onto it
* For the projection we need to define a distance
* Instead of using the usual Euclidean distance, we use a distance based on signs and ranks
$$ \| v_i \|_{\varphi} = \sum_{i=1}^n a(R(v_i)) v_i $$
* with scores $a(1) \le a(2) \le \cdots \le a(n)$ and score function $a(i) = \varphi(i/(n+1))$ 
* $\varphi$ is nondecreasing, centered, standardized and defined on the interval $(0,1)$

## The Geometry of Estimation

* $\|\boldsymbol{v}\|_{\varphi}$ is a pseudo-norm:
    * triangle inequality, non-negative, $\|\alpha \boldsymbol{v}\|_{\varphi} = |\alpha \|\boldsymbol{v}\|_{\varphi}$, and 
    * additionally $\|\boldsymbol{v}\|_{\varphi} = 0$ if and only if $v_1 = \dots = v_n$
* By setting $\varphi_R(u) = \sqrt{12}(u − 1/2)$, we get the Wilcoxon pseudo-norm
* By setting $\varphi_S(u) = \operatorname{sgn}(u − 1/2)$, we get the sign pseudo-norm (equivalent to using the $L_1$ norm)
* In general
$$ D(\boldsymbol{Y},\Omega_F) = \| \boldsymbol{Y} - \widehat{\boldsymbol{Y}}_{\varphi} \|_{\varphi} = \underset{\boldsymbol{\eta \in \Omega_F}}{\min} \| \boldsymbol{Y}-\boldsymbol{\eta} \|_{\varphi} $$

## The Geometry of Estimation

$$ \widehat{\boldsymbol{\eta}} = D(\boldsymbol{Y},\Omega_F) = \| \boldsymbol{Y} - \widehat{\boldsymbol{Y}}_{\varphi} \|_{\varphi} = \underset{\boldsymbol{\eta \in \Omega_F}}{\min} \| \boldsymbol{Y}-\boldsymbol{\eta} \|_{\varphi} $$

<div class="columns-2">
```{r, out.height = 270, fig.retina = NULL, echo=FALSE}
knitr::include_graphics("EstimationGeometry.png")
```

* Estimate $\widehat{\boldsymbol{\eta}}_{\varphi}$
* Distance between $Y$ and the space $\Omega_F$ is $d_F$
* Reduced model subspace $\Omega_R \subset \Omega_F$
</div>

Source: Hettmansperger and McKean (2011)  

## References

* Hettmansperger and McKean (2011). Robust Nonparametric Statistical Methods
* Wassermann (2006). All of Nonparametric Statistics
* Hollander, Wolfe, and Chicken (2014). Nonparametric Statistical Methods
