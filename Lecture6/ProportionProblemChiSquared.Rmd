---
title: "Proportion Problem and Chi-Squared Tests"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output:
  ioslides_presentation:
    incremental: yes
    smaller: no
    transition: faster
    widescreen: yes
  beamer_presentation:
    incremental: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Previous Lectures

* One-sample tests: 
    * Sign test
    * Signed-Rank Wilcoxon
* Estimators, confidence intervals, and robustness to outliers
* Bootstrap
    * Error of estimators and significance for hypothesis testing
    * Complete enumerations
    * Tail probability

## Today

* Proportion problems
* $\chi^2$ Tests

## Proportion Problems

* Discrete variables
* The random variable $X$ consists of categories
* For now, we focus on binary categries failure (0) and success (1)
* So, we say $X \sim \operatorname{Bern}(p)$ is a RV distributed according to the Bernoulli distribution 
    * with success probabiliy $p$ and 
    * failure probabililty $1-p$
* We know that $\operatorname{E}(X) = p$ and $\operatorname{Var} = p(1-p)$

## Proportion Problems

* Statistical problems can be 
    * estimating $p$,
    * forming confidence intervals,
    * and testing hypothesis 
    $$H_0: p = p_0 \text{ versus } H_A: p \ne p_0$$
* Let $X_1,\dots,X_n \sim \operatorname{Bern}(p)$ and $S$ be the total number of successes
* Then $S \sim B(p,n)$ follows a binomial distribution
* If $p$ is unkown but we assume that our observation come from a Bernoulli distribution, then
* We can estimate $p$ as $\widehat{p} = \frac{S}{n}$

## Proportion Problems

```{r echo=TRUE}
n = 10; p = 1/2; nsim = 100; obsv = rbinom(nsim, size = n, prob = p)
```

```{r}
library(ggplot2)
df = data.frame(bern = table(obsv))
colnames(df) = c("S","Count")
ggplot(df, aes(x = S, y = Count)) + 
  geom_bar(stat="identity")
```

## Proportion Problems

```{r}
df <- data.frame(y = obsv)
ggplot(df, aes(sample = y)) + stat_qq() + ggtitle("Normal Q–Q Plot")
```

## Proportion Problems

```{r echo=TRUE}
n = 100; p = 1/2; nsim = 100; obsv = rbinom(nsim, size = n, prob = p)
```

```{r}
df = data.frame(bern = table(obsv))
colnames(df) = c("S","Count")
labels = floor(seq(0,n,length.out=25))
ggplot(df, aes(x = S, y = Count)) + 
  geom_bar(stat="identity") +
  scale_x_discrete(breaks=labels, labels=as.character(labels))
```

## Proportion Problems

```{r}
df <- data.frame(y = obsv)
ggplot(df, aes(sample = y)) + stat_qq() + ggtitle("Normal Q–Q Plot")
```

## Proportion Problems

* As $n \to \infty$ while $p$ is fixed:
    * The de Moivre–Laplace theorem (a special case of the central limit theorem) says $S$ approach a normal distribution
    * Easy confidence interval (just evaluate cdf of the normal)

## Example: Squeaky Hip Replacements

143 subjects with ceramic hip replacements

Ten report that their hip replacements squeaked
```{r echo=TRUE}
phat = 10/143
zcv = qnorm(0.975)
phat+c(-1,1)*zcv*sqrt(phat*(1-phat)/143)
```

We estimate between roughly 3 and 11% of patients who receive ceramic hip replacements will report squeaky replacements.

## Hypothesis Testing

* Test statistic is $z^2$ with
$$z = \frac{\widehat{p}-p_0}{\sqrt{p_0(1-p_0)/n}} \sim N(0,1)$$
* We are interested how large the observed proportion deviates this measured in terms of $|z|$
* Squared normal is distributed as $\chi^2$ distribution

## Example: Left-Handed Professional Ball Players

* Theory:  
    * Professional baseball players have different proportion of left handed player than left-handed people in genral population.
    * From previous study, we know that general public is has a proportion of $p = 0.15$.
* Hypothesis testing:
    * $H_0: p = 0.15 \text{ versus } H_A: p \ne 0.15$

## Example: Left-Handed Professional Ball Players

```{r warning=FALSE,echo=TRUE,message=FALSE}
library(Rfit)
head(baseball)
```

## Example: Left-Handed Professional Ball Players

```{r echo=TRUE}
ind = with(baseball,throw=='L')
n = length(ind)
phat = sum(ind)/n
phat
p0 = 0.15
z = (phat-p0)/(sqrt(p0*(1-p0)/n))
pvalue = 1-pchisq(z^2,df=1)
pvalue
```

## What is Nonparametrics Statistics Again?

<!--
* Rank-based methods: we do not estimate any additional parameters (exepct of course the parameter of interest $\theta$)
* Compare to $t$-test, we also have to estimate the variance 
* Tests for discrete observations: No parameter estimation neither in finite sample or asymptotic case
-->
* Think of nonparametric statistics as either:
    * discrete observations, thus we can enumerate everything
        * artificially by transforming continuous observations into ranks
        * observations are just measured on a discrete scale, e.g. counts
    * ininite amount of parameters (inference of functions, we'll see that later in the course)

## Why Not Use Finite Sample Binomial Test?

Since we know that $S$ follows a binomial distribution, why shouldn't we use it?

```{r echo=FALSE}
binom.test(sum(ind),n,p=p0)
```

* Finite sample $p$-values have upper bounded significance level $\alpha$
* Asymptotic $p$-values may be above above $\alpha$

## Why Not Use Finite Sample Binomial Test?

```{r echo=TRUE}
n = 5; S = 5; p = 0.5
phat = S/n
pvalue = 2*p^n; pvalue
```

* Problem is due to discreteness
* More extreme case $n = 5$ and test $H_0: p = 0.5$ versus $H_A: p \ne 0.5$
* Under the null, this distribution is binomial with $n = 5$ and parameter $p = 0.5$
* Suppose outcome is $S = 5$ (most extreme observation)
* Problem is that the null hypethsis can never be true below our standard $\alpha = 0.05$
* So in this case, $\alpha$ has no meaning

## Discrete Random Variable (RV)

* Extension from two categories to multiple caterogries
* Consider discrete RV $X$ with $1,2,\dots,c$ categories
* Let $p(j) = P(X = j)$ define the probabiliy mass function
* We wish to test:
$$H_0: p(j) = p_0(j), j = 1,\dots,c$$
$$H_A: p(j) \ne p_0(j), \text{ for some } j$$

## Discrete RV

* Let $O_j = \#\{ X_i = j \}$
* Observed frequencies are constrained $\sum_{j=1}^c O_j = n$ 
* So $c-1$ degrees of freedom, if we know $c-1$ frequency we can calculate the $c$th by using the constrain
* The expected frequency for category $j$ is $\operatorname{E}_j = \operatorname{E}_{H_0}(O_j)$
* Two cases for $H_0$

## Discrete RV

* Case 1: 
    * All $p_0(j)$ are specified
    * So we get $E_j = np_0(j)$
    * Test stastitics is
    $$\chi^2 = \sum_{j=1}^c \frac{(O_j-E_j)^2}{E_j}$$
* Hypothesis $H_0$ is rejected in favor of $H_A$ for large values of $\chi^2$

## Discrete RV Example {.build}

* Roll a dice $n = 370$ times  
* Observe frequencies
```{r echo=TRUE}
O = c(58,55,62,68,66,61)
n = sum(O); n
```  
* Test whether dice is fair $p(j) \equiv 1/6$
```{r echo=TRUE}
p0 = 1/6
E = rep(n*p0,6)
Chi2_0 = sum((O-E)^2/E); Chi2_0
```

## Discrete RV Example

Assymptotically equal to $\chi^2$ with $c-1$ degress of freedom

```{r echo=TRUE}
pvalue = 1-pchisq(Chi2_0,df=6-1); pvalue
```

## Discrete RV

* Case 2: 
    * Only form of pmf is known
    * Have to estimate $p$
    * Same test stastitics but now with estimate $\widehat{p}$
    $$\chi^2 = \sum_{j=1}^c \frac{(O_j-E_j)^2}{E_j}$$
* Hypothesis $H_0$ is rejected in favor of $H_A$ for large values of $\chi^2$

## Discrete RV Example {.build}

* Number of males in the first seven children for $n = 1334$ Swedish ministers of religion
```{r echo=TRUE}
males = 0:7
ministers = c(6,57,206,362,365,256,69,13)
n = sum(ministers); n
df = data.frame(ministers=ministers,males=males); t(df)
```
* For example, 206 of these ministers had 2 sons in their first 7 children

## Discrete RV Example {.build}

* The maximum likelihood estimator of $p$ is
```{r echo=TRUE}
nChildren = n*7
nMale = sum(df$ministers*df$males)
phat = nMale/nChildren; phat
p0 = dbinom(males,7,phat)
E = n*p0
```

```{r}
df = data.frame(E=round(E,1),ministers=ministers,males=males)
t(df)
```

## Discrete RV Example {.build}

```{r echo=TRUE}
Chi2_0 = sum((df$ministers-E)^2/E)
pvalue = 1-pchisq(Chi2_0,df=8-1-1); pvalue
```

## Discrete RV

* Since $S$ converges in distribution to a normal, we could just evaluate cdf of normal
* Another interesting case where this is possible is for difference in proporitions
* The confidence interval for $p_j - \widehat{p}_k$
* As before the $p_j = O_j/n$ are the proportion of samples of category $j$, then
$$\widehat{p}_j - \widehat{p}_k \pm z_{\alpha/2} \sqrt{\frac{\widehat{p}_j+\widehat{p}_k-(\widehat{p}_j-\widehat{p}_k)^2}{n}}$$

## Discrete RV Example {.build}

* Difference in the probabilities of all daughters or all sons
```{r}
t(df[,-1])
```
* 6 ministers had no sons, and 13 ministers had all sons
```{r echo=TRUE}
n = 1334; p0 = 6/n; p7 = 13/n
se = sqrt((p0+p7-(p0-p7)^2)/n)
zcv = qnorm(0.975)
lb = p0-p7 - zcv*se; ub <- p0-p7 + zcv*se; res = c(p0-p7,lb,ub); res
```
* Confidence interval covers 0, thus no significant difference in the proportions

## Several Discrete RVs

* Goal is to compare several discrete RV, which have same range $\{ 1,2,\dots,c \}$
* Consider hypothesis test:
    * $H_0:$ $X_1,\dots,X_r$ have the same distribution
    * $H_A:$ Distributions of $X_i$ and $X_j$ differ for some $i \ne j$
* Total number of samples $n = \sum_{i=1}^r n_i$
* Observed frequencies: 
$$O_{ij} = \#\{ \text{sample items in sample drawn on } X_i \text{ such that } X_i = j\},$$
* for $i = 1,\dots,r$ and $j = 1,\dots,c$
* $O_{ij}$ is a $r \times c$ matrix of observed frequency
* They are called *contingency tables*

## Several Discrete RVs

* Compare observed frequencies to the expected frequencies under $H_0$
* Estimate the common distribution $(p_1,\dots,p_c)^T$, where $p_j$ is the probability that category $j$ occurs
* Estimate probability of category $j$ overall
$$ \widehat{p}_j = \frac{\sum_{i=1}^r O_{ij}}{n}, j = 1,\dots,c$$
* Estimate expected frequencies $\widehat{E}_{ij} = n_i \widehat{p}_j$
* Notice that the sample size can vary between variables

## Several Discrete RVs

* Test statistics
$$\chi^2 = \sum_{i=1}^r \sum_{j=1}^c \frac{(O_{ij}-\widehat{E}_{ij})^2}{\widehat{E}_{ij}}$$
* degrees of freedom:
    * $c−1$ because total sample size is known, and
    * $c−1$ estimates, then
    * $r(c−1)−(c−1)=(r−1)(c−1)$
* This is called *test for homogeneity*

## Several Discrete RVs Example {.build}

* Type of Crime and Alcoholic Status
* Contingency table with frequencies of criminals who committed crimes with or without alcoholics
```{r}
c1 = c(50,88,155,379,18,63) 
c2 = c(43,62,110,300,14,144)
ct = cbind(c1,c2)
colnames(ct) = c("Alcoholic","Non-Alcoholic")
rownames(ct) = c("Arson","Rape","Violence","Theft","Coining","Fraud")
t(ct)
chifit = chisq.test(ct)
chifit
```

## Several Discrete RVs Example {.build}

```{r echo=TRUE}
t((chifit$observed-chifit$expected)^2/chifit$expected)
```
* Most of the contribution to the test statistic comes from the crime fraud
* Eliminate fraud and retest
```{r}
chifit = chisq.test(ct[-6,])
chifit
```
* Conclusion: Conditional on the criminal not committing fraud, his alcoholic status and type of crime have the same distribution

## Independence of Two Discrete RVs

* Test independence of two discrete RV
* To discrete RV $X$ and $Y$ with different ranges $\{1,2,\dots,r\}$ and $\{1,2,\dots,c\}$
* Consider hyothesis test:
$$
\begin{align}
H_0: P(X=i,Y=j) & = P(X=i)P(Y=j) \text{ for all } i \text{ and } j \\
H_A: P(X=i,Y=j) & \ne P(X=i)P(Y=j) \text{ for some } i \text{ and } j 
\end{align}
$$
* Construct contingency table of $n$ observations 
$$O_{ij} = \#_{1 \le l \le n} \{ (X_l,Y_l) = (i,j) \}$$
* Apply same procdure as for test for homogeneity

## Goodness of Fit

* All the above tests are called goodness of fit tests
* Quantifies how well a model fits observations
* Measures discrepancy between observed values and expected values under the model
* But what if the model doesn't fit
* Can we visualize the residuals?
* Rather than reject this global hypthesis, can we find what is driving the statistic?
* Two ways:
    * Visualization of contingency table with association and mosaics plots
    * Visualization of deviation from model with correspondence analysis

## Association Plots

* Followup on $\chi^2$ tests
* Pearson residuals
$$r_{ij} = \frac{O_{ij}-E_{ij}}{\sqrt{E_{ij}}}$$
* and the $\chi^2$ statistics is squaring and summing over all cells
$$\chi^2 = \sum_j \sum_i r_{ij}^2$$

## Association Plots

* Each cell of contingency table is represented by a rectangle encoding information in width, height, location, and color of the rectangle
* *Height:* Proportional to Pearson residual $\frac{O_{ij}-E_{ij}}{\sqrt{E_{ij}}}$
* *Width:*  Proportional to $\sqrt{E_{ij}}$
* *Area:* Proportional to $O_{ij}-E_{ij}$
* *Baseline:* 
    * If $O_{ij} > E_{ij}$, then rectanble above
    * Otherwise rectangle below
* *Color:* Standardized Pearson residuals that are asymptotically standard normal

## Association Plots Example

```{r echo=TRUE, message=FALSE, fig.width=5.5, fig.height=5.5}
library(vcd); assoc(ct,shade = TRUE)
```

## Mosaic Plots

* *Width:* Relative frequencies of one variable
* *Height:* Relative  (conditional) frequencies of the second variable given the first
* *Color:* Standardized Pearson residuals that are asymptotically standard normal

## Mosaic Plots {.build}

* Subdivide by marginal frequencies of first variable
```{r echo=TRUE}
frequency = rowSums(ct)
proportions = frequency/sum(frequency)
t(data.frame(frequency=round(frequency,0),proportions=round(proportions,2)))
```
* Subdivide by marginal frequencies of second given first variable
```{r echo=TRUE}
t(ct/rowSums(ct))
```

## Mosaic Plots

```{r echo=TRUE, message=FALSE, fig.width=6, fig.height=6}
mosaic(ct,shade = TRUE)
```

## Correspondence Analysis

* Actively developped in 1965 in France
* French school of data analysis: Geometry instead of probability
* Can be used to plot contingency tables for exploratory data analysis
* Decomposes the $\chi^2$ statistic into orthogonal factors
* To first approximation: extension Principal Components Analysis (PCA) by 
* replacing the variance in PCA with an inertia proportional to the $\chi^2$ distance of the table from independence
* decomposes this measure of departure from independence along axes that are orthogonal according to a $\chi^2$ inner product

## Correspondence Analysis

* Denote $m \times p$ contingency table as $N$
* Total number of observations $n = \sum_{i=1}^m \sum_{j=1}^p n_{ij}$
* Denote frequency table as $F = \frac{N}{n}$
* Vector of row sums of $F$: $\operatorname{diag}(D_r) = r = \frac{1}{n} N 1_p$
* Vector of column sums of $F$: $\operatorname{diag}(D_c) = c^T = \frac{1}{n} N^T 1_m$
* Peform Principle Component Analysis (PCA) on new data matrix
$$X = D_r^{-1} F D_c^{-1} − 1_m^T 1_p$$
* Generalized singular value decomposition on $X = USV^T$  
with constrains $U^T D_r U = I_m$ and $V^T D_c V = I_p$

## Correspondence Analysis

* Relationship to $\chi^2$ statistic
$$D_r X^T D_c X = \frac{\chi^2}{n}$$
* In standard PCA, principle components explain variance
* In CA, principle components explain devation from independence

## Mosaic, Association, and Correspondence Analysis Example

```{r echo=TRUE}
library(ca)
HairEye = margin.table(HairEyeColor, c(1, 2)); HairEye
```

----

```{r echo=TRUE}
chisq.test(HairEye)
```

----

```{r echo=TRUE, message=FALSE, fig.width=6.5, fig.height=6.5}
mosaic(HairEye, shade = TRUE)
```

---

```{r echo=TRUE, message=FALSE, fig.width=6, fig.height=6}
assoc(HairEye,shade = TRUE)
```

----

```{r echo=TRUE}
ca.res = ca(HairEye); ca.res
```

----

```{r fig.width=10, fig.height=5}
plot(ca.res)
```

<!--
## McNemar's Test {.build}

* Two discrete RV with two possible value $\{0,1\}$
* $2 \times 2$ contingency table with four categories (0,0), (0,1), (1,0), and (1,1)
* Denote probabilies of categories $p_{ij}$
* Hypothesis test:
$$H_0: p_{01} - p_{10}$$
$$H_A: p_{01} \ne p_{10}$$
```{r}
ct = matrix(c("N_00","N_10","N_01","N_11"),ncol = 2)
rownames(ct) = colnames(ct) = c("0","1")
ct
```

## McNemar's Test

* The estimate of $p_{01}-p_{10}$ is 
$$\widehat{p}_{01} − \widehat{p}_{10} = (N_{01}/n) − (N_{10}/n)$$
same as difference in two proportions

## McNemar's Test Example

* Suppose A and B are two candidates for a political office who are having a debate
* Before and after the debate, the preference, A or B, of each member of the audience is recorded
* Given a change in preference of candidate, we are interested in the difference in the change from B to A minus the change from A to B
* If the estimate of this difference is significantly greater than 0, we might conclude that A won the debate
-->
