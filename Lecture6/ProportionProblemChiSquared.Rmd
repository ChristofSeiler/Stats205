---
title: "Proportion Problem and Chi-Squared Tests"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output:
  ioslides_presentation:
    incremental: yes
    smaller: no
    transition: faster
    widescreen: yes
  beamer_presentation:
    incremental: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Previous Lectures

* One-sample tests: 
    * Sign test
    * Signed-Rank Wilcoxon
* Estimators, confidence intervals, and robustness to outliers
* Bootstrap
    * Error of estimators and significance for hypothesis testing
    * Complete enumerations
    * Tail proabilities

## Today

* Proportion problems
* $\chi^2$ Tests

## Proportion Problems

* Discrete variables
* The random variable $X$ consists of categories
* For now, we focus on binary categries failure (0) and success (1)
* So, we say $X \sim \operatorname{Bern}(p)$ is a random variable distributed according to the Bernoulli distribution 
    * with success probabiliy $p$ and 
    * failure probabililty $1-p$
* We know that $\operatorname{E}(X) = p$ and $\operatorname{Var} = p(1-p)$

## Proportion Problems

* Statistical problems can be 
    * estimating $p$,
    * forming confidence intervals,
    * and testing hypothesis 
    $$H_0: p = p_0 \text{ versus } H_A: p \ne p_0$$
* Let $X_1,\dots,X_n \sim \operatorname{Bern}(p)$ and $S$ be the total number of successes
* Then $S \sim B(p,n)$ follows a binomial distribution
* If $p$ is unkown but we assume that our observation come from a Bernoulli distribution, then
* We can estimate $p$ as $\widehat{p} = \frac{S}{n}$

## Proportion Problems

```{r echo=TRUE}
n = 10; p = 1/2; nsim = 100; obsv = rbinom(nsim, size = n, prob = p)
```

```{r}
library(ggplot2)
df = data.frame(bern = table(obsv))
colnames(df) = c("S","Count")
ggplot(df, aes(x = S, y = Count)) + 
  geom_bar(stat="identity")
```

## Proportion Problems

```{r}
df <- data.frame(y = obsv)
ggplot(df, aes(sample = y)) + stat_qq() + ggtitle("Normal Q–Q Plot")
```

## Proportion Problems

```{r echo=TRUE}
n = 100; p = 1/2; nsim = 100; obsv = rbinom(nsim, size = n, prob = p)
```

```{r}
df = data.frame(bern = table(obsv))
colnames(df) = c("S","Count")
labels = floor(seq(0,n,length.out=25))
ggplot(df, aes(x = S, y = Count)) + 
  geom_bar(stat="identity") +
  scale_x_discrete(breaks=labels, labels=as.character(labels))
```

## Proportion Problems

```{r}
df <- data.frame(y = obsv)
ggplot(df, aes(sample = y)) + stat_qq() + ggtitle("Normal Q–Q Plot")
```

## Proportion Problems

* As $n \to \infty$ while $p$ is fixed:
    * The de Moivre–Laplace theorem (a special case of the central limit theorem) says $S$ approach a normal distribution
    * Easy confidence interval (just evaluate cdf of the normal)

## Example: Squeaky Hip Replacements

143 subjects with ceramic hip replacements

Ten report that their hip replacements squeaked
```{r echo=TRUE}
phat = 10/143
zcv = qnorm(0.975)
phat+c(-1,1)*zcv*sqrt(phat*(1-phat)/143)
```

We estimate between roughly 3 and 11% of patients who receive ceramic hip replacements will report squeaky replacements.

## Hypothesis Testing

* Test statistic is $z^2$ with
$$z = \frac{\widehat{p}-p_0}{\sqrt{p_0(1-p_0)/n}} \sim N(0,1)$$
* We are interested how large the observed proportion deviates this measured in terms of $|z|$
* Squared normal is distributed as $\chi^2$ distribution

## Example: Left-Handed Professional Ball Players

* Theory:  
    * Professional baseball players have different proportion of left handed player than left-handed people in genral population.
    * From previous study, we know that general public is has a proportion of $p = 0.15$.
* Hypothesis testing:
    * $H_0: p = 0.15 \text{ versus } H_A: p \ne 0.15$

## Example: Left-Handed Professional Ball Players

```{r warning=FALSE,echo=TRUE,message=FALSE}
library(Rfit)
head(baseball)
```

## Example: Left-Handed Professional Ball Players

```{r echo=TRUE}
ind = with(baseball,throw=='L')
n = length(ind)
phat = sum(ind)/n
phat
p0 = 0.15
z = (phat-p0)/(sqrt(p0*(1-p0)/n))
pvalue = 1-pchisq(z^2,df=1)
pvalue
```

## What is Nonparametrics Statistics Again?

* Rank-based methods: we do not estimate any additional parameters (exepct of course the parameter of interest $\theta$)
* Compare to $t$-test, we also have to estimate the variance
* Tests for discrete observations: No parameter estimation neither in finite sample or asymptotic case
* Think of nonparametric statistics as either:
    * discrete observations, thus we can enumerate everything
        * artificially by transforming continuous observations into ranks
        * observations are just measured on a discrete scale
    * ininite amount of parameters (inference of functions, we'll see that later in the course)

## Why Not Use Finite Sample Binomial Test?

Since we know that $S$ follows a binomial distribution, why shouldn't we use it?

```{r echo=FALSE}
binom.test(sum(ind),n,p=p0)
```

* Finite sample $p$-values have upper bounded significance level $\alpha$
* Asymptotic $p$-values may be above above $\alpha$

## Why Not Use Finite Sample Binomial Test?

```{r echo=TRUE}
n = 5; S = 5; p = 0.5
phat = S/n
pvalue = 2*p^n; pvalue
```

* Problem is due to discreteness
* More extreme case $n = 5$ and test $H_0: p = 0.5$ versus $H_A: p \ne 0.5$
* Under the null, this distribution is binomial with $n = 5$ and parameter $p = 0.5$
* Suppose outcome is $S = 5$ (most extreme observation)
* Problem is that the null hypethsis can never be true below our standard $\alpha = 0.05$
* So in this case, $\alpha$ has no meaning

## Discrete Random Variable

* Extension from two categories to multiple caterogries
* Consider discrete random variable $X$ with $1,2,\dots,c$ categories
* Let $p(j) = P(X = j)$ define the probabiliy mass function
* We wish to test:
$$H_0: p(j) = p_0(j), j = 1,\dots,c$$
$$H_A: p(j) \ne p_0(j), \text{ for some } j$$

## Discrete Random Variable

* Let $O_j = \#\{ X_i = j \}$
* Observed frequencies are constrained $\sum_{j=1}^c O_j = n$ 
* So $c-1$ degrees of freedom, if we know $c-1$ frequency we can calculate the $c$th by using the constrain
* The expected frequency for category $j$ is $\operatorname{E}_j = \operatorname{E}_{H_0}(O_j)$
* Two cases for $H_0$

## Discrete Random Variable

* Case 1: 
    * All $p_0(j)$ are specified
    * So we get $E_j = np_0(j)$
    * Test stastitics is
    $$\chi^2 = \sum_{j=1}^c \frac{(O_j-E_j)^2}{E_j}$$
* Hypothesis $H_0$ is rejected in favor of $H_A$ for large values of $\chi^2$

## Discrete Random Variable Example {.build}

* Roll a dice $n = 370$ times  
* Observe frequencies
```{r echo=TRUE}
O = c(58,55,62,68,66,61)
n = sum(O); n
```  
* Test whether dice is fair $p(j) \equiv 1/6$
```{r echo=TRUE}
p0 = 1/6
E = rep(n*p0,6)
Chi2_0 = sum((O-E)^2/E); Chi2_0
```

## Discrete Random Variable Example

Assymptotically equal to $\chi^2$ with $c-1$ degress of freedom

```{r echo=TRUE}
pvalue = 1-pchisq(Chi2_0,df=6-1); pvalue
```

## Discrete Random Variable

* Case 2: 
    * Only form of pmf is known
    * Have to estimate $p$
    * Same test stastitics but now with estimate $\widehat{p}$
    $$\chi^2 = \sum_{j=1}^c \frac{(O_j-E_j)^2}{E_j}$$
* Hypothesis $H_0$ is rejected in favor of $H_A$ for large values of $\chi^2$

## Discrete Random Variable Example {.build}

* Number of males in the first seven children for $n = 1334$ Swedish ministers of religion
```{r echo=TRUE}
males = 0:7
ministers = c(6,57,206,362,365,256,69,13)
n = sum(ministers); n
df = data.frame(ministers=ministers,males=males); t(df)
```
* For example, 206 of these ministers had 2 sons in their first 7 children

## Discrete Random Variable Example {.build}

* The maximum likelihood estimator of $p$ is
```{r echo=TRUE}
nChildren = n*7
nMale = sum(df$ministers*df$males)
phat = nMale/nChildren; phat
p0 = dbinom(males,7,phat)
E = n*p0
```

```{r}
df = data.frame(E=round(E,1),ministers=ministers,males=males)
t(df)
```

## Discrete Random Variable Example {.build}

```{r echo=TRUE}
Chi2_0 = sum((df$ministers-E)^2/E)
pvalue = 1-pchisq(Chi2_0,df=8-1-1); pvalue
```

## Discrete Random Variable

* Since $S$ converges in distribution to a normal, we could just evaluate cdf of normal
* Another interesting case where this is possible is for difference in proporitions
* The confidence interval for $p_j - \widehat{p}_k$
* As before the $p_j = O_j/n$ are the proportion of samples of category $j$, then
$$\widehat{p}_j - \widehat{p}_k \pm z_{\alpha/2} \sqrt{\frac{\widehat{p}_j+\widehat{p}_k-(\widehat{p}_j-\widehat{p}_k)^2}{n}}$$

## Discrete Random Variable Example {.build}

* Difference in the probabilities of all daughters or all sons
```{r}
t(df[,-1])
```
* 6 ministers had no sons, and 13 ministers had all sons
```{r echo=TRUE}
n = 1334; p0 = 6/n; p7 = 13/n
se = sqrt((p0+p7-(p0-p7)^2)/n)
zcv = qnorm(0.975)
lb = p0-p7 - zcv*se; ub <- p0-p7 + zcv*se; res = c(p0-p7,lb,ub); res
```
* Confidence interval covers 0, thus no significant difference in the proportions

## Several Discrete Random Variables

TODO

## Independence of Two Discrete Random Variables

TODO

## McNemar's Test

TODO
