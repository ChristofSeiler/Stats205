---
title: "Proportion Problem, Chi-Squared Test, and McNemar’s Test"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output:
  ioslides_presentation:
    incremental: yes
    smaller: no
    transition: faster
    widescreen: yes
  beamer_presentation:
    incremental: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Previous Lectures

* One-sample tests: 
    * Sign test
    * Signed-Rank Wilcoxon
* Estimators, confidence intervals, and robustness to outliers
* Bootstrap
    * Error of estimators and significance for hypothesis testing
    * Complete enumerations
    * Tail proabilities

## Today

* One- and two-sample proportion problems
* $\chi^2$ Tests
* McNemar’s Test

## Proportion Problems

* Discrete variables
* The random variable $X$ consists of categories
* For now, we focus on binary categries failure (0) and success (1)
* So, we say $X \sim \operatorname{Bern}(p)$ is a random variable distributed according to the Bernoulli distribution 
    * with success probabiliy $p$ and 
    * failure probabililty $1-p$
* We know that $\operatorname{E}(X) = p$ and $\operatorname{Var} = p(1-p)$

## Proportion Problems

* Statistical problems can be 
    * estimating $p$,
    * forming confidence intervals,
    * and testing hypothesis 
    $$H_0: p = p_0 \text{ versus } H_A: p \ne p_0$$
* Let $X_1,\dots,X_n \sim \operatorname{Bern}(p)$ and $S$ be the total number of successes
* Then $S \sim B(p,n)$ follows a binomial distribution
* If $p$ is unkown but we assume that our observation come from a Bernoulli distribution, then
* We can estimate $p$ as $\widehat{p} = \frac{S}{n}$

## Proportion Problems

```{r echo=TRUE}
n = 10; p = 1/2; nsim = 100; obsv = rbinom(nsim, size = n, prob = p)
```

```{r}
library(ggplot2)
df = data.frame(bern = table(obsv))
colnames(df) = c("S","Count")
ggplot(df, aes(x = S, y = Count)) + 
  geom_bar(stat="identity")
```

## Proportion Problems

```{r}
df <- data.frame(y = obsv)
ggplot(df, aes(sample = y)) + stat_qq() + ggtitle("Normal Q–Q Plot")
```

## Proportion Problems

```{r echo=TRUE}
n = 100; p = 1/2; nsim = 100; obsv = rbinom(nsim, size = n, prob = p)
```

```{r}
df = data.frame(bern = table(obsv))
colnames(df) = c("S","Count")
labels = floor(seq(0,n,length.out=25))
ggplot(df, aes(x = S, y = Count)) + 
  geom_bar(stat="identity") +
  scale_x_discrete(breaks=labels, labels=as.character(labels))
```

## Proportion Problems

```{r}
df <- data.frame(y = obsv)
ggplot(df, aes(sample = y)) + stat_qq() + ggtitle("Normal Q–Q Plot")
```

## Proportion Problems

* As $n \to \infty$ while $p$ is fixed:
    * The de Moivre–Laplace theorem (a special case of the central limit theorem) says $S$ approach a normal distribution
    * Easy confidence interval (just evaluate cdf of the normal)

## Example: Squeaky Hip Replacements

143 subjects with ceramic hip replacements

Ten report that their hip replacements squeaked
```{r echo=TRUE}
phat = 10/143
zcv = qnorm(0.975)
phat+c(-1,1)*zcv*sqrt(phat*(1-phat)/143)
```

We estimate between roughly 3 and 11% of patients who receive ceramic hip replacements will report squeaky replacements.

## Hypothesis Testing

* Test statistic is $z^2$ with
$$z = \frac{\widehat{p}-p_0}{\sqrt{p_0(1-p_0)/n}} \sim N(0,1)$$
* We are interested how large the observed proportion deviates this measured in terms of $|z|$
* Squared normal is distributed as $\chi^2$ distribution

## Example: Left-Handed Professional Ball Players

* Theory:  
    * Professional baseball players have different proportion of left handed player than left-handed people in genral population.
    * From previous study, we know that general public is has a proportion of $p = 0.15$.
* Hypothesis testing:
    * $H_0: p = 0 \text{ versus } H_A: p \ne 0.15$

## Example: Left-Handed Professional Ball Players

```{r warning=FALSE,echo=TRUE,message=FALSE}
library(Rfit)
head(baseball)
```

## Example: Left-Handed Professional Ball Players

```{r echo=TRUE}
ind = with(baseball,throw=='L')
n = length(ind)
phat = sum(ind)/n
phat
p0 = 0.15
z = (phat-p0)/(sqrt(p0*(1-p0)/n))
pvalue = 1-pchisq(z^2,df=1)
pvalue
```

## What is Nonparametrics Statistics Again?

* Rank-based methods: we do not estimate any additional parameters (exepct of course the parameter of interest $\theta$)
* Compare to $t$-test, we also have to estimate the variance
* Tests for discrete observations: No parameter estimation neither in finite sample or asymptotic case
* Think of nonparametric statistics as either:
    * discrete observations, so we can enumerate everyting (artificially by ranking, or observations are measured that way)
    * ininite amount of parameters (inference of functions, we'll see that later in the course)

## Why Not Use Finite Sample Binomial Test?

Since we know that $S$ follows a binomial distribution, why shouldn't we use it?

```{r echo=FALSE}
binom.test(sum(ind),n,p=p0)
```

* Finite sample $p$-values have upper bounded significance level $\alpha$
* Asymptotic $p$-values may be above above $\alpha$

## Why Not Use Finite Sample Binomial Test?

```{r echo=TRUE}
n = 5; S = 5; p = 0.5
phat = S/n
pvalue = 2*p^n; pvalue
```

* Problem is due to discreteness
* More extreme case $n = 5$ and test $H_0: p = 0.5$ versus $H_A: p \ne 0.5$
* Under the null, this distribution is binomial with $n = 5$ and parameter $p = 0.5$
* Suppose outcome is $S = 5$ (most extreme observation)
* Problem is that the null hypethsis can never be true below our standard $\alpha = 0.05$
* So in this case, $\alpha$ has no meaning

## $\chi^2$ Test

* Extension from two categories to multiple caterogries
* Consider discrete random variable $X$ with $1,2,\dots,c$ categories
* Let $p(j) = P(X = j)$ define the probabiliy mass function
* We wish to test:
$$H_0: p(j) = p_0(j), j = 1,\dots,c$$
$$H_A: p(j) \ne p_0(j), \text{ for some } j$$

## $\chi^2$ Test

* Let $O_j = \#\{ X_i = j \}$
* Observed frequencies are constrained $\sum_{j=1}^c O_j = n$ 
* So $c-1$ degrees of freedom, if we know $c-1$ frequency we can calculate the $c$th by using the constrain
* The expected frequency for category $j$ is $\operatorname{E}_j = \operatorname{E}_{H_0}(O_j)$
* Two cases for $H_0$

## $\chi^2$ Test

* Case 1: 
    * All $p_0(j)$ are specified
    * So we get $E_j = np_0(j)$
    * Test stastitics is
    $$\chi^2 = \sum_{j=1}^c \frac{(O_j-E_j)^2}{E_j}$$
* Hypothesis $H_0$ is rejected in favor of $H_A$ for large values of $\chi^2$

## $\chi^2$ Test Example {.build}

* Roll a dice $n = 370$ times  
* Observe frequencies
```{r echo=TRUE}
O = c(58,55,62,68,66,61)
n = sum(O); n
```  
* Test whether dice is fair $p(j) \equiv 1/6$
```{r echo=TRUE}
p0 = 1/6
E = rep(n*p0,6)
Chi2_0 = sum((O-E)^2/E); Chi2_0
```

## $\chi^2$ Test Example

Assymptotically equal to $\chi^2$ with $c-1$ degress of freedom

```{r echo=TRUE}
pvalue = 1-pchisq(Chi2_0,df=6-1); pvalue
```

## $\chi^2$ Test

* Case 2: 
    * Only form of pmf is known
    * So we get $E_j = np_0(j)$
    * Test stastitics is
    $$\chi^2 = \sum_{j=1}^c \frac{(O_j-E_j)^2}{E_j}$$
* Hypothesis $H_0$ is rejected in favor of $H_A$ for large values of $\chi^2$


