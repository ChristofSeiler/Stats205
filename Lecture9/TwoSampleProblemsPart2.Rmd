---
title: "Two-Sample Problems (Part 2)"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output:
  ioslides_presentation:
    incremental: yes
    smaller: no
    transition: faster
    widescreen: yes
  beamer_presentation:
    incremental: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(1234)
library(ggplot2)
library(reshape2)
library(npsm)
```

## Analyses for a Shift in Location

* The estimate of $\Delta$ is the Hodges–Lehmann estimator
$$ \widehat{\Delta} = \operatorname{median}_{i,j} \{ Y_j - X_i \}$$
* There are $n_1 n_2$ differences
* Order the differnces $D_{(1)} < D_{(2)} < \dots < D_{(n_1 n_2)}$
* Fix confidence level at $1 - \alpha$
* Find $c$ such that
$$ \alpha/2 = \operatorname{P}_{H_0} ( T^+ \le c ) $$
* Then the interval $(D_{(c+1)},D_{(n-c)})$ is $(1-\alpha) 100\%$ confidence interval for $\widehat{\Delta}$

<!--
* The asymptotic interval for $c$ is $c = \frac{n_1 n_2}{2} - \frac{1}{2} - z_{\alpha/2} \sqrt{\frac{n_1 n_2 (n + 1)}{12}}$
-->

## Analyses for a Shift in Location Example

$t$-distribution with 5 degrees of freedom and  
a true shift parameter $\Delta$ was set at the value $8$ 
```{r echo=TRUE}
n1 = 42
n2 = 50
trails = 10
x = round(rt(11,5)*trails+n1,1)
y = round(rt(9,5)*trails+n2,1)
sort(x)
sort(y)
```

## Analyses for a Shift in Location Example

Estimate of shift parameter $\Delta$ and confidence intervals:
```{r echo=TRUE}
wilcox.test(y,x,conf.int=TRUE)
```

## Linear Regression Model

* Frame the two-sample location problem as a regression problem
* Combine sample in one vector $\boldsymbol{Z} = (X_1,\dots,X_{n_1},Y_1,\dots,Y_{n_2}^T)$
* Let $c$ be a $n \times 1$ vector with 
    * zeros at position $1$ to $n_1$ and 
    * ones at positions $n_1+1$ to $n$
* Then we can rewrite the location model as
$$Z_i = \alpha + c_i \Delta + e_i$$
where $e_1,\dots,e_n$ are iid with pdf $f(t)$
* We can use method of Least Squares (SL) to estimate $\widehat{\Delta}$
* Or the Hodges–Lehmann estimator
* The intercept $\alpha$ is estimated in a second step on the residuals

## Efficiency of Estimator

* Suppose, two estimators $\widehat{\Delta}_1,\widehat{\Delta}_2$ converge in distribution 
$$ \sqrt{n} \left( \widehat{\Delta}_i - \Delta \right) \overset{d}{\to} N(0,\sigma^2_i) \text{ for } i = 1,2$$
* Asymptotic Relative Efficiency (ARE) between two estimators $\widehat{\Delta}_1$ and $\widehat{\Delta}_2$ is defined as:
$$ \operatorname{ARE}\left(\widehat{\Delta}_1,\widehat{\Delta}_2\right) = \frac{\sigma_2^2}{\sigma_1^2}$$

## Efficiency of Estimator

Contaminated normal $(0 < \epsilon < 0.5, \sigma_c > 1)$: $$F(x) = (1 − \epsilon) \Phi(x) + \epsilon \Phi(x/\sigma_c)$$

```{r echo=TRUE}
n = 10000
sigmaC = 3
epsilon = 0.25
sample = c(rnorm((1-epsilon)*n,0,1),rnorm(epsilon*n,0,sigmaC))
```

## Efficiency of Estimator

```{r fig.width=10,fig.height=4}
library(gridExtra)
library(ggplot2)
df = data.frame(sample=sample)
p1 = ggplot(df, aes(sample)) + geom_histogram(binwidth = 0.1) + ggtitle("Contaminated Normal")
p2 = ggplot(df, aes(sample = sample)) + stat_qq() + ggtitle("Normal Q–Q Plot")
grid.arrange(p1, p2, ncol=2)
```

```{r}
epsilon = c(0.00,0.01,0.02,0.03,0.05,0.10,0.15,0.25)
ARE = c(0.955,1.009,1.060,1.108,1.196,1.373,1.497,1.616)
con = rbind(epsilon,ARE)
rownames(con) = c("epsilon","ARE(Hodges–Lehmann,LS)")
con
```

## Analyses for a Shift in Location

```{r}
t = seq(0,10,0.01)
df = data.frame(t=t,
                g=dnorm(t,mean = 3,sd = 0.7),
                f=dnorm(t,mean = 5,sd = 0.7))
df = melt(df,id = "t")
ggplot(data = df,aes(x=t,y=value,group=variable,colour=variable)) + geom_line(size=1)
```

## Analyses for a Shift in Location

* Random sample $X_1,\dots,X_{n_1}$ with cdf $F(t)$ and pdf $f(t)$
* Random sample $Y_1,\dots,Y_{n_2}$ with cdf $F(t-\Delta)$ and pdf $f(t-\Delta)$
* Hypothesis test: $$H_0: \Delta = 0 \hspace{1cm} \text{versus} \hspace{1cm} H_A: \Delta \ne 0$$
* Additionally, we can esimate $\widehat{\Delta}$ and form confidence intervals
* Wilcoxon test statistics:
$$ T = \sum_{j=1}^{n_2} \operatorname{R}(Y_j)$$
among the combined samples $X_1,\dots,X_{n1},Y_1,\dots,Y_{n2}$

## Test for Dispertion

```{r}
t = seq(-5,5,0.01)
df = data.frame(t=t,
                g=dnorm(t,mean = 0,sd = 1.5),
                f=dnorm(t,mean = 0,sd = 1))
df = melt(df,id = "t")
ggplot(data = df,aes(x=t,y=value,group=variable,colour=variable)) + geom_line(size=1)
```

## Test for Dispertion

* Same as before, we assume that there are two populations with cdf $F$ and $G$
* The null hypothesis is that $X$ and $Y$ have the same but unspecified distribution
$$ H_0: F(t) = G(t) $$
* Consider the alternative that $X$ and $Y$ have different variability with same median 
$$ \frac{X-\theta}{\sigma_1} \overset{d}{=} \frac{X-\theta}{\sigma_2} $$
* Then, what's left to analysis is the dispertion
$$ \eta^2 = \frac{\sigma_1}{\sigma_2} = \frac{\operatorname{Var}(X)}{\operatorname{Var}(Y)}$$

## Test for Dispertion

* So that our null hypothesis reduces to 
$$ H_0: \eta^2 = 1 $$
* We will use the Ansari-Bradley two-sample scale statistic $C$
* Rank combined sample from smallest to largest
* Assign score 1 to smallest and largest
* Assign socre 2 to second smallest and largest
* etc.
* If $n$ is even: $1,2,3,\dots,n/2,n/2,\dots,3,2,1$
* If $n$ is odd: $1,2,3,\dots,(n-1)/2,(n+1)/2,(n-1)/2,\dots,3,2,1$
* then the statistic is a function of these ranks $C = \sum_{j=1}^{n_2} R(Y_j)$

## Placement Test for the Behrens–Fisher Problem

```{r message=FALSE}
hg = c(227,250,277,290,297,325,337,340)
lg = c(293,291,289,430,510,353,318)
df = data.frame(geese=c(rep("healthy",length(hg)),rep("lead-poisoned",length(lg))),plasma=c(hg,lg))
ggplot(df, aes(x=factor(geese),fill=factor(geese),y=plasma)) +
  geom_dotplot(binaxis = "y", stackdir = "center", binpositions="all")
```

## Placement Test for the Behrens–Fisher Problem

* Suppose that we have two populations which differ by location and scale and we are interested in testing that the locations are the same
* Random sample $X_1,\dots,X_{n_1}$ with cdf $F(t)$
* Random sample $Y_1,\dots,Y_{n_2}$ with cdf $G(t)$
* Let $θ_X$ and $θ_Y$ denote the medians of the distributions $F(t)$ and $G(t)$
* Hypothesis test
$$ H_0: \theta_X = \theta_Y \hspace{1cm} \text{versus} \hspace{1cm} H_A: \theta_X \ne \theta_Y$$
* This is called the Behrens–Fisher problem and the traditional test in this situation is the two-sample $t$-test which uses a $t$-statistic with the Satterthwaite degrees of freedom correction
* There is a two-sample Fligner-Policello test which serves as a robust alternative to this approximate $t$-test

## Placement Test for the Behrens–Fisher Problem

* Assume that the cdfs $F(t)$ and $G(t)$ are symmetric about $\theta_X$ and $\theta_Y$ 
* Let $P1,\dots,P_{n1}$ denote the placements of the $X_i$'s in terms of the $Y$-sample
$$P_i = \#_j \{ Y_j < X_i \}, i = 1,\dots,n_1$$
* Let $Q_1,\dots,Q_n$ denote the placements of the $Y_j$'s in terms of the $X$-sample
$$Q_j = \#_i \{X_i < Y_j \}, j=1,\dots,n_2$$

<!--
* Null expectation of $T+$ same as in the location problem but different null variance
* Mann–Whitney–Wilcoxon test statistic
$$ T^+ = \#_{i,j} \{ X_i < Y_j \} = \sum_{j=1}^{n_2} \operatorname{R}(Y_j) - \frac{n_2(n_2+1)}{2}$$
-->

## Placement Test for the Behrens–Fisher Problem

* Define 
$$\bar{P} = \frac{1}{n_1} \sum_{i=1}^{n_1} P_i \hspace{1cm}\text{and}\hspace{1cm} \bar{Q} = \frac{1}{n_2} \sum_{j=1}^{n_2} Q_j$$
$$V_1 = \sum_{i=1}^{n_1} (P_i-\bar{P})^2 \hspace{1cm}\text{and}\hspace{1cm} V_2 = \sum_{j=1}^{n_2} (Q_j-\bar{Q})^2$$
* Then the standardized test statistic is
$$ U = \frac{\sum_{j=1}^{n_2} Q_j - \sum_{i=1}^{n_1} P_j}{2 \left(V_1 + V_2 + \bar{P}\bar{Q}\right)^{1/2}} $$

## Placement Test for the Behrens–Fisher Problem Example

* Hollander and Wolfe (1999) a study of healthy and lead-poisoned geese 
* The study involved 7 healthy geese and 8 lead-poisoned geese
* The response of interest was the amount of plasma glucose in the geese in mg/100 ml of plasma
* The hypotheses of interest are:
$$H_0:θ_L = θ_H \hspace{1cm}\text{vesus}\hspace{1cm} H_A: θ_L > θ_H$$
* $θ_L$ and $θ_H$ denote the true median plasma glucose values of lead-poisoned geese and healthy geese

## Placement Test for the Behrens–Fisher Problem Example

```{r echo=TRUE}
hg
lg
fp.test(hg,lg,alternative='greater')
```


## Placement Test for the Behrens–Fisher Problem

Monte Carlo simulation of distibution of $U$ under the null

```{r echo=TRUE}
n1 = length(hg); n2 = length(lg); n = n1+n2; nSim = 10000

Yj = replicate(nSim,sample(n,n2,replace = FALSE))
getXi = function(j) (1:n)[-j]
Xi = apply(Yj,2,getXi)

U = function(Xi,Yj) {
  Pi = function(i) { sum(Yj < Xi[i]) }; Qj = function(j) { sum(Xi < Yj[j]) }
  P = sapply(1:n1,Pi); Q = sapply(1:n2,Qj)
  Phat = mean(P); Qhat = mean(Q)
  V1 = sum((P-Phat)^2); V2 = sum((Q-Qhat)^2)
  (sum(Q)-sum(P))/(2*sqrt(V1+V2+Phat*Qhat)) }

UNull = rep(0,nSim)
for(trial in 1:nSim) { UNull[trial] = U(Xi[,trial],Yj[,trial]) }
```

<!--
from Kloke & McKean book:
Tplus = function(Yj) { sum(Yj) + n2*(n2+1)/2 }
Zfp = ((Tplus(Yj[,trial])-n1*n2)/2) / sqrt(V1+V2+Phat*Qhat)
-->

## Placement Test for the Behrens–Fisher Problem

Test statistic of our sample

```{r echo=TRUE}
ranks = rank(c(hg,lg))
XiObsv = ranks[1:n1]
YjObsv = ranks[(n1+1):n]
UObsv = U(XiObsv,YjObsv)
UObsv
pvalue = mean(UNull > UObsv)
pvalue
```

## Placement Test for the Behrens–Fisher Problem

```{r warning=FALSE}
df = data.frame(UNull=UNull)
ggplot(df, aes(UNull)) + geom_histogram(binwidth = 0.1) + 
  ggtitle("Null Distribution of U") + 
  geom_vline(xintercept = UObsv,colour = "red",size = 1.5)
```

<!--
## Scale Problem

* Besides differences in location, we are often interested in the difference between scales for populations
* Random sample $X_1,\dots,X_{n_1}$ with common pdf $f\left( (x-\theta_1)/\sigma_1 \right)$, $\sigma_1 > 0$
* Random sample $Y_1,\dots,Y_{n_2}$ with common pdf $f\left( (y-\theta_2)/\sigma_2 \right)$, $\sigma_2 > 0$
* Hypothesis test ($\eta = \sigma_2/\sigma_1$)
$$ H_0: \eta = 1 \hspace{1cm} \text{versus} \hspace{1cm} H_A: \eta \ne 1 $$
* Besides discussing rank-based tests for these hypotheses, we also consider the associated estimation of $\eta$, along with a confidence interval for $\eta$
* So here the location parameters $\theta_1$ and $\theta_2$ are nuisance parameters

## Scale Problem

* Fligner–Killeen procedure is a asymptotically distribution-free rank-based procedure
* First align samples
$$ X_i^* = X_i - \operatorname{median}(X_l), i = 1,\dots,n_1 $$
$$ Y_j^* = Y_j - \operatorname{median}(Y_l), i = 1,\dots,n_2 $$
* Folded samples: $|X_1^*|,\dots,|X_{n_1}^*|,|Y_1^*|,\dots,|Y_{n_2}^*|$
* Define $Z_i$ as 
$$ 
Z_i = 
\begin{cases}
\log |X_i^*| & i = 1,\dots,n_1 \\
\log |Y_{i-n_1}^*| & j = n_1 + 1,\dots,n
\end{cases}
$$

## Scale Problem

* Let $c$ be the indicator vector with its first $n_1$ entries set at $0$ and its last $n_2$ entries set at $1$
* Then the log-linear model for the aligned, folded sample is
$$ Z_i = \Delta c_i + e_i, i = 1,2,\dots,n$$
-->

<!--
## Analyses for a Shift in Location

* Wilcoxon test equivalently to
$$ T_W = \sum_{i=1}^{n_2} a(\operatorname{R}(Y_i)) $$
* when $a(i) = \varphi_W (i/(n+1))$ and $\varphi_W(u) = \sqrt{12} (u-(1/2))$
* with the following identity
$$T_W = \frac{\sqrt{12}}{n+1} \left( T - \frac{n_2(n+1)}{2} \right)$$
* Hence $T_W$ is a linear function of the ranks 
* $\varphi_W(u)$ is called the Wilcoxon score function, and
* $a_W(i)$ are called Wilcoxon scores

## Normal Scores

* When taking $a(i) = \varphi_{ns} (i/(n+1))$ and normal cdf $\Phi$
$$\varphi_{ns}(u) = \Phi^{-1}(u)$$
* then the scores follow a normal distribution with $\operatorname{E}(T_{ns}) = 0$ and 
$$\operatorname{Var}(T_{ns}) = \frac{n_1 n_2}{n-1} \sum_{i=1}^n a_{ns}^2(i)$$
* We can then reject using asymptotic level $\alpha$ when 
$$\left| \frac{T_{ns}}{\sqrt{\operatorname{Var}_{H_0}(T_{ns})}} \right| > z_{\alpha/2}$$

## Analyses Based on General Score Functions

* Set of rank-based scores is generated by $\varphi(u)$ on interval $(0,1)$
* Assume that $\varphi(u)$ is square integrable 
$$ \int_0^1 \varphi(u) \, du = 0 \hspace{1cm} \text{and} \hspace{1cm} \int_0^1 \varphi^2(u) \, du = 1 $$

## Efficiency and Optimal Scores
-->
