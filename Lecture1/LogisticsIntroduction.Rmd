---
title: "Logistics and Introduction"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output: 
  ioslides_presentation: 
    smaller: yes
    transition: faster
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Stats 205

* You can find everything on our course website: http://christofseiler.github.io/stats205/
* Homeworks every 8 days (send solutions to TA's)
* Finals will be a project
* Instructor's office hours (TBD): Wednesday 2:45 - 3:45 in room ...
* TA's Office hours (TBD): 2 x 1.5 hours in room ...

## Why?

1. Few assumptions about underlying populations from which data is obtained, e.g. populations don't need to follow a normal distribution
2. Often easier to understand and apply than parametric tests
3. Slightly less efficient than parametric test when parametric assumptions hold, but if assumptions don't hold then wildly more efficient
4. Jackknife and bootstrap can be used in many practical situations where theory is intractable
5. Bayesian methods are available so prior information can be incorporated

## Goals

Overview: 

* Get an overview of classical, Bayesian, and modern methods
* Learn how to implement methods yourself and use existing R packages
* Be aware and understand underlying assumptions
* Apply to modern data analysis problems that you care about

Specific learning goals:

1. Monte Carlo simulations for analytically intractable problems
2. Rank-based methods for hypothesis testing and parameter estimation
3. Permutations test for two sample tests
4. Bootstrap for uncertainty quantification

## Textbooks

Our main textbook with lots of practical computations in R [(Stanford library link)](https://searchworks.stanford.edu/view/10718635): 

* Kloke and McKean (2015). Nonparametric Statistical Methods Using R

In-depth coverage of the bootstrap with lot of examples:

* Efron and Tibshirani (1994). An Introduction to the Bootstrap

## More Textbooks

A Bayesian view [(Stanford library link)](https://searchworks.stanford.edu/view/11351113):

* Müller, Quintana, Jara, and Hanson (2015). Bayesian Nonparametric Data Analysis

Very comprehensive covering most of the material of the previous books [(Stanford library link)](https://searchworks.stanford.edu/view/10356739):

* Hollander and Wolfe, and Chicken (2013). Nonparametric Statistical Methods

Classical textbook for rank-based method with lot of mathematical details:

* Lehmann (2006). Nonparametrics Statistical Methods Based on Ranks

## Grading: Homework and Projects

* Weekly homework assignments (40%), mostly R exercises
* Project (60%)

Goal of project: Write a paper on 

* applying nonparametric statistics to your field of interest or 
* study one particular theoretical aspects that your care about

## Grading: Details on Projects

The project will be split in two parts:

1. Midterm project (3 pages with references) (20%):
    + Project proposal and outline of planned tasks 

2. Final project (12 pages plus references) (40%):
    + A theoretical part: Explanation of the method studied and its properties
    + A computational part: Preferably in R
    + A data-analysis part: Plots and interpretations


## Modern Statistics

To quote Andrew Gelman [(source)](http://arxiv.org/pdf/1001.2968.pdf):

```
          "If you wanted to do foundational research 
          in statistics in the mid-twentieth century, 
          you had to be bit of a mathematician, 
          ... if you want to do statistical research 
          at the turn of the twenty-first century, 
          you have to be a computer programmer."
```

## History of Nonparametric Statistics

Check on Google Ngram Viewer.

```{r warning = FALSE,echo = FALSE}
library(ggplot2)
library(ngramr)
#ng = ngram(c("Nonparametric Statistics"), year_start = 1950)
#ggplot(ng, aes(x=Year, y=Frequency, colour=Phrase)) + geom_line()
```

## History 1930's to 1970's

* Beginning of nonparametric statistics (Hotelling and Pabst 1936)
* Wilcoxon (1945) introduced the two-sample rank sum test for equal sample sizes, and Mann and Whitney (1947) generalize it
* Pitman (1948), Hodges and Lehmann (1956), and Chernoff and Savage (1958) showed desirable efficiency properties
* Jackknife, introduced by Quenouille (1949) as a bias-reduction technique and extended by Tukey (1958, 1962) to provide approximate significance tests and confidence intervals
* Hodges and Lehmann (1963) showed how to derive estimators from rank tests and established that these estimators have desirable properties
* Cox (1972) model and methods for survival analysis

## History 1970’s to Now

Efron’s (1979) bootstrap makes use of increasing computational resources to provide standard errors and confidence intervals where difficult, if not impossible, to use a parametric approach

Some examples from special issue of Statistical Science (Randles, Hettmansperger, and Casella, 2004):

* Robust analysis of linear models (McKean, 2004)
* Density estimation (Sheather, 2004)
* Data modeling via quantile methods (Parzen, 2004)
* Kernel smoothers (Schucany, 2004)
* Permutation-based inference (Ernst, 2004)
* Multivariate signed rank tests in time series problems (Hallin and Paindaveine, 2004)
* Generalizations for nonlinear manifolds (Patrangenaru and Ellingson 2015)

## Bayesian History

* Ferguson (1973) introduced nonparametric Bayesian methods
* Susarla and van Ryzin (1976) derived nonparametric Bayesian estimators of survival curves
* Dykstra and Laud (1981) developed a Bayesian nonparametric approach to reliability
* Hjort (1990b) proposed nonparametric Bayesian estimators to model the cumulative hazard
* In the late 1980s and the 1990s, there was a surge of activity in Bayesian methods due to the Markov chain Monte Carlo (MCMC) methods, e.g. Gelfand and Smith (1990), Gamerman (1991), West (1992), Smith and Roberts (1993), and Arjas and Gasbarra (1994)
* Key algorithms for developing and implementing modern Bayesian methods include the Metropolis-Hastings-Green algorithm (see Metropolis et al. (1953), Hastings (1970), and Green (1995)) and the Tanner-Wong (1987) data augmentation algorithm

## R and R Markdown Basics

[Start session](RBasics.html)

## Rank-Based Method Example

$X_1,X_2,\dots,X_n$ denotes random sample following the model
$$ X_i = \theta + e_i, $$
random errors are independently and identically distributed with a continuous probability density function $f(t)$ symmetric around $0$.

We test hypothesis:
$$ H_0: \theta = 0 \text{ versus } H_A: \theta > 0 $$

The null distribution of the test statistic did not depend on $f(t)$

[Start R markdown session](Examples.html#signed_rank_wilcoxon)

## Bootstrap Example

We can computed confidence intervals of an estimate using computer simulations

[Start R markdown session](Examples.html#bootstrap)

## What's Next

Closer look at the boostrap and Monte Carlo simulations.
