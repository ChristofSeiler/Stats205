---
title: "Logistics and Introduction"
author: "Christof Seiler"
date: "Stanford University, Spring 2016, STATS 205"
output: 
  ioslides_presentation: 
    smaller: yes
    transition: faster
    widescreen: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Stats 205

* You can find everything on our course website: http://christofseiler.github.io/stats205/
* Homework every 8 days (send solutions to TA's)
* Midterm and finals will be a project
* Instructor's office hours: Wednesdays 10:00 to 11:30 in Sequoia Hall 105
* TA's Office hours: 2 x 1.5 hours (time and place will be announced)

## Why?

1. Few assumptions about underlying populations from which data is obtained, e.g. populations don't need to follow a normal distribution
2. Often easier to understand and apply than parametric tests
3. Slightly less efficient than parametric test when parametric assumptions hold, but if assumptions don't hold then wildly more efficient
4. Jackknife and bootstrap can be used in many practical situations where theory is intractable
5. Bayesian methods are available so prior information can be incorporated

## Goals

### Overview

* Get an overview of classical, Bayesian, and modern methods
* Learn how to implement methods yourself and use existing R packages
* Be aware and understand underlying assumptions
* Apply to modern data analysis problems that you care about

### Specific learning goals

The students will learn to apply and explain the statistical assumptions of 

1. Monte Carlo simulations for analytically intractable problems
2. rank-based methods for parameter estimation, confidence Intervals, and hypothesis testing
3. permutations test for hypothesis testing
4. the bootstrap for confidence intervals

## Textbooks

Our main textbook with lots of practical computations in R [(Stanford library link)](https://searchworks.stanford.edu/view/10718635): 

* Kloke and McKean (2015). Nonparametric Statistical Methods Using R

In-depth coverage of the bootstrap with lot of examples:

* Efron and Tibshirani (1994). An Introduction to the Bootstrap

## More Textbooks

A Bayesian view [(Stanford library link)](https://searchworks.stanford.edu/view/11351113):

* Müller, Quintana, Jara, and Hanson (2015). Bayesian Nonparametric Data Analysis

Very comprehensive covering most of the material of the previous books [(Stanford library link)](https://searchworks.stanford.edu/view/10356739):

* Hollander and Wolfe, and Chicken (2013). Nonparametric Statistical Methods

Classical textbook for rank-based method with lot of mathematical details:

* Lehmann (2006). Nonparametrics Statistical Methods Based on Ranks

## Grading: Homework and Projects

* Weekly homework assignments (40%), mostly R exercises
* class participation (10%)
* Project (50%)

Goal of project: Write a paper on 

* applying nonparametric statistics to your field of interest or 
* study one particular theoretical aspects that your care about

## Grading: Details on Projects

The project will be split in two parts:

1. Midterm project (3 pages with references) (10%):
    + Project proposal and outline of planned tasks 

2. Final project (12 pages plus references) (40%):
    + A theoretical part: Explanation of the method studied and its properties
    + A computational part: Preferably in R
    + A data-analysis part: Plots and interpretations


## Modern Statistics

To quote Andrew Gelman [(source)](http://arxiv.org/pdf/1001.2968.pdf):

``
"If you wanted to do foundational research in statistics in the mid-twentieth century, you had to be bit of a mathematician, ... if you want to do statistical research at the turn of the twenty-first century, you have to be a computer programmer."
``

## History of Nonparametric Statistics

Check on Google Ngram Viewer.

```{r warning = FALSE,echo = FALSE,eval = TRUE}
library(ggplot2)
library(ngramr)
ng = ngram(c("Nonparametric Statistics"), year_start = 1950)
ggplot(ng, aes(x=Year, y=Frequency, colour=Phrase)) + geom_line()
```

## History 1930's to 1970's

* Beginning of nonparametric statistics (Hotelling and Pabst 1936)
* Wilcoxon (1945) introduced the two-sample rank sum test for equal sample sizes, and Mann and Whitney (1947) generalize it
* Pitman (1948), Hodges and Lehmann (1956), and Chernoff and Savage (1958) showed desirable efficiency properties
* Jackknife, introduced by Quenouille (1949) as a bias-reduction technique and extended by Tukey (1958, 1962) to provide approximate significance tests and confidence intervals
* Hodges and Lehmann (1963) showed how to derive estimators from rank tests and established that these estimators have desirable properties
* Cox (1972) model and methods for survival analysis

## History 1970’s to Now

Efron’s (1979) bootstrap makes use of increasing computational resources to provide standard errors and confidence intervals where difficult, if not impossible, to use a parametric approach

Some examples from special issue of Statistical Science (Randles, Hettmansperger, and Casella, 2004):

* Robust analysis of linear models (McKean, 2004)
* Density estimation (Sheather, 2004)
* Data modeling via quantile methods (Parzen, 2004)
* Kernel smoothers (Schucany, 2004)
* Permutation-based inference (Ernst, 2004)
* Multivariate signed rank tests in time series problems (Hallin and Paindaveine, 2004)
* Generalizations for nonlinear manifolds (Patrangenaru and Ellingson 2015)

## Bayesian History

* Ferguson (1973) introduced nonparametric Bayesian methods
* Susarla and van Ryzin (1976) derived nonparametric Bayesian estimators of survival curves
* Dykstra and Laud (1981) developed a Bayesian nonparametric approach to reliability
* Hjort (1990b) proposed nonparametric Bayesian estimators to model the cumulative hazard
* In the late 1980s and the 1990s, there was a surge of activity in Bayesian methods due to the Markov chain Monte Carlo (MCMC) methods, e.g. Gelfand and Smith (1990), Gamerman (1991), West (1992), Smith and Roberts (1993), and Arjas and Gasbarra (1994)
* Key algorithms for developing and implementing modern Bayesian methods include the Metropolis-Hastings-Green algorithm (see Metropolis et al. (1953), Hastings (1970), and Green (1995)) and the Tanner-Wong (1987) data augmentation algorithm

# R and R Markdown Basics

## For This Course

During this course will write a lot of R code and run computer simulations.
All my examples and homeworks will be written in R. 
Choosing R for your homework solutions is highly recommended.

What is R:

- R is an interpreted programming language, which means you will not have to compile your code. 
- R is very interactive, which means that you can play around with vectors and matrices, plot results.
- To keep track of your progress and to be able to construction your analysis steps, we will use R markdown. 
- R markdown is a format to make web reports that you can share with your collaborators.

## Vectors

Make a vector:

```{r}
x = c(11,218,123,36,1001)
y = rep(1,5)
z = seq(1,5,by=1)
x + y
z + 10
```

## Operations on Vectors

Some operations:

```{r}
sum(y)
c(mean(z),sd(z))
length(z)
```

## Coin Tossing

And most importantly there is a lot of functions for statistics. 

Like tossing a coin three times:

```{r}
set.seed(1)
coin = c('H','T')
samples = sample(coin,100,replace = TRUE)
```

Should come about half of the times head:

```{r}
sum(samples == 'H')
```

## Matrices

We can combine vectors of the same type into matrices.

```{r}
X = cbind(x,y,z)
X
```

## Data Frames

Data frames are used to combine variables of different types in one object.

```{r}
subjects = c('Jim','Jack','Joe','Mary','Jean')
sex = c('M','M','M','F','F')
score = c(85,90,75,100,70)
D = data.frame(subjects,sex,score)
D
```

## Generating Random Data

R provides a wide variety of distributions that we can sample from. All the ones we know from intro stats courses. For instance the normal distribution:

```{r}
z = rnorm(1000)
mean(z)
sd(z)
```

## Basic Plotting

Plotting the sample from last slide.

```{r}
hist(z,breaks=30)
```

## Fancy Plotting

The ggplot2 package is very popular to make more sophisticated plots.

```{r}
library(ggplot2)
```

You will have to learn the grammar of ggplot. There are many tutorials online. Here is one example [link](http://www.cookbook-r.com/Graphs/). 

Let's see how it looks like in action on sleep study data.

```{r}
library(Lock5Data)
data(SleepStudy)
```

## Fancy Plotting

```{r eval=TRUE,echo=TRUE}
ggplot(SleepStudy, aes(x=Drinks,y=GPA)) + 
  geom_point(position=position_jitter(w=0.1,h=0)) +
  geom_smooth() + xlab('number of alcoholic drinks per week')
```

## Repeated Tasks

In addition to the usual ``for`` loops are R provides ``apply`` and ``tapply`` functions.

```{r}
X
apply(X,1,mean)
apply(X,2,mean)
```

## Repeated Tasks

```{r}
D
tapply(D$score,D$sex,mean)
```

## User Defined Functions

To define your own functions:

```{r}
mSummary = function(x) {
  q1 = quantile(x,.25)
  q3 = quantile(x,.75) 
  list(med=median(x),iqr=q3-q1)
}
xsamp = 1:13
mSummary(xsamp)
```

## Monte Carlo Simulations

Generate a matrix with 100 rows and 10 columns elements drawn from a normal distribution:
```{r}
X = matrix(rnorm(10*100),ncol=10)
```

Each row is a distinct sample of size 10. The sample mean of each sample is
```{r}
xbar = apply(X,1,mean)
```

and the variance of the sample means is 
```{r}
var(xbar)
```
compare to theoretical result due to the central limit theorem $\frac{\sigma^2}{n} = 0.1$

## R Packages

In addition to the base functionality, there are thousands of packages available. 
This is the command for installing the pacakge ``ggplot2``:

```{r eval=FALSE}
install.packages("bootstrap")
```

once installed, you can use it in your code:

```{r}
library(bootstrap)
data(law)
head(law)
```

## Homeworks with R Markdown

```{r, out.width = 800, fig.retina = NULL, echo=FALSE}
knitr::include_graphics("RMarkdownScreenshot.png")
```

# Rank-Based Method Example

## Question

Does sending kids to school improve social awareness over home schooling?

## Experiment

Eight pairs of identical twins who are of nursery school age. For each pair, one is randomly selected to attend nursery school while the other remains at home. At the end of the study period, all 16 children are given the same social awareness test.

```{r}
school = c(82,69,73,43,58,56,76,65)
home = c(63,42,74,37,51,43,80,62)
```

## Model

$X_1,X_2,\dots,X_n$ denotes random sample following the model
$$ X_i = \theta + e_i, $$
random errors are independently and identically distributed with a continuous probability density function $f(t)$ symmetric around $0$.

We test hypothesis:
$$ H_0: \theta = 0 \text{ versus } H_A: \theta > 0 $$

The null distribution of the test statistic does not depend on $f(t)$

## Ranks

For each pair, the response of interest is the difference in the twins’ scores:

```{r}
school
home
response = school - home
response
```

## Ranks

Calculate ranks of absolute values:

```{r}
response
abs(response)
rank(abs(response))
```

## Statistic

We use signed-rank Wilcoxon procedure to find out.
The usual statistic is the sum of the ranks of positive items:

```{r}
wplus = sum((response>0)*rank(abs(response)))
wplus
```

or equivalently:

```{r}
n = length(response)
w = sum(sign(response)*rank(abs(response)))
wplus = 0.5*w + n*(n+1)/4
wplus
```

## Null Distribution of Test Statistics

Is this due to chance? Luckily, the probability mass function for our test statistic is known.

To see this, let's look at an even simpler example of only three differences, with 8 possible outcomes:

```{r message=FALSE}
library(R.utils)
sign = intToBin(0:7)
sign = sapply(sign,function(i) c(substr(i,1,1),substr(i,2,2),substr(i,3,3)) )
sign = t(matrix(as.integer(sign),nrow = 3))
rank = t(matrix(rep(1:3,8),nrow = 3))
data.frame(sign=sign,rank=rank)
```

## Null Distribution of Test Statistics

```{r}
outcome = data.frame(sign=sign,rank=rank,mul=sign*rank,sum=apply(sign*rank,1,sum))
outcome
table(outcome$sum)
```

## Null Distribution of Test Statistics

Or through Monte Carlo simulations:

```{r}
n = 3; nsim = 10000
X = matrix(rnorm(n*nsim),ncol=n)
WplusSim = apply(X,1, function(x) { r = rank(abs(x)); sum(r * (x>0)) })
table(WplusSim)/nsim
```

and compare with theoretical result:

```{r echo=FALSE}
prop = c(1/8,1/8,1/8,2/8,1/8,1/8,1/8)
names(prop) = 0:6
prop
```

## Computations

Now back to our "more complicated" school example.

```{r eval=FALSE}
n = length(response)
x = seq(0,n*(n+1)/2,1)
colors = rep("white",length(x))
colors[x >= wplus] = "red"
barplot(dsignrank(x,n),
        names.arg = x,
        main = paste0("dsignrank(x, n = ", n, ")"),
        col = colors)
```

## Computations

```{r echo=FALSE}
n = length(response)
x = seq(0,n*(n+1)/2,1)
colors = rep("white",length(x))
colors[x >= wplus] = "red"
barplot(dsignrank(x,n),names.arg = x,main = paste0("dsignrank(x, n = ", n, ")"),col = colors)
```

## Computations

and using the cumulative distribution function $F_{W^+}(w^+-1,n)$ of our test statistic:

```{r}
plot(x,psignrank(x, n = n),type = "s",main = paste0("psignrank(x, n = ", n, ")"))
abline(v=wplus,col = "red"); abline(h=psignrank(wplus,n),col = "blue")
```

## Computations

we can find the $p$-value using $P_{H_0}(W^+ \ge w^+) = 1 - F_{W^+}(w^+ - 1,n)$:

```{r}
pvalue = 1-psignrank(wplus-1,n,lower.tail = TRUE)
pvalue
```

There is also an R base implementation of this test:

```{r}
wilcox.test(response,alternative="greater")
```

## Conclusion

We can reject the null hypothesis at significance level $\alpha = 0.05$ in favor of the alternative that sending kids to school improves their social awareness.

# Bootstrap Example

## Question

Does small dose of aspirin prevent heart attacks in healthy middle-aged men?

## Experiment

A randomized double-blind study was designed to collect data. The subjects were randomly assigned to the aspirin and placebo groups. Doctors and subjects didn't know whether they treated or received aspirin or placebo substance.

```{r}
labels = c("nattacks","nsubjects")
aspirin = c(104,11037)
placebo = c(189,11034)
data = data.frame(aspirin,placebo)
rownames(data) = labels
data
```

## Statistic

Ratio of the rates:

```{r}
rate = function(v) v[1]/v[2]
theta.hat = rate(data$aspirin)/rate(data$placebo)
theta.hat
```

This means that in this sample aspirin-takers only have 55% as many heart attacks as placebo-takers.

## Computations

Is this due to chance? Or can this results be reproduced in other similar experiments? 
Let's use the bootstrap method to find out:

```{r}
population.one = c( rep(1,data["nattacks","aspirin"]), 
                    rep(0,data["nsubjects","aspirin"]-data["nattacks","aspirin"]) )
population.two = c( rep(1,data["nattacks","placebo"]), 
                    rep(0,data["nsubjects","placebo"]-data["nattacks","placebo"]) )

draw.bootstrap.sample = function() {
  boot.pop.one = sample(population.one,replace = TRUE)
  boot.pop.two = sample(population.two,replace = TRUE)
  rate.one = sum(boot.pop.one)/length(boot.pop.one)
  rate.two = sum(boot.pop.two)/length(boot.pop.two)
  return(rate.one/rate.two)
}
```

## Computations

and now simulate:

```{r eval = FALSE}
nrep = 10000
theta.boot = replicate(nrep,draw.bootstrap.sample())
hist(theta.boot,breaks=100)

# sample estimate
abline(v=theta.hat,col = "red",lwd = 4)

# bootstrap confidence interval
confidence.lower = sort(theta.boot)[nrep*.025]
confidence.upper = sort(theta.boot)[nrep*.975]
abline(v=c(confidence.lower,confidence.upper),col = "blue",lwd = 4)
```

## Computations

```{r echo = FALSE}
nrep = 10000
theta.boot = replicate(nrep,draw.bootstrap.sample())
hist(theta.boot,breaks=100)

# sample estimate
abline(v=theta.hat,col = "red",lwd = 4)

# bootstrap confidence interval
confidence.lower = sort(theta.boot)[nrep*.025]
confidence.upper = sort(theta.boot)[nrep*.975]
abline(v=c(confidence.lower,confidence.upper),col = "blue",lwd = 4)
```

## Conclusion

We can conclude that aspirin was found to be significantly beneficial for preventing heart attacks.

# Overall Conclusion

## Summary

* You learned some basics about R
* You saw an example using the ranks of difference instead the acutal difference for hypothesis testing
* You saw an example of estimating the uncertainty of a parameter using the bootstrap

## Next Lecture

Closer look at the boostrap and Monte Carlo simulations.

## Homeworks

Homework 1 will be posted on the course website today.

Deadline: Monday, April 4th before class at 1:30 pm.
