\documentclass[10pt]{beamer}

\input ../pack.tex
\input ../defs.tex
\input ../form.tex

\title{\large \bfseries Stats 205: \\ Introduction to Nonparametric Statistics \linebreak \linebreak \linebreak
Lecture 1: \\ Logistics and Introduction}

\author{Instructor: Christof Seiler}

\date{Spring 2016}

\begin{document}

\frame{
\thispagestyle{empty}
\titlepage
}

\begin{frame}
\frametitle{Course}

You can find everything on our course website:
\url{http://christofseiler.github.io/stats205/} \newline

Office hours (\alert{check room availability}): \newline
Monday 2:45 -- 3:45 \newline
Friday 2:45 -- 3:45

\end{frame}

\begin{frame}
\frametitle{Today}

\begin{itemize}
\item Why?
\item Goals
\item Textbooks
\item Grading
\item History
\item Examples
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Why?}

\begin{enumerate}
\item Few assumptions about underlying populations from which data is obtained, e.g. populations don't need to follow a normal distribution
\item Often easier to understand and apply than parametric tests
\item Slightly less efficient than parametric test when parametric assumptions hold, but if assumptions don't hold then wildly more efficient
\item Jackknife and bootstrap can be used in many practical situations where theory is intractable
\item Bayesian methods are available so prior information can be incorporated
\end{enumerate}

\end{frame}

\begin{frame}
\frametitle{Goals}

\begin{itemize}
\item Get an overview of classical, Bayesian, and modern methods
\item Learn how to implement methods yourself and use existing R packages
\item Be aware and understand underlying assumptions
\item Apply to modern data analysis problems that you care about
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Textbooks}

Our main textbook with lots of practical computations in R (free online): \newline
Kloke and McKean (2015). Nonparametric Statistical Methods Using R \newline

Bayesian view (free online): \newline
M\"uller, Quintana, Jara, and Hanson (2015). Bayesian Nonparametric Data Analysis \newline

In-depth coverage of the bootstrap: \newline
Efron and Tibshirani (1994). An Introduction to the Bootstrap  \newline

In-depth mathematical account on rank-based methods: \newline
Lehmann (2006). Nonparametrics Statistical Methods Based on Ranks \newline

Very comprehensive covering most of the material of the previous books (free online): \newline
Hollander and Wolfe, and Chicken (2013). Nonparametric Statistical Methods \newline

\end{frame}

\begin{frame}
\frametitle{Grading: Homework and Projects}

\begin{itemize}
\item Weekly homework assignments (40\%), mostly R exercises
\item Project (60\%)
\end{itemize}

The overall goal of the project is to write a paper on applying nonparametric statistics to your field of interest or studying some theoretical aspects that your care about.

\end{frame}

\begin{frame}
\frametitle{Grading: Details on Projects}

The project will be split in two parts:
\begin{enumerate}
\item Midterm project (3 pages with references) (20\%): 
\begin{itemize}
\item Project proposal and outline of planned tasks
\end{itemize}
\item Final project (12 pages plus references) (40\%): 
\begin{itemize}
\item A theoretical part: Explanation of the method studied and its properties
\item A computational part: Preferably in R
\item A data-analysis part: Plots and interpretations
\end{itemize}
\end{enumerate}

\end{frame}

\begin{frame}
\frametitle{History 1930's to 1970's}

\begin{itemize}
\item Beginning of nonparametric statistics (Hotelling and Pabst 1936)
\item Wilcoxon (1945) introduced the two-sample rank sum test for equal sample sizes, and Mann and Whitney (1947) generalize it
\item Pitman (1948), Hodges and Lehmann (1956), and Chernoff and Savage (1958) showed desirable efficiency properties
\item Jackknife, introduced by Quenouille (1949) as a bias-reduction technique and extended by Tukey (1958, 1962) to provide approximate significance tests and confidence intervals
\item Hodges and Lehmann (1963) showed how to derive estimators from rank tests and established that these estimators have desirable properties
\item Cox (1972) model and methods for survival analysis
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{History 1970's to Now}

\begin{itemize}
\item Efron's (1979) bootstrap makes use of increasing computational resources to provide standard errors and confidence intervals where  difficult, if not impossible, to use a parametric approach \newline

Some examples from special issue of Statistical Science (Randles, Hettmansperger, and Casella, 2004):
\item Robust analysis of linear models (McKean, 2004)
%\item Comparing variances and other dispersion measures (Boos and Brownie, 2004)
%\item Use of sign statistics in one-way layouts (Elmore, Hettmansperger, and Xuan, 2004)
\item Density estimation (Sheather, 2004)
%\item Multivariate nonparametric tests (Oja and Randles, 2004)
%\item Quantile-quantile (QQ) plots (Marden, 2004)
%\item Survival analysis (Akritas, 2004)
%\item Spatial statistics (Chang, 2004)
%\item Ranked set sampling (Wolfe, 2004) 
%\item Reliability (Hollander and Pe\~na, 2004)
\item Data modeling via quantile methods (Parzen, 2004)
\item Kernel smoothers (Schucany, 2004)
\item Permutation-based inference (Ernst, 2004) 
%\item Data depth tests for location and scale differences for multivariate distributions (Li and Liu, 2004)
\item Multivariate signed rank tests in time series problems (Hallin and Paindaveine, 2004)
%\item Rank-based analyses of crossover studies (Putt and Chinchilli, 2004)
\item Generalizations for nonlinear manifolds (Patrangenaru and Ellingson 2015)
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Bayesian History}

\begin{itemize}
\item Ferguson (1973) introduced nonparametric Bayesian methods
\item Susarla and van Ryzin (1976) derived nonparametric Bayesian estimators of survival curves
\item Dykstra and Laud (1981) developed a Bayesian nonparametric approach to reliability
\item Hjort (1990b) proposed nonparametric Bayesian estimators to model the cumulative hazard
\item In the late 1980s and the 1990s, there was a surge of activity in Bayesian methods due to the Markov chain Monte Carlo (MCMC) methods, e.g. Gelfand and Smith (1990), Gamerman (1991), West (1992), Smith and Roberts (1993), and Arjas and Gasbarra (1994)
\item Key algorithms for developing and implementing modern Bayesian methods include the Metropolis-Hastings-Green algorithm (see Metropolis et al. (1953), Hastings (1970), and Green (1995)) and the Tanner-Wong (1987) data augmentation algorithm
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Rank-Based Method Example}

$X_1,X_2,\dots,X_n$ denotes random sample following the model
\[ X_i = \theta + e_i, \]
random errors are independently and identically distributed with a continuous probability density function $f(t)$ symmetric around $0$.

We test hypothesis:
\[ H_0: \theta = 0 \text{ versus } H_A: \theta > 0 \]

\begin{itemize}
\item R markdown session
%\item Blackboard Derivation of the Distribution of $W^+$ under $H_0$: \linebreak Comment 5 on P. 46 in Hollander, Myles, and Wolfe \linebreak
\item The null distribution of the test statistic did not depend on $f(t)$
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Distribution of Test Statistic under $H_0$}

$B$ denotes number of positive $Z$'s \newline
$r_1 < \dots < r_B$ denotes ordered ranks of absolute values of $Z$'s \newline

\centering
\renewcommand{\arraystretch}{1.5}
\small
\begin{tabular}{c|l|c|c}
B & $(r_1,r_2,\dots,r_B)$ & Probability under $H_0$ & $T^+ = \sum_{i=1}^B r_i$\\
\hline
0	&	&	$\frac{1}{8}$	&	0 \\
1	&	$r_1 = 1$		&	$\frac{1}{8}$	&	1 \\
1	&	$r_2 = 2$		&	$\frac{1}{8}$	&	2 \\
1	&	$r_3 = 3$		&	$\frac{1}{8}$	&	3 \\
2	&	$r_1 = 1,r_2 = 2$		&	$\frac{1}{8}$	&	3 \\
2	&	$r_1 = 1,r_2 = 3$		&	$\frac{1}{8}$	&	4 \\
2	&	$r_1 = 2,r_2 = 3$		&	$\frac{1}{8}$	&	5 \\
3	&	$r_1 = 1,r_2 = 2,r_3 = 3$		&	$\frac{1}{8}$	&	6 \\
\end{tabular}

\end{frame}

\begin{frame}
\frametitle{Distribution of Test Statistic under $H_0$}

\centering
\renewcommand{\arraystretch}{1.5}
\small
\begin{tabular}{c|c}
Possible value of $T^+$ & Probability under $H_0$ \\
\hline
0	&	$\frac{1}{8}$ \\
1	&	$\frac{1}{8}$ \\
2	&	$\frac{1}{8}$ \\
3	&	$\frac{2}{8}$ \\
4	&	$\frac{1}{8}$ \\
5	&	$\frac{1}{8}$ \\
6	&	$\frac{1}{8}$ \\
\end{tabular}

\vspace{1cm}
More details on page 46, Comment 5 in Hollander, Wolfe, and Chicken.

\end{frame}

\begin{frame}
\frametitle{Bootstrap Example}

\begin{itemize}
\item R markdown session
\item We can computed confidence intervals of an estimate using computer simulations
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Next Lecture}

In the next lecture, we will focus on
\begin{itemize}
\item simulations and enumerations
\end{itemize}

\end{frame}

\end{document}
